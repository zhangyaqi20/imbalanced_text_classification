{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def read_from_csv(data_name, header=0, names=None):\n",
    "    if \"tsv\" in data_name:\n",
    "        data = pd.read_csv(data_name,\n",
    "                            sep='\\t',\n",
    "                            encoding = \"utf-8\",\n",
    "                            engine = \"python\",\n",
    "                            header = header,\n",
    "                            names = names)\n",
    "    elif \"csv\" in data_name:\n",
    "        data = pd.read_csv(data_name,\n",
    "                        encoding = \"utf-8\",\n",
    "                        engine = \"python\",\n",
    "                        header = header,\n",
    "                        names = names)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Given data file type is not supported yet.\")\n",
    "    return data\n",
    "\n",
    "def print_data_info(data, split, label_col):\n",
    "    label_counts = data[label_col].value_counts().to_dict()\n",
    "    output = f\"{split}\\t{len(data)}\"\n",
    "    for label in sorted(label_counts.keys()):\n",
    "        output += f\"\\t{label}: {label_counts[label]}, \"\n",
    "        output += \"{:.1%}\".format(label_counts[label]/len(data))\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMI\n",
    "# https://live.european-language-grid.eu/catalogue/corpus/7272/download/\n",
    "data_name = \"ami\"\n",
    "train_data = read_from_csv(f\"./{data_name}/train.tsv\")\n",
    "test_data = read_from_csv(f\"./{data_name}/test.tsv\")\n",
    "print('AMI\\nMulti - {\"discredit\": 0, \"stereotype\": 1, \"dominance\": 2, \"sexual harassment\": 3, \"derailing\": 4}')\n",
    "category2index = {\"discredit\": 0, \"stereotype\": 1, \"dominance\": 2, \"sexual_harassment\": 3, \"derailing\": 4}\n",
    "train_misogynous = train_data[train_data[\"misogynous\"] != 0].reset_index(drop=True)\n",
    "train_misogynous[\"label_multi\"] = train_misogynous['misogyny_category'].map(category2index)\n",
    "# train_misogynous.to_csv(\"./ami/train_clean.csv\", index=False)\n",
    "# dict(train_misogynous.label_multi.value_counts()) # {0: 1014, 3: 352, 1: 179, 2: 148, 4: 92}\n",
    "test_misogynous = test_data[test_data[\"misogynous\"] != 0].reset_index(drop=True)\n",
    "test_misogynous[\"label_multi\"] = test_misogynous['misogyny_category'].map(category2index)\n",
    "# test_misogynous.to_csv(\"./ami/test_clean.csv\", index=False)\n",
    "# dict(test_misogynous.label_multi.value_counts()) # {0: 141, 1: 140, 2: 124, 3: 44, 4: 11}\n",
    "# lc = [760, 134, 111, 264, 69]\n",
    "# {label: round((1/count)*100, 1) for label, count in enumerate(lc)} # {0: 0.1, 1: 0.7, 2: 0.9, 3: 0.4, 4: 1.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bretschneider-TH: bin - 0: neutral, 1: harassment\n",
    "merged_data = read_from_csv(\"./bretschneider-th/merged_data.csv\")\n",
    "merged_data = merged_data.dropna().reset_index(drop=True)\n",
    "merged_data = merged_data.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "school_data = read_from_csv(\"./bretschneider-th/school_labels.csv\")\n",
    "school_data = school_data.rename(columns={\"tweetId\": \"tweet_id\"})\n",
    "school_data = school_data[[\"tweet_id\", \"bullyLabel\"]]\n",
    "school_data = school_data.drop_duplicates().reset_index(drop=True)\n",
    "school_data = school_data.merge(merged_data, on='tweet_id')\n",
    "# school_data.to_csv(\"./bretschneider-th/school_data_clean.csv\", index=False)\n",
    "# school_data.label.value_counts() # 1285, 111\n",
    "\n",
    "main_data = read_from_csv(\"./bretschneider-th/main_labels.csv\")\n",
    "main_data = main_data.rename(columns={\"tweetId\": \"tweet_id\"})\n",
    "main_data = main_data[[\"tweet_id\", \"bullyLabel\"]]\n",
    "main_data = main_data.drop_duplicates().reset_index(drop=True)\n",
    "main_data = main_data.merge(merged_data, on='tweet_id')\n",
    "# main_data.to_csv(\"./bretschneider-th/main_data_clean.csv\", index=False)\n",
    "# main_data.label.value_counts() # 2713, 185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMSB - TSD\n",
    "# https://search.gesis.org/research_data/SDN-10.7802-2251\n",
    "data = read_from_csv(\"./cmsb-tsd/sexism_data.csv\")\n",
    "data[\"label\"] = 0\n",
    "data[\"label\"] = data[\"label\"].where(data[\"sexist\"] == False, 1)\n",
    "# data.label.value_counts().to_dict() # {0: 11822, 1: 1809}\n",
    "data.to_csv(\"./cmsb-tsd/data_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.2, 1: 0.1, 2: 0.4}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Davidson-THON: multi - 0: hate speech, 1: offensive language, 2: neither\n",
    "data = read_from_csv(\"./davidson-thon/davidson-thon.csv\")\n",
    "data = data.rename(columns={\"tweet\": \"text\"})\n",
    "data = data.rename(columns={\"class\": \"label_multi\"})\n",
    "data = data[[\"text\", \"label_multi\"]]\n",
    "# data.to_csv(\"./davidson-thon/data_clean.csv\", index=False)\n",
    "data.label_multi.value_counts().to_dict()\n",
    "lc = [858, 11514, 2497]\n",
    "{label: round((1/count)*1000, 1) for label, count in enumerate(lc)} # {0: 1.2, 1: 0.1, 2: 0.4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Founta 2018 - THAS: multi - normal, spam, abusive, hateful\n",
    "data = read_from_csv(\"./founta-2018-thas/large_scale_hatespeechtwitter.csv\", header=None, names=['id', 'text', 'class'])\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "class2label = {\"normal\": 0, \"spam\": 1, \"abusive\": 2, \"hateful\": 3}\n",
    "data[\"label_multi\"] = data[\"class\"].map(class2label)\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "data.to_csv(\"./founta-2018-thas/data_clean.csv\", index=False)\n",
    "# dict(data.label_multi.value_counts()) # {0: 33325, 1: 7555, 2: 3934, 3: 1638}\n",
    "# lc = [19994, 4533, 2360, 983]\n",
    "# {label: round((1/count)*1000, 1) for label, count in enumerate(lc)} # {0: 0.1, 1: 0.2, 2: 0.4, 3: 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1093, 1: 435}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gao 2018 FHC - bin\n",
    "data = read_from_csv(\"./gao-2018-fhc/data_clean.csv\")\n",
    "dict(data.label.value_counts()) # {0: 1093, 1: 435}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibert 2018 - SHS binary\n",
    "data_annotations = read_from_csv(\"./gibert-2018-shs/annotations_metadata.csv\")\n",
    "# print(dict(data_annotations.label.value_counts())) # {'noHate': 9507, 'hate': 1196, 'relation': 168, 'idk/skip': 73}\n",
    "data_annotations = data_annotations.rename(columns={\"label\": \"class\"})\n",
    "class2label = {\"noHate\": 0, \"hate\": 1, \"relation\": -1, \"idk/skip\": -1}\n",
    "data_annotations[\"label\"] = data_annotations[\"class\"].map(class2label)\n",
    "data_annotations = data_annotations[data_annotations[\"label\"] != -1].reset_index(drop=True)\n",
    "# dict(data_annotations.label.value_counts()) # {0: 9507, 1: 1196}\n",
    "def get_text_by_file_id(file_id):\n",
    "    with open(f\"./gibert-2018-shs/all_files/{file_id}.txt\", \"r\") as f:\n",
    "        text = f.readlines()[0]\n",
    "    return text\n",
    "data_annotations[\"text\"] = data_annotations.apply(lambda row: get_text_by_file_id(row.file_id), axis=1)\n",
    "data = data_annotations[[\"text\", \"label\"]]\n",
    "# data.to_csv(\"./gibert-2018-shs/data_clean.csv\", index=False)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter hate speech - TSA: binary - not hate speech, hate speech\n",
    "data = read_from_csv(\"./twitter-hate-speech-tsa/data.csv\")\n",
    "data = data.rename(columns={\"tweet\": \"text\"})\n",
    "data.to_csv(\"./twitter-hate-speech-tsa/data_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US Election 2020: binary - HOF, Non-HOF\n",
    "train_data = read_from_csv(\"./us-election-2020/train.tsv\")\n",
    "train_data[\"label\"] = 0\n",
    "train_data[\"label\"] = train_data[\"label\"].where(train_data[\"HOF\"] == \"Non-Hateful\", 1)\n",
    "train_data.to_csv(\"./us-election-2020/train_clean.csv\", index=False)\n",
    "\n",
    "test_data = read_from_csv(\"./us-election-2020/test.tsv\")\n",
    "test_data[\"label\"] = 0\n",
    "test_data[\"label\"] = test_data[\"label\"].where(test_data[\"HOF\"] == \"Non-Hateful\", 1)\n",
    "test_data.to_csv(\"./us-election-2020/test_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    7392\n",
       "1    2662\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Waseem-and-Hovy 2016: bin - 0: neither, 1: sexism or racism\n",
    "data = read_from_csv(\"./waseem-and-hovy-2016/srw.csv\")\n",
    "data[\"label\"] = 0\n",
    "data[\"label\"] = data[\"label\"].where(data[\"category\"] == \"none\", 1)\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "# data.to_csv(\"./waseem-and-hovy-2016/data_clean.csv\", index=False)\n",
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get a list of lexicon, saved in 'augmentation_src/abusive_language_lexicon/abusive_lexicon.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Wikipedia English Swear words: https://en.wiktionary.org/wiki/Category:English_swear_words\n",
    "f = open(\"augmentation_src/abusive_language_lexicon/wikipedia_english_swear_words.txt\", \"r\")\n",
    "wikipedia_swear_words = f.read().splitlines()\n",
    "len(wikipedia_swear_words) # 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Wikipedia English profenity: https://en.wikipedia.org/wiki/Category:English_profanity\n",
    "f = open(\"augmentation_src/abusive_language_lexicon/wikipedia_english_profanity.txt\", \"r\")\n",
    "wikipedia_profanity_words = f.read().splitlines()\n",
    "len(wikipedia_profanity_words) # 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. MOL: https://github.com/franciellevargas/MOL\n",
    "mol = read_from_csv(\"augmentation_src/abusive_language_lexicon/mol.csv\")\n",
    "mol = mol[mol['en-contextual-label'] == 1][['en-american-english']].reset_index(drop=True)\n",
    "mol_words = mol['en-american-english'].values.tolist() # 610\n",
    "mol_words_lowercase = [word.lower() for word in mol_words]\n",
    "mol_words_unique = list(set(mol_words_lowercase)) # drop duplicates\n",
    "mol_words_str = [word for word in mol_words_unique if not word.isdigit()] # remove only digit 0\n",
    "mol_words_clean = [] # refine word of formats: \"a / b\", \"a (x) b\"\n",
    "optional_words = []\n",
    "for word in mol_words_str:\n",
    "    word_nospace = word.replace(\" \", \"\")\n",
    "    word_nohyper = word_nospace.replace(\"-\", \"\")\n",
    "    if not word_nohyper.isalnum():\n",
    "        optional_words.append(word)\n",
    "    else:\n",
    "        mol_words_clean.append(word)\n",
    "# print(optional_words) # [\"ass kissers (adj) / to kiss someone's ass (v)\", 'loser / fool', '(white) trash', \n",
    "#                         # \"to kiss (someone's) ass\", 'discard / blow something off', '(to get) dumped', \n",
    "#                         # 'obnoxious / loudmouth', 'cuck / cuckold', 'cuck / cuckold', \"buffin' the muffin\", \n",
    "#                         # 'ladies of the night (hoes)']\n",
    "mol_words_clean += [\"ass kissers\", \"to kiss ass\", \"loser\", \"fool\", \"trash\", \"white trash\", \n",
    "                    \"discard\", \"blow off\", \"dumped\", \"to get dumped\",\n",
    "                    \"obnoxious\", \"loudmouth\", \"cuck \", \"cuckold\", \"buffin' the muffin\", \n",
    "                    \"ladies of the night\", \"ladies of the night hoes\"]\n",
    "len(mol_words_clean) # 375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Davidson Lexicon: https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/lexicons/refined_ngram_dict.csv\n",
    "davidson_ngram_lexicon = read_from_csv(\"augmentation_src/abusive_language_lexicon/refined_ngram_dict.csv\")\n",
    "davidson_ngram_words = davidson_ngram_lexicon.ngram.values.tolist()\n",
    "len(davidson_ngram_words) # 178"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Lexicon of Abusive words https://github.com/uds-lsv/lexicon-of-abusive-words\n",
    "f = open(\"augmentation_src/abusive_language_lexicon/law_baseLexicon.txt\", \"r\")\n",
    "base_lexicon = f.read().splitlines()\n",
    "base_lexicon_clean = [word.split()[0].split(\"_\")[0] for word in base_lexicon if word.split()[1] == \"TRUE\"]\n",
    "len(base_lexicon_clean) # 551"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2987"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "# 5. Lexicon of Abusive words https://github.com/uds-lsv/lexicon-of-abusive-words\n",
    "f = open(\"augmentation_src/abusive_language_lexicon/law_expandedLexicon.txt\", \"r\")\n",
    "expanded_lexicon = f.read().splitlines()\n",
    "expanded_lexicon_words = [word.split()[0].split(\"_\")[0] for word in expanded_lexicon if '-' not in word.split()[1]] # 2989\n",
    "expanded_lexicon_words = [word for word in expanded_lexicon_words if not word.isdigit()] # remove only digits\n",
    "len(expanded_lexicon_words) # 2987"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "abusive_lexicon = (wikipedia_swear_words + wikipedia_profanity_words \n",
    "                    + mol_words_clean + davidson_ngram_words\n",
    "                    + base_lexicon_clean + expanded_lexicon_words)\n",
    "# len(abusive_lexicon) # 4208\n",
    "abusive_lexicon_lowercase = [word.lower().strip() for word in abusive_lexicon]\n",
    "abusive_lexicon_unique = list(set(abusive_lexicon_lowercase))\n",
    "abusive_lexicon_unique.sort()\n",
    "len(abusive_lexicon_unique) # 3331\n",
    "lexicon_path = '../data/augmentation_src/abusive_language_lexicon/abusive_lexicon.json'\n",
    "with open(lexicon_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(abusive_lexicon_unique, f, ensure_ascii=False, indent=4)\n",
    "# with open(lexicon_path) as f:\n",
    "#     lexicon = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Get the fastText vectors for each of the expression in the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3331, 300)\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "fasttext_model_path = '/mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/data/augmentation_src/abusive_language_lexicon/cc.en.300.bin'\n",
    "fasttext_model = fasttext.load_model(fasttext_model_path)\n",
    "\n",
    "def get_fasttext_vector(text):\n",
    "    fasttext_vec = np.mean([fasttext_model.get_word_vector(word) for word in text.split()], axis=0)\n",
    "    return fasttext_vec\n",
    "fasttext_vectors_path = \"augmentation_src/abusive_language_lexicon/abusive_lexicon_fasttext_vectors.npy\"\n",
    "fasttext_vectors = []\n",
    "for word in abusive_lexicon_unique:\n",
    "    fasttext_vectors.append(get_fasttext_vector(word))\n",
    "print(np.stack(fasttext_vectors).shape) # (3331, 300)\n",
    "with open(fasttext_vectors_path, \"wb\") as f:\n",
    "    np.save(f, np.stack(fasttext_vectors))\n",
    "# # with open(fasttext_vectors_path, 'rb') as f:\n",
    "# #     vectors = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ailab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
