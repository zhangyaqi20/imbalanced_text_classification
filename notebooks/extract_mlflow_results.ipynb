{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# \n",
    "bin_data_names = ['twitter-hate-speech-tsa', 'civil-comments', 'gibert-2018-shs', 'us-election-2020', 'cmsb-tsd']\n",
    "multi_class_data_names = ['founta-2018-thas', 'davidson-thon', 'ami']\n",
    "eval_data_names = bin_data_names + multi_class_data_names\n",
    "check_variants = [\"baseline\", \n",
    "                    \"sampling_modifiedRS_oversampling\", \"sampling_modifiedRS_undersampling\", \"sampling_weightedRS_combi\", \n",
    "                    \"augmentation_bert\", \n",
    "                    \"th\", \"wce\", \"fl\", \"wfl\",\n",
    "                    \"augmentation_external_data_preprocessing\", \"augmentation_abusive_lexicon\", ]\n",
    "\n",
    "cc = True\n",
    "if cc:\n",
    "    eval_data_names = ['civil-comments', 'civil-comments-5k-7p5', 'civil-comments-20k', 'civil-comments-40k']\n",
    "    check_variants = ['augmentation_abusive_lexicon', \n",
    "        'augmentation_external_data_preprocessing', 'baseline', 'fl',\n",
    "        'sampling_modifiedRS_oversampling',\n",
    "        'sampling_modifiedRS_undersampling', 'th']\n",
    "\n",
    "data_cols = [\"data_name\", \"num_classes\", \"data_type\", \"size\", \"rho\", \"distribution\"]\n",
    "variant_cols = ['variant', 'sampling_modifiedRS_rho', 'sampling_weightedRS_percentage', 'augmentation_rho', 'augmentation_percentage', 'augmentation_top_k', 'wce_alpha', 'fl_gamma', \"augmentation_bert_top_k\"]\n",
    "metric_suffices = ['f1_macro', 'f1_per_label_0', 'f1_per_label_1', 'f1_per_label_2', 'f1_per_label_3', 'f1_per_label_4', 'accuracy', 'precision_macro', 'precision_weighted', 'recall_macro', 'recall_weighted', 'auprc']\n",
    "metrics_cols = [f'train_{suffix}' for suffix in metric_suffices] + [f'val_{suffix}' for suffix in metric_suffices] + [f'test_{suffix}' for suffix in metric_suffices]\n",
    "cols_raw = data_cols + variant_cols + ['pl_seed'] + metrics_cols + [\"best_epoch\"] + ['mlflow_run_id']\n",
    "\n",
    "cols_seeds_mean = data_cols + variant_cols[:-1] + ['val_f1_macro', 'test_f1_macro', 'test_f1_macro_std'] + [metric for metric in metrics_cols if metric not in ['val_f1_macro', 'test_f1_macro', 'test_f1_macro_std']]\n",
    "\n",
    "results_name = \"results\"\n",
    "if cc:\n",
    "    results_name += \"_cc\"\n",
    "results_excel_path = f\"results/{results_name}_all.xlsx\"\n",
    "cols_results = data_cols + variant_cols[:-1] + ['test_f1_macro', 'test_f1_macro_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2distribution = {}\n",
    "data2size = {}\n",
    "data2rho = {}\n",
    "\n",
    "def read_from_csv(data_name, header=0, names=None):\n",
    "    if \"tsv\" in data_name:\n",
    "        data = pd.read_csv(data_name,\n",
    "                            sep='\\t',\n",
    "                            encoding = \"utf-8\",\n",
    "                            engine = \"python\",\n",
    "                            header = header,\n",
    "                            names = names)\n",
    "    elif \"csv\" in data_name:\n",
    "        data = pd.read_csv(data_name,\n",
    "                        encoding = \"utf-8\",\n",
    "                        engine = \"python\",\n",
    "                        header = header,\n",
    "                        names = names)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Given data file type is not supported yet.\")\n",
    "    return data\n",
    "def get_data_info_by_data_name(data_name, label_col, category2label, train_filename, val_filename=None, test_filename=None):\n",
    "    data = read_from_csv(f\"./data/{data_name}/{train_filename}\")\n",
    "    label_counts = data[label_col].value_counts()\n",
    "    if val_filename:\n",
    "        val_data = read_from_csv(f\"./data/{data_name}/{val_filename}\")\n",
    "        label_counts += val_data[label_col].value_counts()\n",
    "    if test_filename:\n",
    "        test_data = read_from_csv(f\"./data/{data_name}/{test_filename}\")\n",
    "        label_counts += test_data[label_col].value_counts()\n",
    "    data2size[data_name] = sum(label_counts.values)\n",
    "    data2distribution[data_name] = {f\"{label}_{cat}\": round(label_counts[label]/data2size[data_name], 3) for cat, label in category2label.items()}\n",
    "    data2rho[data_name] = round(max(label_counts.values) / min(label_counts.values), 2)\n",
    "\n",
    "get_data_info_by_data_name(\"twitter-hate-speech-tsa\", \"label\", {\"non-hate\": 0, \"hate\": 1}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"civil-comments\", \"label\", {\"non-toxic\": 0, \"toxic\": 1}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"civil-comments-5k-7p5\", \"label\", {\"non-toxic\": 0, \"toxic\": 1}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"civil-comments-20k\", \"label\", {\"non-toxic\": 0, \"toxic\": 1}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"civil-comments-40k\", \"label\", {\"non-toxic\": 0, \"toxic\": 1}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"gibert-2018-shs\", \"label\", {\"no-hate\": 0, \"hate\": 1}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"us-election-2020\", \"label\", {\"non-HOF\": 0, \"HOF\": 1}, \"train_clean.csv\", test_filename=\"test_clean.csv\")\n",
    "get_data_info_by_data_name(\"cmsb-tsd\", \"label\", {\"non-sexist\": 0, \"sexist\": 1}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"waseem-and-hovy-2016\", \"label\", {\"neither\": 0, \"racism/sexism\": 1}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"founta-2018-thas\", \"label_multi\", {\"normal\": 0, \"spam\": 1, \"abusive\": 2, \"hateful\": 3}, \"data_clean.csv\")\n",
    "get_data_info_by_data_name(\"ami\", \"label_multi\", {\"discredit\": 0, \"stereotype\": 1, \"dominance\": 2, \"sexual harassment\": 3, \"derailing\": 4}, \"train_clean.csv\", test_filename=\"test_clean.csv\")\n",
    "get_data_info_by_data_name(\"davidson-thon\", \"label_multi\", {\"hate speech\": 0, \"offensive language\": 1, \"neither\": 2}, \"data_clean.csv\")\n",
    "\n",
    "\n",
    "data_name_orig2display = {'twitter-hate-speech-tsa': 'Twitter-Hate-Speech', \n",
    "                            'civil-comments': \n",
    "                                    'CC-5k-rho=11.5', \n",
    "                                    # 'Civil-Comments',\n",
    "                            'civil-comments-20k': 'CC-20k-rho=11.5', \n",
    "                            'civil-comments-40k': 'CC-40k-rho=11.5',\n",
    "                            'civil-comments-5k-7p5': 'CC-5k-rho=7.5',\n",
    "                            'gibert-2018-shs': 'Gibert-2018', \n",
    "                            'us-election-2020': 'US-Election-2020', \n",
    "                            'cmsb-tsd': 'CMSB', \n",
    "                            'waseem-and-hovy-2016': 'Waseem-and-Hovy-2016',\n",
    "                            'founta-2018-thas': 'Founta-2018', \n",
    "                            'davidson-thon': 'Davidson-2017', \n",
    "                            'ami': 'AMI-2018'}\n",
    "\n",
    "for data_name in eval_data_names:\n",
    "    print(f\"{data_name_orig2display[data_name]}\\t{data2size[data_name]}\\t{data2distribution[data_name]}\\t{data2rho[data_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_by_run_id(data_dir, run_id, key):\n",
    "    value = None\n",
    "    for root, _, files in os.walk(data_dir + run_id):\n",
    "        if key in files:\n",
    "            with open(f\"{root}/{key}\", \"r\") as f:\n",
    "                value = f.readlines()[-1]\n",
    "                if \"train\" in key or \"val\" in key or \"test\" in key:\n",
    "                    value = value.split()[1]\n",
    "                if value == \"sampling_modifiedRS\": # Forgot to specify in the variant value\n",
    "                    value += \"_oversampling\"\n",
    "                if value == \"sampling_weightedRS\":\n",
    "                    value += \"_combi\"\n",
    "                if \"tensor\" in value:\n",
    "                    value = \"-\"\n",
    "                try:\n",
    "                    value = ast.literal_eval(value)\n",
    "                    if isinstance(value, list):\n",
    "                        value = [round(v, 2) for v in value]\n",
    "                        value = tuple(value)\n",
    "                except (ValueError, SyntaxError):\n",
    "                    pass\n",
    "                break\n",
    "    if value is None:\n",
    "        value = \"-\"\n",
    "        if \"train\" in key or \"val\" in key or \"test\" in key:\n",
    "            if \"per_label\" in key:\n",
    "                value = 0\n",
    "            else:\n",
    "                raise RuntimeError(f\"This run log {data_dir + run_id} does not have test metrics. \")\n",
    "    return value\n",
    "\n",
    "def get_best_epoch_by_run_id(data_dir, run_id):\n",
    "    ckpt_dir = f\"{data_dir}{run_id}/artifacts/model_checkpoints/\"\n",
    "    ckpt = os.listdir(ckpt_dir)[0]\n",
    "    epoch = ast.literal_eval(ckpt.split(\"epoch=\")[1][1:2])\n",
    "    return epoch\n",
    "\n",
    "def get_log_by_data_name(data_name):\n",
    "    data_runs_dir = f'/mounts/data/proj/zhangyaq/imbalanced_text_classification_logs/{data_name}/'\n",
    "    run_ids = os.listdir(data_runs_dir)\n",
    "    rows = []\n",
    "    for run_id in run_ids:\n",
    "        run_result = {col_name: get_value_by_run_id(data_runs_dir, run_id, col_name) for col_name in cols_raw[:-2]}\n",
    "        run_result['best_epoch'] = get_best_epoch_by_run_id(data_runs_dir, run_id)\n",
    "        run_result['mlflow_run_id'] = run_id\n",
    "        rows.append(run_result)\n",
    "    df = pd.DataFrame(columns=cols_raw, data=rows)\n",
    "    #  - add wfl(alpha=1.0) => fl\n",
    "    df.loc[(df[\"variant\"] == \"wfl\") & (df[\"wce_alpha\"] == \"-\"),  \"variant\"] = \"fl\" \n",
    "    #  - combine these two columns\n",
    "    def fix_aug_top_k(row):\n",
    "        if row[\"augmentation_top_k\"] != \"-\" or row[\"augmentation_bert_top_k\"] != \"-\":\n",
    "            if row[\"augmentation_top_k\"] != \"-\":\n",
    "                return row[\"augmentation_top_k\"]\n",
    "            else:\n",
    "                return row[\"augmentation_bert_top_k\"]\n",
    "        else:\n",
    "            return \"-\"\n",
    "    df[\"augmentation_top_k\"] = df.apply(lambda row: fix_aug_top_k(row), axis=1)\n",
    "    df = df.drop('augmentation_bert_top_k', axis=1)\n",
    "    return df\n",
    "\n",
    "def aggregate_results(data_name, writer):\n",
    "    # Get raw mlflow logs:\n",
    "    df = get_log_by_data_name(data_name)\n",
    "    df.to_excel(writer, sheet_name=f\"{data_name}_raw\")\n",
    "    assert len(df) % 3 == 0\n",
    "    # Aggregate results from 3 seeds\n",
    "    agg_target = {metric: 'mean' for metric in metrics_cols}\n",
    "    agg_target.update({col: 'first' for col in data_cols})\n",
    "    df['test_f1_macro_std'] = df.loc[:, 'test_f1_macro']\n",
    "    agg_target.update({'test_f1_macro_std': 'std'})\n",
    "    df_seeds_mean = df.groupby(variant_cols[:-1], as_index=False).agg(agg_target)\n",
    "    #  - change metric to be 00.00\n",
    "    for metric in metrics_cols + ['test_f1_macro_std']:\n",
    "        df_seeds_mean[metric] = df_seeds_mean[metric].apply(lambda x: ast.literal_eval(\"{0:.2f}\".format(x*100)))\n",
    "    def convert_str2number(x):\n",
    "        if type(x) == str and x != \"-\":\n",
    "            return ast.literal_eval(x)\n",
    "        else:\n",
    "            return x\n",
    "    for var in variant_cols[1:-1]:\n",
    "        df_seeds_mean[var] = df_seeds_mean[var].apply(lambda x: convert_str2number(x))\n",
    "    df_seeds_mean.loc[:, \"size\"] = [data2size[data_name]] * len(df_seeds_mean)\n",
    "    df_seeds_mean.loc[:, \"distribution\"] = [data2distribution[data_name]] * len(df_seeds_mean)\n",
    "    df_seeds_mean.loc[:, \"rho\"] = [data2rho[data_name]] * len(df_seeds_mean)\n",
    "    #  - re-order the columns\n",
    "    df_seeds_mean = df_seeds_mean[cols_seeds_mean]\n",
    "    if data_name == \"founta-2018-thas\":\n",
    "        df_seeds_mean = df_seeds_mean[~((df_seeds_mean[\"data_name\"] == data_name) & (df_seeds_mean[\"variant\"] == \"augmentation_external_data\"))]\n",
    "        df_seeds_mean.loc[(df_seeds_mean[\"data_name\"] == data_name) & (df_seeds_mean[\"variant\"] == \"augmentation_external_data_oversampling\"), \"variant\"] = \"augmentation_external_data\"\n",
    "        df_seeds_mean = df_seeds_mean[~((df_seeds_mean[\"data_name\"] == data_name) & (df_seeds_mean[\"variant\"] == \"augmentation_external_data_oversampling\"))]\n",
    "    df_seeds_mean.to_excel(writer, sheet_name=f\"{data_name}_seeds-mean\")\n",
    "    df_seeds_mean = df_seeds_mean[df_seeds_mean[\"variant\"].isin(check_variants)]\n",
    "    df_val_f1_max = df_seeds_mean.loc[df_seeds_mean.groupby([\"variant\"])['val_f1_macro'].idxmax()].reset_index(drop=True)\n",
    "    return df, df_seeds_mean, df_val_f1_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = eval_data_names\n",
    "\n",
    "writer = pd.ExcelWriter(results_excel_path)\n",
    "df_all_raw_results = pd.DataFrame(columns=cols_raw)\n",
    "df_all_mean_results = pd.DataFrame(columns=cols_seeds_mean)\n",
    "df_all_agg_results = pd.DataFrame(columns=cols_results)\n",
    "for data_name in data_names:\n",
    "    print(f\"Extracting logs for {data_name} ...\")\n",
    "    df, df_seeds_mean, df_val_f1_max = aggregate_results(data_name, writer)\n",
    "    df_all_raw_results = pd.concat([df_all_raw_results, df], ignore_index=True)\n",
    "    df_all_mean_results = pd.concat([df_all_mean_results, df_seeds_mean], ignore_index=True)\n",
    "    df_all_agg_results = pd.concat([df_all_agg_results, df_val_f1_max], ignore_index=True)\n",
    "df_all_raw_results.to_csv(f\"results/raw_{results_name}.csv\", index=False)\n",
    "df_all_mean_results.to_csv(f\"results/mean_{results_name}.csv\", index=False)\n",
    "df_all_agg_results = df_all_agg_results.sort_values(by=['data_name', 'test_f1_macro']).reset_index(drop=True)\n",
    "df_all_agg_results.to_csv(f\"results/agg_{results_name}.csv\", index=False)\n",
    "df_all_agg_results.to_excel(writer, sheet_name=f\"best_of_all\")\n",
    "\n",
    "variants = df_all_agg_results.variant.unique()\n",
    "df_table = pd.DataFrame(columns=data_names)\n",
    "for variant in variants:\n",
    "    for data_name in data_names:\n",
    "        condition = (df_all_agg_results[\"variant\"] == variant) & (df_all_agg_results[\"data_name\"] == data_name)\n",
    "        if len(df_all_agg_results[condition]) > 0:\n",
    "            df_table.loc[variant, data_name] = f\"{df_all_agg_results[condition][\"test_f1_macro\"].values[0]} ({df_all_agg_results[condition][\"test_f1_macro_std\"].values[0]})\"\n",
    "df_table = df_table.reindex(check_variants)\n",
    "\n",
    "df_table.to_excel(writer, sheet_name=f\"Table\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_agg_results = df_all_agg_results[df_all_agg_results[\"variant\"].isin(['augmentation_abusive_lexicon', \n",
    "        'augmentation_external_data_preprocessing', 'baseline', 'fl',\n",
    "        'sampling_modifiedRS_oversampling',\n",
    "        'sampling_modifiedRS_undersampling', 'th'])]\n",
    "df_all_agg_results = df_all_agg_results.sort_values(by=['data_name', 'test_f1_macro']).reset_index(drop=True)\n",
    "# df_all_agg_results.variant.unique()\n",
    "df_all_agg_results.to_csv(f\"results/agg_{results_name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
