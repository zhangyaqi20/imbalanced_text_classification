{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_from_csv(data_name, header=0, names=None):\n",
    "    if \"tsv\" in data_name:\n",
    "        data = pd.read_csv(data_name,\n",
    "                            sep='\\t',\n",
    "                            encoding = \"utf-8\",\n",
    "                            engine = \"python\",\n",
    "                            header = header,\n",
    "                            names = names)\n",
    "    elif \"csv\" in data_name:\n",
    "        data = pd.read_csv(data_name,\n",
    "                        encoding = \"utf-8\",\n",
    "                        engine = \"python\",\n",
    "                        header = header,\n",
    "                        names = names)\n",
    "    else:\n",
    "        raise NotImplementedError(\"Given data file type is not supported yet.\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.color_palette()\n",
    "rc = {\n",
    "    'figure.figsize':(5,4),\n",
    "      'axes.facecolor':'white',\n",
    "      'axes.grid' : True,\n",
    "      'grid.color': '.8',\n",
    "      'text.color': 'black',\n",
    "      'xtick.color': 'black',\n",
    "      'ytick.color': 'black',\n",
    "      'font.family':'Times New Roman',\n",
    "      'font.size' : 16}\n",
    "plt.rcParams.update(rc)\n",
    "\n",
    "data_name_orig2display = {'twitter-hate-speech-tsa': 'Twitter-Hate-Speech', \n",
    "                            'civil-comments': \n",
    "                                # r'CC-5k-$\\rho$=11.5', \n",
    "                                    'Civil-Comments',\n",
    "                            'civil-comments-20k': r'CC-20k-$\\rho$=11.5', \n",
    "                            'civil-comments-40k': r'CC-40k-$\\rho$=11.5',\n",
    "                            'civil-comments-5k-7p5': r'CC-5k-$\\rho$=7.5',\n",
    "                            'gibert-2018-shs': 'Gibert-2018', \n",
    "                            'us-election-2020': 'US-Election-2020', \n",
    "                            'cmsb-tsd': 'CMSB', \n",
    "                            'waseem-and-hovy-2016': 'Waseem-and-Hovy-2016',\n",
    "                            'founta-2018-thas': 'Founta-2018', \n",
    "                            'davidson-thon': 'Davidson-2017', \n",
    "                            'ami': 'AMI-2018'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_mean_results = read_from_csv(\"mean_results.csv\")\n",
    "df_all_agg_results = read_from_csv(\"agg_results.csv\")\n",
    "def convert_str2number(x):\n",
    "    if type(x) == str and x != \"-\":\n",
    "        return ast.literal_eval(x)\n",
    "    else:\n",
    "        return x\n",
    "for var in variant_cols[1:-1]:\n",
    "    df_all_mean_results[var] = df_all_mean_results[var].apply(lambda x: convert_str2number(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_mean_results.variant.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"YlOrBr\", as_cmap=True).get_over()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,2.6))#figsize=(5,2.5)4.6\n",
    "df_std = df_all_agg_results[[\"data_name\", \"test_f1_macro_std\"]]\n",
    "df_std = df_std.rename({\"data_name\":\"Dataset\", \"test_f1_macro_std\":\"Standard Deviation of Macro F1 Scores\"}, axis=1)\n",
    "df_std[\"Dataset\"] = df_std['Dataset'].map(data_name_orig2display)\n",
    "data_order = [data_name_orig2display[data_name] for data_name in eval_data_names]\n",
    "if cc:\n",
    "    palette = {r'CC-5k-$\\rho$=11.5': (0.8666666666666667, 0.5176470588235295, 0.3215686274509804),\n",
    "               r'CC-5k-$\\rho$=7.5': [1.        , 0.8        , 0.6, 1.        ],\n",
    "               r'CC-20k-$\\rho$=11.5': [0.8       , 0.35, 0.1, 0.2        ],\n",
    "               r'CC-40k-$\\rho$=11.5':[0.7       , 0.25, 0.1, 0.2       ],\n",
    "               } \n",
    "else:\n",
    "    palette = sns.color_palette()\n",
    "sns.boxplot(x=\"Standard Deviation of Macro F1 Scores\", y=\"Dataset\", data=df_std, order=data_order, palette=palette)\n",
    "# ax.set_xticks([0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0])\n",
    "ax.figure.savefig('cc_std_distribution.svg', dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho_name = \"augmentation_rho\"\n",
    "# data_clear_overfitting = [\"civil-comments\", \"davidson-thon\"]\n",
    "# which_data_to_check = (df_all_mean_results[\"variant\"].isin([\"baseline\", \"augmentation_external_data\"])) & (df_all_mean_results[\"data_name\"].isin(data_clear_overfitting))\n",
    "# df_overfitting = df_all_mean_results[which_data_to_check][[\"data_name\", rho_name, \"train_f1_macro\", \"val_f1_macro\", \"test_f1_macro\"]]\n",
    "# def compute_deviation_to_baseline(row, split):\n",
    "#     baseline_condition = (df_overfitting[\"data_name\"] == row[\"data_name\"]) & (df_overfitting[rho_name] == \"-\")\n",
    "#     baseline_value = df_overfitting.loc[baseline_condition, f\"{split}_f1_macro\"].values[0]\n",
    "#     return row[f\"{split}_f1_macro\"] - baseline_value\n",
    "# for split in [\"train\", \"val\", \"test\"]:\n",
    "#     df_overfitting[f\"{split}_f1_macro_delta\"] = df_overfitting.apply(lambda row: compute_deviation_to_baseline(row, split), axis=1)\n",
    "# df_overfitting = df_overfitting[df_overfitting[rho_name] != \"-\"]\n",
    "# df_overfitting = df_overfitting[[rho_name, \"data_name\", \"train_f1_macro_delta\", \"val_f1_macro_delta\", \"test_f1_macro_delta\"]]\n",
    "# df_overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "data_clear_overfitting = [\"twitter-hate-speech-tsa\", \"civil-comments\", \"cmsb-tsd\", \"founta-2018-thas\", \"davidson-thon\", ]\n",
    "data_no_overfitting = [\"gibert-2018-shs\", \"us-election-2020\", \"ami\"]\n",
    "\n",
    "# fig_name = \"no_overfitting\"\n",
    "# data = data_no_overfitting\n",
    "# rho_ticks = [1,2,3,5,7.5]\n",
    "\n",
    "fig_name = \"clear_overfitting\"\n",
    "data = data_clear_overfitting\n",
    "rho_ticks = [1,2,3,5,7.5,10,15]\n",
    "\n",
    "which_data_to_check = ((df_all_mean_results[\"variant\"].isin([\"baseline\", \"sampling_modifiedRS_oversampling\"])) \n",
    "                        & (df_all_mean_results[\"data_name\"].isin(data))\n",
    "                        )\n",
    "rho_name = \"sampling_modifiedRS_rho\"\n",
    "splits = [\"train\", \"val\"]\n",
    "df_overfitting = df_all_mean_results[which_data_to_check][[\"data_name\", rho_name]+[f\"{split}_f1_macro\" for split in splits]]\n",
    "def compute_deviation_to_baseline(row, split):\n",
    "    baseline_condition = (df_overfitting[\"data_name\"] == row[\"data_name\"]) & (df_overfitting[rho_name] == \"-\")\n",
    "    baseline_value = df_overfitting.loc[baseline_condition, f\"{split}_f1_macro\"].values[0]\n",
    "    return row[f\"{split}_f1_macro\"] - baseline_value\n",
    "for split in splits:\n",
    "    df_overfitting[f\"{split}_f1_macro_delta\"] = df_overfitting.apply(lambda row: compute_deviation_to_baseline(row, split), axis=1)\n",
    "df_overfitting = df_overfitting[df_overfitting[rho_name] != \"-\"]\n",
    "df_overfitting = df_overfitting[[\"data_name\", rho_name]+[f\"{split}_f1_macro_delta\" for split in splits]]\n",
    "\n",
    "df_overfitting_expanded = pd.DataFrame(columns=[rho_name, \"data_name\", \"split\", \"f1_delta\"])\n",
    "rhos_unique = df_overfitting[rho_name].unique().tolist()[:-1]\n",
    "data_names_unique = df_overfitting.data_name.unique().tolist()\n",
    "data_names_list = []\n",
    "for data_name in data_names_unique:\n",
    "    data_names_list += [data_name_orig2display[data_name]] * len(splits) * len(rhos_unique)\n",
    "df_overfitting_expanded[\"data_name\"] = data_names_list\n",
    "rhos_list = []\n",
    "for rho in rhos_unique:\n",
    "    rhos_list += [rho] * len(splits)\n",
    "df_overfitting_expanded[rho_name] = rhos_list * len(data_names_unique)\n",
    "df_overfitting_expanded[\"split\"] = splits * len(rhos_unique) * len(data_names_unique)\n",
    "for data_name in data_names_unique:\n",
    "    for split in splits:\n",
    "        for rho in rhos_unique:\n",
    "            where_to_assign = ((df_overfitting_expanded[\"data_name\"] == data_name_orig2display[data_name]) \n",
    "                                & (df_overfitting_expanded[\"split\"] == split)\n",
    "                                & (df_overfitting_expanded[rho_name] == rho))\n",
    "            value_from_where = ((df_overfitting[\"data_name\"] == data_name)\n",
    "                                & (df_overfitting[rho_name] == rho))\n",
    "            if len(df_overfitting[value_from_where]) > 0:\n",
    "                value = df_overfitting.loc[value_from_where, f\"{split}_f1_macro_delta\"].values[0]\n",
    "                df_overfitting_expanded.loc[where_to_assign, \"f1_delta\"] = value\n",
    "df_overfitting_expanded = df_overfitting_expanded.rename({\"data_name\":\"Dataset\", \"split\":\"Split\"}, axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.8,4.95))\n",
    "sns.lineplot(x=rho_name, y=\"f1_delta\", data=df_overfitting_expanded, \n",
    "                hue=\"Dataset\", style=\"Split\", marker=\"o\", ax=ax)#\n",
    "ax.set_xticks(rho_ticks, labels=rho_ticks)\n",
    "ax.set_xlabel(r\"Desired $\\rho^{\\prime}$ for Random Oversampling\")\n",
    "# delta_ticks = [y for y in list(range(-2, 8, 2))]\n",
    "# ax.set_yticks(delta_ticks)\n",
    "ax.set_ylabel(r'Deviation from Baseline: $\\delta_{ROS}$')\n",
    "# '-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\n",
    "ax.axhline(y=0, linewidth=0.8, color=\"black\", ls=\"-.\")\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig(f'ROS_{fig_name}.svg', dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2color = {\"Twitter-Hate-Speech\": sns.color_palette()[0], \n",
    "                            \"Civil-Comments\": sns.color_palette()[1],\n",
    "                            'Gibert-2018': sns.color_palette()[2],\n",
    "                            'US-Election-2020': sns.color_palette()[3],\n",
    "                            'CMSB': sns.color_palette()[4],\n",
    "                            'Founta-2018': sns.color_palette()[5],\n",
    "                            'Davidson-2017': sns.color_palette()[6],\n",
    "                            'AMI-2018': sns.color_palette()[7],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_name = \"sampling_modifiedRS_rho\"\n",
    "splits = [\"train\", \"val\"]\n",
    "data_clear_overfitting_binary = [\"twitter-hate-speech-tsa\", \"civil-comments\", \"cmsb-tsd\", ]\n",
    "data_clear_overfitting_multi = [\"founta-2018-thas\", \"davidson-thon\", ]\n",
    "which_data_to_check = ((df_all_mean_results[\"variant\"] == \"sampling_modifiedRS_oversampling\")\n",
    "                       & (df_all_mean_results[\"data_name\"].isin(data_clear_overfitting_binary+data_clear_overfitting_multi)))\n",
    "df_overfitting = df_all_mean_results[which_data_to_check][[\"data_name\", rho_name]+[f\"{split}_f1_macro\" for split in splits]]\n",
    "def compute_deviation_to_baseline(row, split):\n",
    "    baseline_condition = (df_all_mean_results[\"data_name\"] == row[\"data_name\"]) & (df_all_mean_results[\"variant\"] == \"baseline\")\n",
    "    baseline_value = df_all_mean_results.loc[baseline_condition, f\"{split}_f1_macro\"].values[0]\n",
    "    return row[f\"{split}_f1_macro\"] - baseline_value\n",
    "for split in splits:\n",
    "    df_overfitting[f\"{split}_f1_macro_delta\"] = df_overfitting.apply(lambda row: compute_deviation_to_baseline(row, split), axis=1)\n",
    "df_overfitting = df_overfitting[[\"data_name\", rho_name]+[f\"{split}_f1_macro_delta\" for split in splits]]\n",
    "\n",
    "data2ifoverfitting = {data_name: \"Overfitting:Binary Dataset\" for data_name in data_clear_overfitting_binary}\n",
    "data2ifoverfitting.update({data_name: \"Overfitting: Multi-Class Dataset\" for data_name in data_clear_overfitting_multi})\n",
    "df_overfitting[\"IfOverfitting\"] = df_overfitting[\"data_name\"].map(data2ifoverfitting)\n",
    "df_overfitting[\"data_name\"] = df_overfitting['data_name'].map(data_name_orig2display)\n",
    "df_overfitting = df_overfitting.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "\n",
    "rc = {\n",
    "    # 'figure.figsize':(5,4),\n",
    "      'axes.facecolor':'white',\n",
    "      'axes.grid' : True,\n",
    "      'grid.color': '.8',\n",
    "      'text.color': 'black',\n",
    "      'xtick.color': 'black',\n",
    "      'ytick.color': 'black',\n",
    "      'font.family':'Times New Roman',\n",
    "      'font.size' : 20}\n",
    "plt.rcParams.update(rc)\n",
    "g = sns.FacetGrid(df_overfitting, col=\"IfOverfitting\", hue=\"Dataset\", sharex=False, sharey=False, height=4, aspect=1, palette=data2color)\n",
    "g.map(sns.lineplot, \"sampling_modifiedRS_rho\", \"val_f1_macro_delta\", marker=\"o\", linestyle='--')\n",
    "g.map(sns.lineplot, \"sampling_modifiedRS_rho\", \"train_f1_macro_delta\", marker=\"o\")\n",
    "# g.add_legend()\n",
    "# g._legend.remove()\n",
    "\n",
    "axs = list(g.axes_dict.values())\n",
    "axs[0].set_title('Overfitting: Binary Dataset')\n",
    "rho_ticks = [1,2,3,5,7.5,10]\n",
    "axs[0].set_xticks(rho_ticks, labels=rho_ticks)\n",
    "axs[1].set_title('Overfitting: Multi-Class Dataset')\n",
    "rho_ticks = [1,2,3,5,7.5, 10, 15]\n",
    "axs[1].set_xticks(rho_ticks, labels=rho_ticks)\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [\n",
    "                Line2D([0], [0], color=sns.color_palette()[0], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Twitter-Hate-Speech\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[1], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Civil-Comments\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[4], lw=1, linestyle=\"solid\", marker=\"o\", label=\"CMSB\"),\n",
    "                Line2D([0], [0], color=\"black\", lw=1, linestyle=\"solid\", label=\"Train\"),\n",
    "                Line2D([0], [0], color=\"black\", lw=1, linestyle=\"dashed\", label=\"Val\"),\n",
    "                ]\n",
    "axs[0].legend(handles=custom_lines, title=\"Dataset\", loc=\"lower right\", bbox_to_anchor=(1, 0.))#\n",
    "custom_lines = [\n",
    "                Line2D([0], [0], color=sns.color_palette()[5], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Founta-2018\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[6], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Davidson-2017\"),\n",
    "                Line2D([0], [0], color=\"black\", lw=1, linestyle=\"solid\", label=\"Train\"),\n",
    "                Line2D([0], [0], color=\"black\", lw=1, linestyle=\"dashed\", label=\"Val\"),\n",
    "                ]\n",
    "axs[1].legend(handles=custom_lines, title=\"Dataset\", loc=\"upper right\", bbox_to_anchor=(1, 1.))#\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_ylabel(r\"$\\delta_{ROS}$\")\n",
    "    ax.set_xlabel(r\"$\\rho^{\\prime}$\")\n",
    "    ax.axhline(y=0, linewidth=0.8, color=\"black\", ls=\"-.\")\n",
    "sns.despine(fig=None, ax=None, top=False, right=False, left=False, bottom=False, offset=False, trim=False)\n",
    "# sns.move_legend(g, \"upper left\", bbox_to_anchor=(0.865, 0.5))\n",
    "g.figure.tight_layout()\n",
    "g.figure.savefig('ROS_overfitting.svg', dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changes with rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_name = \"sampling_modifiedRS_rho\"\n",
    "data_clear_ros = [\"founta-2018-thas\"]\n",
    "which_data_to_check = ((df_all_mean_results[\"variant\"].isin([\"baseline\", \"sampling_modifiedRS_oversampling\"])) \n",
    "                        # & ~(df_all_mean_results[\"data_name\"].isin(data_clear_ros))\n",
    "                        )\n",
    "df_ros = df_all_mean_results[which_data_to_check][[\"data_name\", rho_name, \"val_f1_macro\"]]\n",
    "def compute_deviation_to_baseline(row, split):\n",
    "    baseline_condition = (df_ros[\"data_name\"] == row[\"data_name\"]) & (df_ros[rho_name] == \"-\")\n",
    "    baseline_value = df_ros.loc[baseline_condition, f\"{split}_f1_macro\"].values[0]\n",
    "    return row[f\"{split}_f1_macro\"] - baseline_value\n",
    "df_ros[\"val_f1_macro_delta\"] = df_ros.apply(lambda row: compute_deviation_to_baseline(row, \"val\"), axis=1)\n",
    "df_ros = df_ros[df_ros[rho_name] != \"-\"]\n",
    "df_ros = df_ros[[rho_name, \"data_name\", \"val_f1_macro_delta\"]]\n",
    "df_ros = df_ros.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "df_ros[\"Dataset\"] = df_ros['Dataset'].map(data_name_orig2display)\n",
    "rhos_unique = df_ros.sampling_modifiedRS_rho.unique().tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5,4.6)) # \n",
    "sns.lineplot(x=\"sampling_modifiedRS_rho\", y=\"val_f1_macro_delta\", data=df_ros, \n",
    "                hue=\"Dataset\", marker=\"o\", ax=ax)#\n",
    "rho_ticks = [1,2,3,5,7.5,10,15]\n",
    "ax.set_xticks(rho_ticks, labels=rho_ticks)\n",
    "ax.set_xlabel(r\"$\\rho^{\\prime}$\")\n",
    "ax.set_ylabel(r\"$\\delta_{ROS}$\")\n",
    "ax.axhline(y=0, linewidth=0.8, color=\"black\", ls=\"-.\")\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig('ROS_trend.svg', dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_name = \"sampling_modifiedRS_rho\"\n",
    "data_noclear_rs = [\"gibert-2018-shs\", \"us-election-2020\", \"ami\"]\n",
    "which_data_to_check = ((df_all_mean_results[\"variant\"].isin([\"sampling_modifiedRS_oversampling\", \"sampling_modifiedRS_undersampling\"])) \n",
    "                       & ~(df_all_mean_results[\"data_name\"].isin(data_noclear_rs))\n",
    "                       )\n",
    "df_ros_rus = df_all_mean_results[which_data_to_check][[\"data_name\", \"variant\", rho_name, \"val_f1_macro\"]]\n",
    "def compute_deviation_to_baseline(row, split):\n",
    "    baseline_condition = (df_all_mean_results[\"data_name\"] == row[\"data_name\"]) & (df_all_mean_results[\"variant\"] == \"baseline\")\n",
    "    baseline_value = df_all_mean_results.loc[baseline_condition, f\"{split}_f1_macro\"].values[0]\n",
    "    return row[f\"{split}_f1_macro\"] - baseline_value\n",
    "df_ros_rus[\"val_f1_macro_delta\"] = df_ros_rus.apply(lambda row: compute_deviation_to_baseline(row, \"val\"), axis=1)\n",
    "df_ros_rus = df_ros_rus[[rho_name, \"variant\", \"data_name\", \"val_f1_macro_delta\"]]\n",
    "df_ros_rus = df_ros_rus.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "df_ros_rus[\"Dataset\"] = df_ros_rus['Dataset'].map(data_name_orig2display)\n",
    "\n",
    "rc = {\n",
    "    # 'figure.figsize':(5,4),\n",
    "      'axes.facecolor':'white',\n",
    "      'axes.grid' : True,\n",
    "      'grid.color': '.8',\n",
    "      'text.color': 'black',\n",
    "      'xtick.color': 'black',\n",
    "      'ytick.color': 'black',\n",
    "      'font.family':'Times New Roman',\n",
    "      'font.size' : 21}\n",
    "plt.rcParams.update(rc)\n",
    "g = sns.FacetGrid(df_ros_rus, col=\"variant\", hue=\"Dataset\", sharey=False, height=4.2, aspect=1, legend_out=False, palette=data2color)\n",
    "g.map(sns.lineplot, \"sampling_modifiedRS_rho\", \"val_f1_macro_delta\", marker=\"o\")\n",
    "# g.add_legend()\n",
    "rho_ticks = [1,2,3,5,7.5,10,15]\n",
    "axs = list(g.axes_dict.values())\n",
    "axs[0].set_title('Random Oversampling')\n",
    "axs[0].set_ylabel(r\"$\\delta_{ROS}$\")\n",
    "# axs[0].set_yticks([-3,-2,-1,0,1,2], labels=[-3,-2,-1,0,1,2])\n",
    "axs[1].set_title('Random Undersampling')\n",
    "axs[1].set_ylabel(r\"$\\delta_{RUS}$\")\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "custom_lines = [\n",
    "                Line2D([0], [0], color=sns.color_palette()[0], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Twitter-Hate-Speech\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[1], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Civil-Comments\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[4], lw=1, linestyle=\"solid\", marker=\"o\", label=\"CMSB\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[5], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Founta-2018\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[6], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Davidson-2017\"),\n",
    "                ]\n",
    "axs[0].legend(handles=custom_lines, title=\"Dataset\", loc=\"center right\", bbox_to_anchor=(1, 0.21))#\n",
    "custom_lines = [\n",
    "                Line2D([0], [0], color=sns.color_palette()[0], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Twitter-Hate-Speech\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[1], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Civil-Comments\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[4], lw=1, linestyle=\"solid\", marker=\"o\", label=\"CMSB\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[5], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Founta-2018\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[6], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Davidson-2017\"),\n",
    "                ]\n",
    "axs[1].legend(handles=custom_lines, title=\"Dataset\", loc=\"lower right\", bbox_to_anchor=(1, 0.))#\n",
    "# rus_yticks = list(range(-10, 2, 2))\n",
    "# axs[1].set_yticks(rus_yticks, labels=rus_yticks)\n",
    "for ax in axs:\n",
    "    ax.set_xticks(rho_ticks, labels=rho_ticks)\n",
    "    ax.set_xlabel(r\"$\\rho^{\\prime}$\")\n",
    "    ax.axhline(y=0, linewidth=0.8, color=\"black\", ls=\"-.\")\n",
    "sns.despine(fig=None, ax=None, top=False, right=False, left=False, bottom=False, offset=None, trim=False)\n",
    "# sns.move_legend(g, loc='lower left', bbox_to_anchor=(0.3, 0.19))\n",
    "g.figure.tight_layout()\n",
    "g.figure.savefig('ROS_RUS_trend.svg', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_noclear_lossinfo_imbalance = [\"founta-2018-thas\"]\n",
    "which_data_to_check = ((df_all_mean_results[\"variant\"].isin([\"baseline\", \"sampling_modifiedRS_undersampling\"]))\n",
    "                        # & (~df_all_mean_results[\"data_name\"].isin(data_noclear_lossinfo_imbalance))\n",
    "                        )\n",
    "df_rus = df_all_mean_results[which_data_to_check][[\"data_name\", \"sampling_modifiedRS_rho\", \"val_f1_macro\"]]\n",
    "def compute_deviation_to_baseline(row, split):\n",
    "    baseline_condition = (df_rus[\"data_name\"] == row[\"data_name\"]) & (df_rus[\"sampling_modifiedRS_rho\"] == \"-\")\n",
    "    baseline_value = df_rus.loc[baseline_condition, f\"{split}_f1_macro\"].values[0]\n",
    "    return row[f\"{split}_f1_macro\"] - baseline_value\n",
    "df_rus[\"val_f1_macro_delta\"] = df_rus.apply(lambda row: compute_deviation_to_baseline(row, \"val\"), axis=1)\n",
    "df_rus = df_rus[df_rus[\"sampling_modifiedRS_rho\"] != \"-\"]\n",
    "df_rus = df_rus[[\"sampling_modifiedRS_rho\", \"data_name\", \"val_f1_macro_delta\"]]\n",
    "df_rus = df_rus.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "df_rus[\"Dataset\"] = df_rus['Dataset'].map(data_name_orig2display)\n",
    "rhos_unique = df_rus.sampling_modifiedRS_rho.unique().tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,4))\n",
    "sns.lineplot(x=\"sampling_modifiedRS_rho\", y=\"val_f1_macro_delta\", data=df_rus, \n",
    "                hue=\"Dataset\", marker=\"o\", ax=ax)#\n",
    "rho_ticks = [1,2,3,5,7.5,10,15]\n",
    "ax.set_xticks(rho_ticks, labels=rho_ticks)\n",
    "ax.set_xlabel(r\"$\\rho^{\\prime}$\")\n",
    "ax.set_ylabel(r\"$\\delta_{RUS}$\")\n",
    "ax.axhline(y=0, linewidth=0.8, color=\"black\", ls=\"-.\")\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig('RUS_trend.svg', dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combi RS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_noclear_lossinfo_imbalance = [\"founta-2018-thas\"]\n",
    "which_data_to_check = ((df_all_mean_results[\"variant\"].isin([\"baseline\", \"sampling_weightedRS_combi\"]))\n",
    "                        # & (~df_all_mean_results[\"data_name\"].isin(data_noclear_lossinfo_imbalance))\n",
    "                        )\n",
    "rho_name = \"sampling_weightedRS_percentage\"\n",
    "df_rus = df_all_mean_results[which_data_to_check][[\"data_name\", rho_name, \"val_f1_macro\"]]\n",
    "def compute_deviation_to_baseline(row, split):\n",
    "    baseline_condition = (df_rus[\"data_name\"] == row[\"data_name\"]) & (df_rus[rho_name] == \"-\")\n",
    "    baseline_value = df_rus.loc[baseline_condition, f\"{split}_f1_macro\"].values[0]\n",
    "    return row[f\"{split}_f1_macro\"] - baseline_value\n",
    "df_rus[\"val_f1_macro_delta\"] = df_rus.apply(lambda row: compute_deviation_to_baseline(row, \"val\"), axis=1)\n",
    "df_rus = df_rus[df_rus[rho_name] != \"-\"]\n",
    "df_rus = df_rus[[rho_name, \"data_name\", \"val_f1_macro\", \"val_f1_macro_delta\"]]\n",
    "df_rus = df_rus.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "df_rus[\"Dataset\"] = df_rus['Dataset'].map(data_name_orig2display)\n",
    "rhos_unique = df_rus[rho_name].unique().tolist()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()#figsize=(4.3,4.4)\n",
    "sns.lineplot(x=rho_name, y=\"val_f1_macro_delta\", data=df_rus, \n",
    "                hue=\"Dataset\", marker=\"o\", ax=ax)#\n",
    "rho_ticks = rhos_unique\n",
    "ax.set_xticks(rho_ticks, labels=rho_ticks)\n",
    "ax.set_xlabel(r\"Sampling Percentage for Combi RS\")\n",
    "# delta_ticks = [y for y in list(range(-2, 8, 2))]\n",
    "# ax.set_yticks(delta_ticks)\n",
    "ax.set_ylabel(r\"Deviation from Baseline: $\\delta_{Combi RS}$\")\n",
    "# '-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\n",
    "ax.axhline(y=0, linewidth=0.8, color=\"black\", ls=\"-.\")\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig('results/Combi-RS_trend.svg', dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare ROS, Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_to_check = [\"baseline\", \"sampling_modifiedRS_oversampling\", \"augmentation_bert\", \"augmentation_abusive_lexicon\", \"augmentation_external_data\"]\n",
    "which_data_to_check = (df_all_agg_results[\"variant\"].isin(variant_to_check))\n",
    "metrics_to_check = [\"test_f1_macro\", \"test_f1_per_label_0\", \"test_f1_per_label_1\", \"test_f1_per_label_2\", \"test_f1_per_label_3\", \"test_f1_per_label_4\"]\n",
    "df_mean_ros_aug = df_all_agg_results[which_data_to_check][[\"data_name\", \"variant\", ] + metrics_to_check]\n",
    "df_mean_ros_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_ros_aug_by_data = pd.DataFrame(columns=[\"variant\"] + sum([[f\"{data_name}_{metric}\" for metric in metrics_to_check] for data_name in data_names], []))\n",
    "df_mean_ros_aug_by_data[\"variant\"] = variant_to_check\n",
    "for data_name in data_names:\n",
    "    for variant in variant_to_check:\n",
    "        conditions = (df_mean_ros_aug[\"data_name\"] == data_name) & (df_mean_ros_aug[\"variant\"] == variant)\n",
    "        if len(df_mean_ros_aug[conditions]) > 0:\n",
    "            value = df_mean_ros_aug.loc[conditions, \"test_f1_macro\"].values[0]\n",
    "            df_mean_ros_aug_by_data.loc[df_mean_ros_aug_by_data[\"variant\"] == variant, f\"{data_name}_test_f1_macro\"] = value\n",
    "            for i in range(5):\n",
    "                value = df_mean_ros_aug.loc[conditions, f\"test_f1_per_label_{i}\"].values[0]\n",
    "                df_mean_ros_aug_by_data.loc[df_mean_ros_aug_by_data[\"variant\"] == variant, f\"{data_name}_test_f1_per_label_{i}\"] = value\n",
    "df_mean_ros_aug_by_data.to_csv(\"compare_ROS_augmentation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"wce\") & (df_all_mean_results[\"num_classes\"] == 2)\n",
    "df_mean_wce = df_all_mean_results[conditions][[\"data_name\", \"wce_alpha\", \"val_f1_macro\", \"val_f1_per_label_0\", \"val_f1_per_label_1\"]]\n",
    "def compute_deviation_to_baseline(row):\n",
    "    baseline_condition = (df_all_mean_results[\"data_name\"] == row[\"data_name\"]) & (df_all_mean_results[\"variant\"] == \"baseline\")\n",
    "    baseline_value = df_all_mean_results.loc[baseline_condition, \"val_f1_per_label_1\"].values[0]\n",
    "    return row['val_f1_per_label_1'] - baseline_value\n",
    "df_mean_wce[\"pos_f1_delta\"] = df_mean_wce.apply(lambda row: compute_deviation_to_baseline(row), axis=1)\n",
    "df_mean_wce[\"data_name\"] = df_mean_wce[\"data_name\"].map(data_name_orig2display)\n",
    "df_mean_wce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"wce\") & (df_all_mean_results[\"num_classes\"] > 2)\n",
    "df_mean_wce_multi = df_all_mean_results[conditions][[\"data_name\", \"wce_alpha\", \"val_f1_macro\", 'val_f1_per_label_0', 'val_f1_per_label_1', 'val_f1_per_label_2', 'val_f1_per_label_3', 'val_f1_per_label_4']]\n",
    "df_mean_wce_multi[\"data_name\"] = df_mean_wce_multi[\"data_name\"].map(data_name_orig2display)\n",
    "df_mean_wce_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wce_alpha_unique = df_mean_wce.wce_alpha.unique().tolist()\n",
    "wce_alpha_unique = [0.1, 0.25, 0.75, 0.878, 0.888, 0.9, 0.93, 0.99]\n",
    "# data_name_unique = df_mean_wce.data_name.unique().tolist()\n",
    "data_name_unique = [data_name_orig2display[data_name] for data_name in [\"twitter-hate-speech-tsa\", \"gibert-2018-shs\", \"us-election-2020\"]]\n",
    "df_wce_delta_by_data = pd.DataFrame(columns=[\"wce_alpha\"] + sum([[f\"{data_name}_macro_f1\", \n",
    "                                                                    f\"{data_name}_f1_non-hate\", \n",
    "                                                                    f\"{data_name}_f1_hate\"] for data_name in data_name_unique], []))\n",
    "df_wce_delta_by_data[\"wce_alpha\"] = wce_alpha_unique\n",
    "for data_name in data_name_unique:\n",
    "    for wce_alpha in wce_alpha_unique:\n",
    "        conditions = (df_mean_wce[\"data_name\"] == data_name) & (df_mean_wce[\"wce_alpha\"] == wce_alpha)\n",
    "        if len(df_mean_wce[conditions]) > 0:\n",
    "            value = df_mean_wce.loc[conditions, \"val_f1_macro\"].values[0]\n",
    "            df_wce_delta_by_data.loc[df_wce_delta_by_data[\"wce_alpha\"] == wce_alpha, f\"{data_name}_macro_f1\"] = value\n",
    "            value = df_mean_wce.loc[conditions, \"val_f1_per_label_0\"].values[0]\n",
    "            df_wce_delta_by_data.loc[df_wce_delta_by_data[\"wce_alpha\"] == wce_alpha, f\"{data_name}_f1_non-hate\"] = value\n",
    "            value = df_mean_wce.loc[conditions, \"val_f1_per_label_1\"].values[0]\n",
    "            df_wce_delta_by_data.loc[df_wce_delta_by_data[\"wce_alpha\"] == wce_alpha, f\"{data_name}_f1_hate\"] = value\n",
    "df_wce_delta_by_data = df_wce_delta_by_data.sort_values(by=['wce_alpha']).reset_index(drop=True)\n",
    "df_wce_delta_by_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"fl\") & (df_all_mean_results[\"num_classes\"] == 2)\n",
    "df_mean_fl = df_all_mean_results[conditions][[\"data_name\", \"fl_gamma\", \"val_f1_macro\", \"val_f1_per_label_0\", \"val_f1_per_label_1\"]]\n",
    "df_mean_fl[\"data_name\"] = df_mean_fl[\"data_name\"].map(data_name_orig2display)\n",
    "\n",
    "fl_gamma_unique = df_mean_fl.fl_gamma.unique().tolist()\n",
    "data_name_unique = df_mean_fl.data_name.unique().tolist()\n",
    "df_mean_fl_by_data = pd.DataFrame()\n",
    "df_mean_fl_by_data[\"fl_gamma\"] = fl_gamma_unique\n",
    "for data_name in data_name_unique:\n",
    "    for fl_gamma in fl_gamma_unique:\n",
    "        conditions = (df_mean_fl[\"data_name\"] == data_name) & (df_mean_fl[\"fl_gamma\"] == fl_gamma)\n",
    "        if len(df_mean_fl[conditions]) > 0:\n",
    "            value = df_mean_fl.loc[conditions, \"val_f1_macro\"].values[0]\n",
    "            df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_macro_f1\"] = value\n",
    "            value = df_mean_fl.loc[conditions, \"val_f1_per_label_0\"].values[0]\n",
    "            df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_f1_non-hate\"] = value\n",
    "            value = df_mean_fl.loc[conditions, \"val_f1_per_label_1\"].values[0]\n",
    "            df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_f1_hate\"] = value\n",
    "df_mean_fl_by_data = df_mean_fl_by_data.sort_values(by=['fl_gamma']).reset_index(drop=True)\n",
    "df_mean_fl_by_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"fl\") & (df_all_mean_results[\"data_name\"] == \"davidson-thon\")\n",
    "df_mean_fl = df_all_mean_results[conditions][[\"data_name\", \"fl_gamma\", \"val_f1_macro\", \"val_f1_per_label_0\", \"val_f1_per_label_1\", \"val_f1_per_label_2\", \"val_f1_per_label_3\"]]\n",
    "df_mean_fl[\"data_name\"] = df_mean_fl[\"data_name\"].map(data_name_orig2display)\n",
    "\n",
    "fl_gamma_unique = df_mean_fl.fl_gamma.unique().tolist()\n",
    "data_name_unique = df_mean_fl.data_name.unique().tolist()\n",
    "df_mean_fl_by_data = pd.DataFrame()\n",
    "df_mean_fl_by_data[\"fl_gamma\"] = fl_gamma_unique\n",
    "for data_name in data_name_unique:\n",
    "    for fl_gamma in fl_gamma_unique:\n",
    "        conditions = (df_mean_fl[\"data_name\"] == data_name) & (df_mean_fl[\"fl_gamma\"] == fl_gamma)\n",
    "        if len(df_mean_fl[conditions]) > 0:\n",
    "            value = df_mean_fl.loc[conditions, \"val_f1_macro\"].values[0]\n",
    "            df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_macro_f1\"] = value\n",
    "            for i in range(3):\n",
    "                value = df_mean_fl.loc[conditions, f\"val_f1_per_label_{i}\"].values[0]\n",
    "                df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_f1_per_label_{i}\"] = value\n",
    "df_mean_fl_by_data = df_mean_fl_by_data.sort_values(by=['fl_gamma']).reset_index(drop=True)\n",
    "df_mean_fl_by_data = df_mean_fl_by_data.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "df_mean_fl_by_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"fl\") & (df_all_mean_results[\"data_name\"] == \"founta-2018-thas\")\n",
    "df_mean_fl = df_all_mean_results[conditions][[\"data_name\", \"fl_gamma\", \"val_f1_macro\", \"val_f1_per_label_0\", \"val_f1_per_label_1\", \"val_f1_per_label_2\", \"val_f1_per_label_3\"]]\n",
    "df_mean_fl[\"data_name\"] = df_mean_fl[\"data_name\"].map(data_name_orig2display)\n",
    "\n",
    "fl_gamma_unique = df_mean_fl.fl_gamma.unique().tolist()\n",
    "data_name_unique = df_mean_fl.data_name.unique().tolist()\n",
    "df_mean_fl_by_data = pd.DataFrame()\n",
    "df_mean_fl_by_data[\"fl_gamma\"] = fl_gamma_unique\n",
    "for data_name in data_name_unique:\n",
    "    for fl_gamma in fl_gamma_unique:\n",
    "        conditions = (df_mean_fl[\"data_name\"] == data_name) & (df_mean_fl[\"fl_gamma\"] == fl_gamma)\n",
    "        if len(df_mean_fl[conditions]) > 0:\n",
    "            value = df_mean_fl.loc[conditions, \"val_f1_macro\"].values[0]\n",
    "            df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_macro_f1\"] = value\n",
    "            for i in range(4):\n",
    "                value = df_mean_fl.loc[conditions, f\"val_f1_per_label_{i}\"].values[0]\n",
    "                df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_f1_per_label_{i}\"] = value\n",
    "df_mean_fl_by_data = df_mean_fl_by_data.sort_values(by=['fl_gamma']).reset_index(drop=True)\n",
    "df_mean_fl_by_data = df_mean_fl_by_data.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "df_mean_fl_by_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"fl\") & (df_all_mean_results[\"data_name\"] == \"ami\")\n",
    "df_mean_fl = df_all_mean_results[conditions][[\"data_name\", \"fl_gamma\", \"val_f1_macro\", \"val_f1_per_label_0\", \"val_f1_per_label_1\", \"val_f1_per_label_2\", \"val_f1_per_label_3\", \"val_f1_per_label_4\"]]\n",
    "df_mean_fl[\"data_name\"] = df_mean_fl[\"data_name\"].map(data_name_orig2display)\n",
    "\n",
    "fl_gamma_unique = df_mean_fl.fl_gamma.unique().tolist()\n",
    "data_name_unique = df_mean_fl.data_name.unique().tolist()\n",
    "df_mean_fl_by_data = pd.DataFrame()\n",
    "df_mean_fl_by_data[\"fl_gamma\"] = fl_gamma_unique\n",
    "for data_name in data_name_unique:\n",
    "    for fl_gamma in fl_gamma_unique:\n",
    "        conditions = (df_mean_fl[\"data_name\"] == data_name) & (df_mean_fl[\"fl_gamma\"] == fl_gamma)\n",
    "        if len(df_mean_fl[conditions]) > 0:\n",
    "            value = df_mean_fl.loc[conditions, \"val_f1_macro\"].values[0]\n",
    "            df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_macro_f1\"] = value\n",
    "            for i in range(5):\n",
    "                value = df_mean_fl.loc[conditions, f\"val_f1_per_label_{i}\"].values[0]\n",
    "                df_mean_fl_by_data.loc[df_mean_fl_by_data[\"fl_gamma\"] == fl_gamma, f\"{data_name}_f1_per_label_{i}\"] = value\n",
    "df_mean_fl_by_data = df_mean_fl_by_data.sort_values(by=['fl_gamma']).reset_index(drop=True)\n",
    "df_mean_fl_by_data = df_mean_fl_by_data.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "df_mean_fl_by_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"wfl\") & (df_all_mean_results[\"num_classes\"] == 2)\n",
    "df_mean_wfl = df_all_mean_results[conditions][[\"data_name\", \"fl_gamma\", \"wce_alpha\", \"test_f1_macro\"]]\n",
    "df_mean_wfl[\"data_name\"] = df_mean_wfl[\"data_name\"].map(data_name_orig2display)\n",
    "\n",
    "fl_gamma_unique = df_mean_wfl.fl_gamma.unique().tolist()\n",
    "wce_alpha_unique = [0.1, 0.25, 0.75, 0.9, 0.99]\n",
    "data_name_unique = df_mean_wfl.data_name.unique().tolist()\n",
    "df_mean_wfl_by_data = pd.DataFrame()\n",
    "df_mean_wfl_by_data[\"fl_gamma\"] = fl_gamma_unique * len(wce_alpha_unique)\n",
    "df_mean_wfl_by_data[\"wce_alpha\"] = sum([[alpha] * len(fl_gamma_unique) for alpha in wce_alpha_unique], [])\n",
    "for data_name in data_name_unique:\n",
    "    for fl_gamma in fl_gamma_unique:\n",
    "        for wce_alpha in wce_alpha_unique:\n",
    "            value_is_from_where = (df_mean_wfl[\"data_name\"] == data_name) & (df_mean_wfl[\"fl_gamma\"] == fl_gamma) & (df_mean_wfl[\"wce_alpha\"] == wce_alpha)\n",
    "            if len(df_mean_wfl[value_is_from_where]) > 0:\n",
    "                value = df_mean_wfl.loc[value_is_from_where, \"test_f1_macro\"].values[0]\n",
    "                where_to_assign = (df_mean_wfl_by_data[\"fl_gamma\"] == fl_gamma) & (df_mean_wfl_by_data[\"wce_alpha\"] == wce_alpha)\n",
    "                df_mean_wfl_by_data.loc[where_to_assign, f\"{data_name}_macro_f1\"] = value\n",
    "df_mean_wfl_by_data = df_mean_wfl_by_data.sort_values(by=['fl_gamma', 'wce_alpha']).reset_index(drop=True)\n",
    "df_mean_wfl_by_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_clear_fl = [\"us-election-2020\", \"cmsb-tsd\", \"founta-2018-thas\"]\n",
    "which_data_to_check = (((df_all_mean_results[\"variant\"] == \"fl\"))\n",
    "                        & (~df_all_mean_results[\"data_name\"].isin(data_noclear_rs))\n",
    "                        )\n",
    "df_mean_fl = df_all_mean_results[which_data_to_check][[\"data_name\", \"fl_gamma\", \"val_f1_macro\"]]\n",
    "def compute_deviation_to_baseline(row):\n",
    "    baseline_condition = (df_all_mean_results[\"data_name\"] == row[\"data_name\"]) & (df_all_mean_results[\"variant\"] == \"baseline\")\n",
    "    baseline_value = df_all_mean_results.loc[baseline_condition, \"val_f1_macro\"].values[0]\n",
    "    return row['val_f1_macro'] - baseline_value\n",
    "df_mean_fl[\"val_f1_macro_delta\"] = df_mean_fl.apply(lambda row: compute_deviation_to_baseline(row), axis=1)\n",
    "df_mean_fl[\"data_name\"] = df_mean_fl[\"data_name\"].map(data_name_orig2display)\n",
    "df_mean_fl = df_mean_fl.rename({\"data_name\":\"Dataset\"}, axis=1)\n",
    "\n",
    "rc = {\n",
    "    # 'figure.figsize':(5,4),\n",
    "    #   'axes.facecolor':'white',\n",
    "    #   'axes.grid' : True,\n",
    "      'grid.color': '.8',\n",
    "      'text.color': 'black',\n",
    "      'xtick.color': 'black',\n",
    "      'ytick.color': 'black',\n",
    "      'font.family':'Times New Roman',\n",
    "      'font.size' : 21}\n",
    "plt.rcParams.update(rc)\n",
    "# sns.set_style(\"whitegrid\")\n",
    "fig, ax = plt.subplots(figsize=(4.8,4.55))\n",
    "sns.lineplot(x=\"fl_gamma\", y=\"val_f1_macro_delta\", data=df_mean_fl, hue=\"Dataset\", marker=\"o\", ax=ax, palette=data2color)\n",
    "gamma_ticks = [0.1,0.2,0.5,1.0,2.0,5.0]\n",
    "ax.set_xticks(gamma_ticks, labels=gamma_ticks, rotation=90)\n",
    "ax.set_xlabel(r\"$\\gamma$\")\n",
    "# # delta_ticks = [y for y in list(range(-2, 8, 2))]\n",
    "# # ax.set_yticks(delta_ticks)\n",
    "\n",
    "custom_lines = [\n",
    "                Line2D([0], [0], color=sns.color_palette()[0], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Twitter-Hate-Speech\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[1], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Civil-Comments\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[4], lw=1, linestyle=\"solid\", marker=\"o\", label=\"CMSB\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[5], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Founta-2018\"),\n",
    "                Line2D([0], [0], color=sns.color_palette()[6], lw=1, linestyle=\"solid\", marker=\"o\", label=\"Davidson-2017\"),\n",
    "                ]\n",
    "ax.legend(handles=custom_lines, title=\"Dataset\", loc=\"best\")#, bbox_to_anchor=(1, 1.)\n",
    "\n",
    "ax.set_ylabel(r\"$\\delta_{FL}$\")\n",
    "# # '-', '--', '-.', ':', 'None', ' ', '', 'solid', 'dashed', 'dashdot', 'dotted'\n",
    "ax.axhline(y=0, linewidth=0.8, color=\"black\", ls=\"-.\")\n",
    "# sns.move_legend(ax, \"upper right\", bbox_to_anchor=(.93, 1)) #\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig('FL_trend.svg', dpi=600, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why are some data not good in FL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if unpromising results in fl is because the negative class was improved\n",
    "data_not_good_in_fl = [\"us-election-2020\", \"waseem-and-hovy-2016\"]\n",
    "relevant_cols = [\"data_name\", \"fl_gamma\", \"val_f1_macro\", \"test_f1_macro\", \n",
    "                    \"train_f1_per_label_0\", \"train_f1_per_label_1\", \n",
    "                    \"val_f1_per_label_0\", \"val_f1_per_label_1\", \n",
    "                    \"test_f1_per_label_0\", \"test_f1_per_label_1\"]\n",
    "which_row_to_check = (df_all_mean_results[\"variant\"] == \"fl\") & (df_all_mean_results[\"data_name\"].isin(data_not_good_in_fl))\n",
    "df_all_mean_results[which_row_to_check][relevant_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if unpromising results in fl is because the negative class was improved\n",
    "data_good_in_fl = [\"founta-2018-thas\", \"ami\"]\n",
    "relevant_cols = [\"data_name\", \"fl_gamma\", \"val_f1_macro\", \"test_f1_macro\", \n",
    "                    \"train_f1_per_label_0\", \"train_f1_per_label_1\", \"train_f1_per_label_2\", \"train_f1_per_label_3\", \"train_f1_per_label_4\",\n",
    "                    \"val_f1_per_label_0\", \"val_f1_per_label_1\", \"val_f1_per_label_2\", \"val_f1_per_label_3\", \"val_f1_per_label_4\",\n",
    "                    \"test_f1_per_label_0\", \"test_f1_per_label_1\", \"test_f1_per_label_2\", \"test_f1_per_label_3\", \"test_f1_per_label_4\"]\n",
    "which_row_to_check = (df_all_mean_results[\"variant\"] == \"fl\") & (df_all_mean_results[\"data_name\"].isin(data_good_in_fl))\n",
    "df_all_mean_results[which_row_to_check][relevant_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"wfl\")\n",
    "df_mean_wfl = df_all_mean_results[conditions][[\"data_name\", \"wce_alpha\", \"fl_gamma\", \"val_f1_macro\"]]\n",
    "df_mean_wfl[\"data_name\"] = df_mean_wfl[\"data_name\"].map(data_name_orig2display)\n",
    "df_mean_wfl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = (df_all_mean_results[\"variant\"] == \"augmentation_bert\")\n",
    "# data_not_good_in_fl = [\"us-election-2020\", \"waseem-and-hovy-2016\"]\n",
    "relevant_cols = [\"data_name\", \"augmentation_rho\", \"augmentation_percentage\", \"augmentation_top_k\", \"val_f1_macro\", \"test_f1_macro\", \n",
    "                    \"train_f1_per_label_0\", \"train_f1_per_label_1\", \n",
    "                    \"val_f1_per_label_0\", \"val_f1_per_label_1\", \n",
    "                    \"test_f1_per_label_0\", \"test_f1_per_label_1\"]\n",
    "df_mean_aug_bert = df_all_mean_results[conditions][relevant_cols]\n",
    "# df_mean_aug[\"data_name\"] = df_mean_aug[\"data_name\"].map(data_name_orig2display)\n",
    "df_mean_aug_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_mean_results[df_all_mean_results[\"variant\"].isin([\"sampling_modifiedRS_oversampling\", \"augmentation_external_data\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_check = data_names[:-1]\n",
    "conditions = (df_all_mean_results[\"variant\"].isin([\"sampling_modifiedRS_oversampling\", \"augmentation_external_data\"]) \n",
    "                & df_all_mean_results[\"data_name\"].isin(data_to_check))\n",
    "relevant_cols = [\"data_name\", \"variant\", \"sampling_modifiedRS_rho\", \"augmentation_rho\", \"val_f1_macro\", \"test_f1_macro\", \n",
    "                    \"train_f1_per_label_0\", \"train_f1_per_label_1\", \n",
    "                    \"val_f1_per_label_0\", \"val_f1_per_label_1\", \n",
    "                    \"test_f1_per_label_0\", \"test_f1_per_label_1\"]\n",
    "df_mean_aug_ext = df_all_mean_results[conditions][relevant_cols]\n",
    "def get_rho(row):\n",
    "    if row[\"sampling_modifiedRS_rho\"] != \"-\":\n",
    "        return row[\"sampling_modifiedRS_rho\"]\n",
    "    if row[\"augmentation_rho\"] != \"-\":\n",
    "        return row[\"augmentation_rho\"]\n",
    "df_mean_aug_ext[\"rho\"] = df_mean_aug_ext.apply(lambda row: get_rho(row), axis=1)\n",
    "df_mean_aug_ext[\"data_name\"] = df_mean_aug_ext[\"data_name\"].map(data_name_orig2display)\n",
    "df_mean_aug_ext = df_mean_aug_ext[[\"data_name\", \"variant\", \"rho\"] + relevant_cols[4:]]\n",
    "df_mean_aug_ext = df_mean_aug_ext.sort_values(by=['data_name', 'rho']).reset_index(drop=True)\n",
    "df_mean_aug_ext.to_csv(\"check_external_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
