/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning_utilities/core/imports.py:14: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
  import pkg_resources
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(pkg)
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning_fabric/__init__.py:40: Deprecated call to `pkg_resources.declare_namespace('lightning_fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/pytorch_lightning/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('pytorch_lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning/fabric/__init__.py:40: Deprecated call to `pkg_resources.declare_namespace('lightning.fabric')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(parent)
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning/pytorch/__init__.py:37: Deprecated call to `pkg_resources.declare_namespace('lightning.pytorch')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('lightning')`.
Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages
  declare_namespace(parent)
Seed set to 0
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  torch.utils._pytree._register_pytree_node(
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python imbalanced_text_classification/main.py --data_name u ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]

  | Name               | Type                          | Params
---------------------------------------------------------------------
0 | classifier         | BertForSequenceClassification | 109 M 
1 | loss_fn            | CrossEntropyLoss              | 0     
2 | train_metrics      | MetricCollection              | 0     
3 | train_f1_per_label | MulticlassF1Score             | 0     
4 | val_metrics        | MetricCollection              | 0     
5 | val_f1_per_label   | MulticlassF1Score             | 0     
6 | test_metrics       | MetricCollection              | 0     
7 | test_f1_per_label  | MulticlassF1Score             | 0     
---------------------------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.935   Total estimated model params size (MB)
INFO: 'imbalalanced_text_classification_us-election-2020' does not exist. Creating a new experiment

MLFlow run_name = sampling_modifiedRS-us-election-2020-bin-Trial-0
MLflow Saved Child Search Trial 0 Log in './mlruns/5/2b5f3a3a0eb3455ab8f1139d0a6fbca3'
Using Cross Entropy Loss: alpha=None
Runing optuna for hyperparameter search:
Parmas: {'sampling_modifiedRS_mode': 'oversampling', 'sampling_modifiedRS_rho': 5, 'sampling_weightedRS_percentage': None, 'wce_alpha': None, 'fl_gamma': None, 'loss': <Loss.CE_Loss: 'CE_Loss'>}
Calling ImbalancedDataModule.setup() for train...
------ Dataset Statistics ------
Raw data reading from CSV files:
| Split	 | Size	 | Label Counts
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
| train_val	 | 2400 |	0: 2107, 87.8%	1: 293, 12.2%
After splitting for training:
| Split	 | Size	 | Label Counts
| train	 | 1800 |	0: 1580, 87.8%	1: 220, 12.2%
| val	 | 600 |	0: 527, 87.8%	1: 73, 12.2%
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
Resampling with sampling_modifiedRS_rho = 5 => training set has 1896.0 samples.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2026457) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Label counts after resampling: {0: 1580, 1: 316}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] Epoch 0: |          | 1/? [00:01<00:00,  0.84it/s]Epoch 0: |          | 1/? [00:01<00:00,  0.84it/s, v_num=bca3]Epoch 0: |          | 2/? [00:01<00:00,  1.05it/s, v_num=bca3]Epoch 0: |          | 2/? [00:01<00:00,  1.05it/s, v_num=bca3]Epoch 0: |          | 3/? [00:02<00:00,  1.14it/s, v_num=bca3]Epoch 0: |          | 3/? [00:02<00:00,  1.14it/s, v_num=bca3]Epoch 0: |          | 4/? [00:03<00:00,  1.19it/s, v_num=bca3]Epoch 0: |          | 4/? [00:03<00:00,  1.19it/s, v_num=bca3]Epoch 0: |          | 5/? [00:04<00:00,  1.23it/s, v_num=bca3]Epoch 0: |          | 5/? [00:04<00:00,  1.23it/s, v_num=bca3]Epoch 0: |          | 6/? [00:04<00:00,  1.25it/s, v_num=bca3]Epoch 0: |          | 6/? [00:04<00:00,  1.25it/s, v_num=bca3]Epoch 0: |          | 7/? [00:05<00:00,  1.26it/s, v_num=bca3]Epoch 0: |          | 7/? [00:05<00:00,  1.26it/s, v_num=bca3]Epoch 0: |          | 8/? [00:06<00:00,  1.28it/s, v_num=bca3]Epoch 0: |          | 8/? [00:06<00:00,  1.28it/s, v_num=bca3]Epoch 0: |          | 9/? [00:06<00:00,  1.29it/s, v_num=bca3]Epoch 0: |          | 9/? [00:06<00:00,  1.29it/s, v_num=bca3]Epoch 0: |          | 10/? [00:07<00:00,  1.29it/s, v_num=bca3]Epoch 0: |          | 10/? [00:07<00:00,  1.29it/s, v_num=bca3]Epoch 0: |          | 11/? [00:08<00:00,  1.30it/s, v_num=bca3]Epoch 0: |          | 11/? [00:08<00:00,  1.30it/s, v_num=bca3]Epoch 0: |          | 12/? [00:09<00:00,  1.31it/s, v_num=bca3]Epoch 0: |          | 12/? [00:09<00:00,  1.31it/s, v_num=bca3]Epoch 0: |          | 13/? [00:09<00:00,  1.31it/s, v_num=bca3]Epoch 0: |          | 13/? [00:09<00:00,  1.31it/s, v_num=bca3]Epoch 0: |          | 14/? [00:10<00:00,  1.32it/s, v_num=bca3]Epoch 0: |          | 14/? [00:10<00:00,  1.32it/s, v_num=bca3]Epoch 0: |          | 15/? [00:11<00:00,  1.32it/s, v_num=bca3]Epoch 0: |          | 15/? [00:11<00:00,  1.32it/s, v_num=bca3]Epoch 0: |          | 16/? [00:12<00:00,  1.32it/s, v_num=bca3]Epoch 0: |          | 16/? [00:12<00:00,  1.32it/s, v_num=bca3]Epoch 0: |          | 17/? [00:12<00:00,  1.33it/s, v_num=bca3]Epoch 0: |          | 17/? [00:12<00:00,  1.33it/s, v_num=bca3]Epoch 0: |          | 18/? [00:13<00:00,  1.33it/s, v_num=bca3]Epoch 0: |          | 18/? [00:13<00:00,  1.33it/s, v_num=bca3]Epoch 0: |          | 19/? [00:14<00:00,  1.33it/s, v_num=bca3]Epoch 0: |          | 19/? [00:14<00:00,  1.33it/s, v_num=bca3]Epoch 0: |          | 20/? [00:14<00:00,  1.33it/s, v_num=bca3]Epoch 0: |          | 20/? [00:14<00:00,  1.33it/s, v_num=bca3]Epoch 0: |          | 21/? [00:15<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 21/? [00:15<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 22/? [00:16<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 22/? [00:16<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 23/? [00:17<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 23/? [00:17<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 24/? [00:17<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 24/? [00:17<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 25/? [00:18<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 25/? [00:18<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 26/? [00:19<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 26/? [00:19<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 27/? [00:20<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 27/? [00:20<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 28/? [00:20<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 28/? [00:20<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 29/? [00:21<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 29/? [00:21<00:00,  1.34it/s, v_num=bca3]Epoch 0: |          | 30/? [00:22<00:00,  1.35it/s, v_num=bca3]Epoch 0: |          | 30/? [00:22<00:00,  1.35it/s, v_num=bca3]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.02it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028

                                                                        [AEpoch 0: |          | 30/? [00:26<00:00,  1.11it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000]Epoch 0: |          | 30/? [00:26<00:00,  1.11it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000]Epoch 0: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000]         Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 1: |          | 1/? [00:02<00:00,  0.47it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000]Epoch 1: |          | 1/? [00:02<00:00,  0.47it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 2/? [00:02<00:00,  0.70it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 2/? [00:02<00:00,  0.70it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 3/? [00:03<00:00,  0.83it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 3/? [00:03<00:00,  0.83it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 4/? [00:04<00:00,  0.92it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 4/? [00:04<00:00,  0.92it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 5/? [00:05<00:00,  0.98it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 5/? [00:05<00:00,  0.98it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 6/? [00:05<00:00,  1.03it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 6/? [00:05<00:00,  1.03it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 7/? [00:06<00:00,  1.07it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 7/? [00:06<00:00,  1.07it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 8/? [00:07<00:00,  1.10it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 8/? [00:07<00:00,  1.10it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 9/? [00:07<00:00,  1.13it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 9/? [00:07<00:00,  1.13it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 10/? [00:08<00:00,  1.15it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 10/? [00:08<00:00,  1.15it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 11/? [00:09<00:00,  1.16it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 11/? [00:09<00:00,  1.16it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 12/? [00:10<00:00,  1.18it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 12/? [00:10<00:00,  1.18it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 13/? [00:10<00:00,  1.19it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 13/? [00:10<00:00,  1.19it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 14/? [00:11<00:00,  1.20it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 14/? [00:11<00:00,  1.20it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 15/? [00:12<00:00,  1.21it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 15/? [00:12<00:00,  1.21it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 16/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 16/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 17/? [00:13<00:00,  1.23it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 17/? [00:13<00:00,  1.23it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 18/? [00:14<00:00,  1.24it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 18/? [00:14<00:00,  1.24it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 19/? [00:15<00:00,  1.24it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 19/? [00:15<00:00,  1.24it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 20/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 20/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 22/? [00:17<00:00,  1.26it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 22/? [00:17<00:00,  1.26it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 24/? [00:18<00:00,  1.27it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 24/? [00:18<00:00,  1.27it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 25/? [00:19<00:00,  1.27it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 25/? [00:19<00:00,  1.27it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 27/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 27/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 28/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 28/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 30/? [00:23<00:00,  1.30it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 30/? [00:23<00:00,  1.30it/s, v_num=bca3, val_loss=0.372, val_accuracy=0.878, val_auprc=0.589, val_f1_macro=0.468, val_f1_weighted=0.821, val_precision_macro=0.439, val_precision_weighted=0.771, val_recall_macro=0.500, val_recall_weighted=0.878, val_f1_per_label_0=0.935, val_f1_per_label_1=0.000, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 1: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]         Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 2: |          | 1/? [00:02<00:00,  0.45it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.477, train_accuracy=0.810, train_auprc=0.501, train_f1_macro=0.471, train_f1_weighted=0.753, train_precision_macro=0.488, train_precision_weighted=0.718, train_recall_macro=0.497, train_recall_weighted=0.810, train_f1_per_label_0=0.894, train_f1_per_label_1=0.0475]Epoch 2: |          | 1/? [00:02<00:00,  0.45it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 2/? [00:02<00:00,  0.68it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 2/? [00:02<00:00,  0.68it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 3/? [00:03<00:00,  0.81it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 3/? [00:03<00:00,  0.81it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 4/? [00:04<00:00,  0.90it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 4/? [00:04<00:00,  0.90it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 5/? [00:05<00:00,  0.97it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 5/? [00:05<00:00,  0.97it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 6/? [00:05<00:00,  1.02it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 6/? [00:05<00:00,  1.02it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 7/? [00:06<00:00,  1.06it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 7/? [00:06<00:00,  1.06it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 8/? [00:07<00:00,  1.09it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 8/? [00:07<00:00,  1.09it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 9/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 9/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 10/? [00:08<00:00,  1.13it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 10/? [00:08<00:00,  1.13it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 11/? [00:09<00:00,  1.15it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 11/? [00:09<00:00,  1.15it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 13/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 13/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 14/? [00:11<00:00,  1.19it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 14/? [00:11<00:00,  1.19it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 15/? [00:12<00:00,  1.20it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 15/? [00:12<00:00,  1.20it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 16/? [00:13<00:00,  1.21it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 16/? [00:13<00:00,  1.21it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 17/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 17/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 19/? [00:15<00:00,  1.24it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 19/? [00:15<00:00,  1.24it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 25/? [00:19<00:00,  1.27it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 25/? [00:19<00:00,  1.27it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 28/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 28/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.307, val_accuracy=0.882, val_auprc=0.670, val_f1_macro=0.530, val_f1_weighted=0.838, val_precision_macro=0.755, val_precision_weighted=0.853, val_recall_macro=0.531, val_recall_weighted=0.882, val_f1_per_label_0=0.937, val_f1_per_label_1=0.123, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.13it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.13it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.33it/s][A
                                                                        [AEpoch 2: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]         Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 3: |          | 1/? [00:02<00:00,  0.44it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.424, train_accuracy=0.834, train_auprc=0.596, train_f1_macro=0.458, train_f1_weighted=0.759, train_precision_macro=0.917, train_precision_weighted=0.861, train_recall_macro=0.502, train_recall_weighted=0.834, train_f1_per_label_0=0.909, train_f1_per_label_1=0.00631]Epoch 3: |          | 1/? [00:02<00:00,  0.44it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]  Epoch 3: |          | 2/? [00:03<00:00,  0.66it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 2/? [00:03<00:00,  0.66it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 3/? [00:03<00:00,  0.79it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 3/? [00:03<00:00,  0.79it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 4/? [00:04<00:00,  0.88it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 4/? [00:04<00:00,  0.88it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 5/? [00:05<00:00,  0.95it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 5/? [00:05<00:00,  0.95it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 6/? [00:06<00:00,  0.99it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 6/? [00:06<00:00,  0.99it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 7/? [00:06<00:00,  1.03it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 7/? [00:06<00:00,  1.03it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 8/? [00:07<00:00,  1.06it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 8/? [00:07<00:00,  1.06it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 9/? [00:08<00:00,  1.09it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 9/? [00:08<00:00,  1.09it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 10/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 10/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 11/? [00:09<00:00,  1.13it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 11/? [00:09<00:00,  1.13it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 12/? [00:10<00:00,  1.15it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 12/? [00:10<00:00,  1.15it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 13/? [00:11<00:00,  1.16it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 13/? [00:11<00:00,  1.16it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 14/? [00:11<00:00,  1.17it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 14/? [00:11<00:00,  1.17it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 15/? [00:12<00:00,  1.18it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 15/? [00:12<00:00,  1.18it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 16/? [00:13<00:00,  1.19it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 16/? [00:13<00:00,  1.19it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 17/? [00:14<00:00,  1.20it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 17/? [00:14<00:00,  1.20it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 18/? [00:14<00:00,  1.21it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 18/? [00:14<00:00,  1.21it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 19/? [00:15<00:00,  1.22it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 19/? [00:15<00:00,  1.22it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 20/? [00:16<00:00,  1.22it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 20/? [00:16<00:00,  1.22it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 21/? [00:17<00:00,  1.23it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 21/? [00:17<00:00,  1.23it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 22/? [00:17<00:00,  1.23it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 22/? [00:17<00:00,  1.23it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 23/? [00:18<00:00,  1.24it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 23/? [00:18<00:00,  1.24it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 24/? [00:19<00:00,  1.25it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 24/? [00:19<00:00,  1.25it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 25/? [00:20<00:00,  1.25it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 25/? [00:20<00:00,  1.25it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 26/? [00:20<00:00,  1.25it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 26/? [00:20<00:00,  1.25it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 27/? [00:21<00:00,  1.26it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 27/? [00:21<00:00,  1.26it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 28/? [00:22<00:00,  1.26it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 28/? [00:22<00:00,  1.26it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 29/? [00:22<00:00,  1.26it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 29/? [00:22<00:00,  1.26it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 30/? [00:23<00:00,  1.28it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 30/? [00:23<00:00,  1.28it/s, v_num=bca3, val_loss=0.326, val_accuracy=0.850, val_auprc=0.816, val_f1_macro=0.734, val_f1_weighted=0.867, val_precision_macro=0.700, val_precision_weighted=0.901, val_recall_macro=0.820, val_recall_weighted=0.850, val_f1_per_label_0=0.910, val_f1_per_label_1=0.559, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s][A
                                                                        [AEpoch 3: |          | 30/? [00:28<00:00,  1.06it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 30/? [00:28<00:00,  1.06it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]         Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 4: |          | 1/? [00:02<00:00,  0.44it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.268, train_accuracy=0.892, train_auprc=0.815, train_f1_macro=0.796, train_f1_weighted=0.889, train_precision_macro=0.812, train_precision_weighted=0.887, train_recall_macro=0.782, train_recall_weighted=0.892, train_f1_per_label_0=0.936, train_f1_per_label_1=0.655]Epoch 4: |          | 1/? [00:02<00:00,  0.44it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 2/? [00:02<00:00,  0.67it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 2/? [00:02<00:00,  0.67it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 3/? [00:03<00:00,  0.81it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 3/? [00:03<00:00,  0.81it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 4/? [00:04<00:00,  0.90it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 4/? [00:04<00:00,  0.90it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 5/? [00:05<00:00,  0.96it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 5/? [00:05<00:00,  0.96it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 6/? [00:05<00:00,  1.01it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 6/? [00:05<00:00,  1.01it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 7/? [00:06<00:00,  1.05it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 7/? [00:06<00:00,  1.05it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 8/? [00:07<00:00,  1.08it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 8/? [00:07<00:00,  1.08it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 9/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 9/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 10/? [00:08<00:00,  1.13it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 10/? [00:08<00:00,  1.13it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 11/? [00:09<00:00,  1.15it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 11/? [00:09<00:00,  1.15it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 13/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 13/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 14/? [00:11<00:00,  1.19it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 14/? [00:11<00:00,  1.19it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 15/? [00:12<00:00,  1.20it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 15/? [00:12<00:00,  1.20it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 16/? [00:13<00:00,  1.21it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 16/? [00:13<00:00,  1.21it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 17/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 17/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 19/? [00:15<00:00,  1.23it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 19/? [00:15<00:00,  1.23it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 25/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 25/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 28/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 28/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.289, val_accuracy=0.898, val_auprc=0.825, val_f1_macro=0.745, val_f1_weighted=0.895, val_precision_macro=0.765, val_precision_weighted=0.892, val_recall_macro=0.730, val_recall_weighted=0.898, val_f1_per_label_0=0.943, val_f1_per_label_1=0.548, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.13it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.10it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 4: |          | 30/? [00:27<00:00,  1.07it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 30/? [00:27<00:00,  1.07it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]         Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 5: |          | 1/? [00:02<00:00,  0.44it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.180, train_accuracy=0.944, train_auprc=0.914, train_f1_macro=0.898, train_f1_weighted=0.944, train_precision_macro=0.903, train_precision_weighted=0.943, train_recall_macro=0.893, train_recall_weighted=0.944, train_f1_per_label_0=0.967, train_f1_per_label_1=0.830]Epoch 5: |          | 1/? [00:02<00:00,  0.44it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 2/? [00:02<00:00,  0.67it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 2/? [00:02<00:00,  0.67it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 3/? [00:03<00:00,  0.80it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 3/? [00:03<00:00,  0.80it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 4/? [00:04<00:00,  0.90it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 4/? [00:04<00:00,  0.90it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 5/? [00:05<00:00,  0.96it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 5/? [00:05<00:00,  0.96it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 6/? [00:05<00:00,  1.02it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 6/? [00:05<00:00,  1.02it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 7/? [00:06<00:00,  1.06it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 7/? [00:06<00:00,  1.06it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 8/? [00:07<00:00,  1.09it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 8/? [00:07<00:00,  1.09it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 9/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 9/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 10/? [00:08<00:00,  1.13it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 10/? [00:08<00:00,  1.13it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 11/? [00:09<00:00,  1.15it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 11/? [00:09<00:00,  1.15it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 13/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 13/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 14/? [00:11<00:00,  1.19it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 14/? [00:11<00:00,  1.19it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 15/? [00:12<00:00,  1.20it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 15/? [00:12<00:00,  1.20it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 16/? [00:13<00:00,  1.21it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 16/? [00:13<00:00,  1.21it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 17/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 17/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 19/? [00:15<00:00,  1.23it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 19/? [00:15<00:00,  1.23it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 21/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 21/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 23/? [00:18<00:00,  1.25it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 23/? [00:18<00:00,  1.25it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 25/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 25/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 28/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 28/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.276, val_accuracy=0.903, val_auprc=0.807, val_f1_macro=0.756, val_f1_weighted=0.900, val_precision_macro=0.779, val_precision_weighted=0.897, val_recall_macro=0.738, val_recall_weighted=0.903, val_f1_per_label_0=0.946, val_f1_per_label_1=0.567, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s][A
                                                                        [AEpoch 5: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]         Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 6: |          | 1/? [00:02<00:00,  0.45it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.0727, train_accuracy=0.977, train_auprc=0.983, train_f1_macro=0.959, train_f1_weighted=0.977, train_precision_macro=0.955, train_precision_weighted=0.977, train_recall_macro=0.962, train_recall_weighted=0.977, train_f1_per_label_0=0.986, train_f1_per_label_1=0.931]Epoch 6: |          | 1/? [00:02<00:00,  0.45it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973] Epoch 6: |          | 2/? [00:02<00:00,  0.68it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 2/? [00:02<00:00,  0.68it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 3/? [00:03<00:00,  0.81it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 3/? [00:03<00:00,  0.81it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 4/? [00:04<00:00,  0.91it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 4/? [00:04<00:00,  0.91it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 5/? [00:05<00:00,  0.97it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 5/? [00:05<00:00,  0.97it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 6/? [00:05<00:00,  1.02it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 6/? [00:05<00:00,  1.02it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 7/? [00:06<00:00,  1.06it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 7/? [00:06<00:00,  1.06it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 8/? [00:07<00:00,  1.09it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 8/? [00:07<00:00,  1.09it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 9/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 9/? [00:08<00:00,  1.11it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 10/? [00:08<00:00,  1.14it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 10/? [00:08<00:00,  1.14it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 11/? [00:09<00:00,  1.15it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 11/? [00:09<00:00,  1.15it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 13/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 13/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 14/? [00:11<00:00,  1.19it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 14/? [00:11<00:00,  1.19it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 15/? [00:12<00:00,  1.20it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 15/? [00:12<00:00,  1.20it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 16/? [00:13<00:00,  1.21it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 16/? [00:13<00:00,  1.21it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 17/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 17/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 19/? [00:15<00:00,  1.23it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 19/? [00:15<00:00,  1.23it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 21/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 21/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 23/? [00:18<00:00,  1.25it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 23/? [00:18<00:00,  1.25it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 25/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 25/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 26/? [00:20<00:00,  1.26it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 26/? [00:20<00:00,  1.26it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 28/? [00:22<00:00,  1.27it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 28/? [00:22<00:00,  1.27it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 29/? [00:22<00:00,  1.27it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 29/? [00:22<00:00,  1.27it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.297, val_accuracy=0.910, val_auprc=0.812, val_f1_macro=0.779, val_f1_weighted=0.908, val_precision_macro=0.794, val_precision_weighted=0.906, val_recall_macro=0.766, val_recall_weighted=0.910, val_f1_per_label_0=0.949, val_f1_per_label_1=0.609, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.21it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.12it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.12it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.33it/s][A
                                                                        [AEpoch 6: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]         Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 7: |          | 1/? [00:02<00:00,  0.45it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.045, train_accuracy=0.991, train_auprc=0.988, train_f1_macro=0.984, train_f1_weighted=0.991, train_precision_macro=0.987, train_precision_weighted=0.991, train_recall_macro=0.981, train_recall_weighted=0.991, train_f1_per_label_0=0.995, train_f1_per_label_1=0.973]Epoch 7: |          | 1/? [00:02<00:00,  0.45it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 2/? [00:02<00:00,  0.68it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 2/? [00:02<00:00,  0.68it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 3/? [00:03<00:00,  0.82it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 3/? [00:03<00:00,  0.82it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 4/? [00:04<00:00,  0.91it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 4/? [00:04<00:00,  0.91it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 5/? [00:05<00:00,  0.97it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 5/? [00:05<00:00,  0.97it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 6/? [00:05<00:00,  1.02it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 6/? [00:05<00:00,  1.02it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 7/? [00:06<00:00,  1.06it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 7/? [00:06<00:00,  1.06it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 8/? [00:07<00:00,  1.09it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 8/? [00:07<00:00,  1.09it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 9/? [00:08<00:00,  1.12it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 9/? [00:08<00:00,  1.12it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 10/? [00:08<00:00,  1.14it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 10/? [00:08<00:00,  1.14it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 11/? [00:09<00:00,  1.16it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 11/? [00:09<00:00,  1.16it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 12/? [00:10<00:00,  1.17it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 13/? [00:10<00:00,  1.19it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 13/? [00:10<00:00,  1.19it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 14/? [00:11<00:00,  1.20it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 14/? [00:11<00:00,  1.20it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 15/? [00:12<00:00,  1.21it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 15/? [00:12<00:00,  1.21it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 16/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 16/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 17/? [00:13<00:00,  1.23it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 17/? [00:13<00:00,  1.23it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 19/? [00:15<00:00,  1.24it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 19/? [00:15<00:00,  1.24it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 20/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 20/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 22/? [00:17<00:00,  1.26it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 22/? [00:17<00:00,  1.26it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 24/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 24/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 25/? [00:19<00:00,  1.27it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 25/? [00:19<00:00,  1.27it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 28/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 28/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.274, val_accuracy=0.922, val_auprc=0.850, val_f1_macro=0.796, val_f1_weighted=0.917, val_precision_macro=0.837, val_precision_weighted=0.916, val_recall_macro=0.767, val_recall_weighted=0.922, val_f1_per_label_0=0.956, val_f1_per_label_1=0.636, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.10it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 7: |          | 30/? [00:27<00:00,  1.07it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 30/? [00:27<00:00,  1.07it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]         Epoch 8: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 8: |          | 1/? [00:02<00:00,  0.49it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.994, train_f1_macro=0.986, train_f1_weighted=0.992, train_precision_macro=0.989, train_precision_weighted=0.992, train_recall_macro=0.983, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.976]Epoch 8: |          | 1/? [00:02<00:00,  0.48it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 2/? [00:02<00:00,  0.72it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 2/? [00:02<00:00,  0.72it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 3/? [00:03<00:00,  0.84it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 3/? [00:03<00:00,  0.84it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 4/? [00:04<00:00,  0.93it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 4/? [00:04<00:00,  0.93it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 5/? [00:05<00:00,  0.99it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 5/? [00:05<00:00,  0.99it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 6/? [00:05<00:00,  1.04it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 6/? [00:05<00:00,  1.04it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 7/? [00:06<00:00,  1.07it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 7/? [00:06<00:00,  1.07it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 8/? [00:07<00:00,  1.10it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 8/? [00:07<00:00,  1.10it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 9/? [00:08<00:00,  1.12it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 9/? [00:08<00:00,  1.12it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 10/? [00:08<00:00,  1.14it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 10/? [00:08<00:00,  1.14it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 11/? [00:09<00:00,  1.16it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 11/? [00:09<00:00,  1.16it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 12/? [00:10<00:00,  1.18it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 12/? [00:10<00:00,  1.18it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 13/? [00:10<00:00,  1.19it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 13/? [00:10<00:00,  1.19it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 14/? [00:11<00:00,  1.20it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 14/? [00:11<00:00,  1.20it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 15/? [00:12<00:00,  1.21it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 15/? [00:12<00:00,  1.21it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 16/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 16/? [00:13<00:00,  1.22it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 17/? [00:13<00:00,  1.23it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 17/? [00:13<00:00,  1.23it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 18/? [00:14<00:00,  1.23it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 19/? [00:15<00:00,  1.24it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 19/? [00:15<00:00,  1.24it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 20/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 21/? [00:16<00:00,  1.25it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 22/? [00:17<00:00,  1.25it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 23/? [00:18<00:00,  1.26it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 24/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 25/? [00:19<00:00,  1.27it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 25/? [00:19<00:00,  1.27it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 26/? [00:20<00:00,  1.27it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 27/? [00:21<00:00,  1.27it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 28/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 28/? [00:21<00:00,  1.28it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 29/? [00:22<00:00,  1.28it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 30/? [00:23<00:00,  1.29it/s, v_num=bca3, val_loss=0.356, val_accuracy=0.918, val_auprc=0.823, val_f1_macro=0.771, val_f1_weighted=0.910, val_precision_macro=0.846, val_precision_weighted=0.911, val_recall_macro=0.729, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.588, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.12it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.12it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.33it/s][A
                                                                        [AEpoch 8: |          | 30/? [00:27<00:00,  1.07it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 30/? [00:27<00:00,  1.07it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 8: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]         Epoch 9: |          | 0/? [00:00<?, ?it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 5
Epoch 9: |          | 1/? [00:02<00:00,  0.45it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.0134, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.994, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.993, train_recall_weighted=0.997, train_f1_per_label_0=0.998, train_f1_per_label_1=0.990]Epoch 9: |          | 1/? [00:02<00:00,  0.45it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 2/? [00:02<00:00,  0.67it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 2/? [00:02<00:00,  0.67it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 3/? [00:03<00:00,  0.81it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 3/? [00:03<00:00,  0.81it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 4/? [00:04<00:00,  0.90it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 4/? [00:04<00:00,  0.90it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 5/? [00:05<00:00,  0.96it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 5/? [00:05<00:00,  0.96it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 6/? [00:05<00:00,  1.01it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 6/? [00:05<00:00,  1.01it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 7/? [00:06<00:00,  1.05it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 7/? [00:06<00:00,  1.05it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 8/? [00:07<00:00,  1.07it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 8/? [00:07<00:00,  1.07it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 9/? [00:08<00:00,  1.10it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 9/? [00:08<00:00,  1.10it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 10/? [00:08<00:00,  1.12it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 10/? [00:08<00:00,  1.12it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 11/? [00:09<00:00,  1.14it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 11/? [00:09<00:00,  1.14it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 12/? [00:10<00:00,  1.15it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 12/? [00:10<00:00,  1.15it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 13/? [00:11<00:00,  1.17it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 13/? [00:11<00:00,  1.17it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 14/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 14/? [00:11<00:00,  1.18it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 15/? [00:12<00:00,  1.19it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 15/? [00:12<00:00,  1.19it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 16/? [00:13<00:00,  1.20it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 16/? [00:13<00:00,  1.20it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 17/? [00:14<00:00,  1.21it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 17/? [00:14<00:00,  1.21it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 18/? [00:14<00:00,  1.22it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 18/? [00:14<00:00,  1.22it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 19/? [00:15<00:00,  1.22it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 19/? [00:15<00:00,  1.22it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 20/? [00:16<00:00,  1.23it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 20/? [00:16<00:00,  1.23it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 21/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 21/? [00:16<00:00,  1.24it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 22/? [00:17<00:00,  1.24it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 22/? [00:17<00:00,  1.24it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 23/? [00:18<00:00,  1.25it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 23/? [00:18<00:00,  1.25it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 24/? [00:19<00:00,  1.25it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 24/? [00:19<00:00,  1.25it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 25/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 25/? [00:19<00:00,  1.26it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 26/? [00:20<00:00,  1.26it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 26/? [00:20<00:00,  1.26it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 27/? [00:21<00:00,  1.26it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 27/? [00:21<00:00,  1.26it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 28/? [00:22<00:00,  1.27it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 28/? [00:22<00:00,  1.27it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 29/? [00:22<00:00,  1.27it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 29/? [00:22<00:00,  1.27it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 30/? [00:23<00:00,  1.28it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 30/? [00:23<00:00,  1.28it/s, v_num=bca3, val_loss=0.429, val_accuracy=0.923, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.873, val_precision_weighted=0.917, val_recall_macro=0.732, val_recall_weighted=0.923, val_f1_per_label_0=0.958, val_f1_per_label_1=0.603, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.00it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.00it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.02it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.02it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.03it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.04it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.05it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.05it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.26it/s][A
                                                                        [AEpoch 9: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.550, val_accuracy=0.902, val_auprc=0.818, val_f1_macro=0.663, val_f1_weighted=0.878, val_precision_macro=0.862, val_precision_weighted=0.894, val_recall_macro=0.619, val_recall_weighted=0.902, val_f1_per_label_0=0.947, val_f1_per_label_1=0.379, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Epoch 9: |          | 30/? [00:28<00:00,  1.07it/s, v_num=bca3, val_loss=0.550, val_accuracy=0.902, val_auprc=0.818, val_f1_macro=0.663, val_f1_weighted=0.878, val_precision_macro=0.862, val_precision_weighted=0.894, val_recall_macro=0.619, val_recall_weighted=0.902, val_f1_per_label_0=0.947, val_f1_per_label_1=0.379, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]`Trainer.fit` stopped: `max_epochs=10` reached.
Epoch 9: |          | 30/? [00:28<00:00,  1.05it/s, v_num=bca3, val_loss=0.550, val_accuracy=0.902, val_auprc=0.818, val_f1_macro=0.663, val_f1_weighted=0.878, val_precision_macro=0.862, val_precision_weighted=0.894, val_recall_macro=0.619, val_recall_weighted=0.902, val_f1_per_label_0=0.947, val_f1_per_label_1=0.379, train_loss=0.00505, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.997, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.997]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/2b5f3a3a0eb3455ab8f1139d0a6fbca3/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_0-oversampling-sampling_modifiedRS_rho=5-seed0-epoch=06-val_f1_macro=0.80.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/2b5f3a3a0eb3455ab8f1139d0a6fbca3/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_0-oversampling-sampling_modifiedRS_rho=5-seed0-epoch=06-val_f1_macro=0.80.ckpt

Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.86it/s]Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.97it/s]Validation DataLoader 0:  30%|███       | 3/10 [00:01<00:02,  2.98it/s]Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.00it/s]Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.02it/s]Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.04it/s]Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.05it/s]Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.05it/s]Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.05it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.26it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.00it/s]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/2b5f3a3a0eb3455ab8f1139d0a6fbca3/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_0-oversampling-sampling_modifiedRS_rho=5-seed0-epoch=06-val_f1_macro=0.80.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/2b5f3a3a0eb3455ab8f1139d0a6fbca3/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_0-oversampling-sampling_modifiedRS_rho=5-seed0-epoch=06-val_f1_macro=0.80.ckpt

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
     Validate metric           DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      val_accuracy           0.92166668176651
        val_auprc            0.849773108959198
      val_f1_macro          0.7958873510360718
   val_f1_per_label_0       0.9561157822608948
   val_f1_per_label_1       0.6356589198112488
     val_f1_weighted        0.9171268939971924
        val_loss            0.27357912063598633
   val_precision_macro      0.8366596698760986
 val_precision_weighted     0.9157440662384033
    val_recall_macro         0.766590416431427
   val_recall_weighted       0.92166668176651
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.97it/s]Testing DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.06it/s]Testing DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.10it/s]Testing DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s]Testing DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.10it/s]Testing DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s]Testing DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s]Testing DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s]Testing DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.04it/s]Seed set to 0
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python imbalanced_text_classification/main.py --data_name u ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]

  | Name               | Type                          | Params
---------------------------------------------------------------------
0 | classifier         | BertForSequenceClassification | 109 M 
1 | loss_fn            | CrossEntropyLoss              | 0     
2 | train_metrics      | MetricCollection              | 0     
3 | train_f1_per_label | MulticlassF1Score             | 0     
4 | val_metrics        | MetricCollection              | 0     
5 | val_f1_per_label   | MulticlassF1Score             | 0     
6 | test_metrics       | MetricCollection              | 0     
7 | test_f1_per_label  | MulticlassF1Score             | 0     
---------------------------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.935   Total estimated model params size (MB)

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      test_accuracy         0.9133333563804626
       test_auprc           0.7569454908370972
      test_f1_macro         0.7397831678390503
   test_f1_per_label_0      0.9522935748100281
   test_f1_per_label_1      0.5272727012634277
    test_f1_weighted        0.9104998111724854
  test_precision_macro      0.7569913268089294
 test_precision_weighted    0.9083103537559509
    test_recall_macro       0.7254300117492676
  test_recall_weighted      0.9133333563804626
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
END running Trial 0.


Initial trial 0 achieved value: 0.7958873510360718

MLFlow run_name = sampling_modifiedRS-us-election-2020-bin-Trial-1
MLflow Saved Child Search Trial 1 Log in './mlruns/5/a95001dda6e849a58517c85ad6a28e77'
Using Cross Entropy Loss: alpha=None
Runing optuna for hyperparameter search:
Parmas: {'sampling_modifiedRS_mode': 'oversampling', 'sampling_modifiedRS_rho': 1.0, 'sampling_weightedRS_percentage': None, 'wce_alpha': None, 'fl_gamma': None, 'loss': <Loss.CE_Loss: 'CE_Loss'>}
Calling ImbalancedDataModule.setup() for train...
------ Dataset Statistics ------
Raw data reading from CSV files:
| Split	 | Size	 | Label Counts
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
| train_val	 | 2400 |	0: 2107, 87.8%	1: 293, 12.2%
After splitting for training:
| Split	 | Size	 | Label Counts
| train	 | 1800 |	0: 1580, 87.8%	1: 220, 12.2%
| val	 | 600 |	0: 527, 87.8%	1: 73, 12.2%
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
Resampling with sampling_modifiedRS_rho = 1.0 => training set has 3160.0 samples.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2026457) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.0
Label counts after resampling: {1: 1580, 0: 1580}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.0
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] Epoch 0: |          | 1/? [00:00<00:00,  1.38it/s]Epoch 0: |          | 1/? [00:00<00:00,  1.38it/s, v_num=8e77]Epoch 0: |          | 2/? [00:01<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 2/? [00:01<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 3/? [00:02<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 3/? [00:02<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 4/? [00:02<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 4/? [00:02<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 5/? [00:03<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 5/? [00:03<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 6/? [00:04<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 6/? [00:04<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 7/? [00:05<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 7/? [00:05<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 8/? [00:05<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 8/? [00:05<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 9/? [00:06<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 9/? [00:06<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 10/? [00:07<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 10/? [00:07<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 11/? [00:08<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 11/? [00:08<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 12/? [00:08<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 12/? [00:08<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 13/? [00:09<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 13/? [00:09<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 14/? [00:10<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 14/? [00:10<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 15/? [00:10<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 15/? [00:10<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 16/? [00:11<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 16/? [00:11<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 17/? [00:12<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 17/? [00:12<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 18/? [00:13<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 18/? [00:13<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 19/? [00:13<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 19/? [00:13<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 20/? [00:14<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 20/? [00:14<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 21/? [00:15<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 21/? [00:15<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 22/? [00:16<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 22/? [00:16<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 23/? [00:16<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 23/? [00:16<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 24/? [00:17<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 24/? [00:17<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 25/? [00:18<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 25/? [00:18<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 26/? [00:18<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 26/? [00:18<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 27/? [00:19<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 27/? [00:19<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 28/? [00:20<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 28/? [00:20<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 29/? [00:21<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 29/? [00:21<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 30/? [00:21<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 30/? [00:21<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 31/? [00:22<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 31/? [00:22<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 32/? [00:23<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 32/? [00:23<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 33/? [00:24<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 33/? [00:24<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 34/? [00:24<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 34/? [00:24<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 35/? [00:25<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 35/? [00:25<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 36/? [00:26<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 36/? [00:26<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 37/? [00:27<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 37/? [00:27<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 38/? [00:27<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 38/? [00:27<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 39/? [00:28<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 39/? [00:28<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 40/? [00:29<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 40/? [00:29<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 41/? [00:29<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 41/? [00:29<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 42/? [00:30<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 42/? [00:30<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 43/? [00:31<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 43/? [00:31<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 44/? [00:32<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 44/? [00:32<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 45/? [00:32<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 45/? [00:32<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 46/? [00:33<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 46/? [00:33<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 47/? [00:34<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 47/? [00:34<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 48/? [00:35<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 48/? [00:35<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 49/? [00:35<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 49/? [00:35<00:00,  1.37it/s, v_num=8e77]Epoch 0: |          | 50/? [00:36<00:00,  1.38it/s, v_num=8e77]Epoch 0: |          | 50/? [00:36<00:00,  1.38it/s, v_num=8e77]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.02it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.08it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028

                                                                        [AEpoch 0: |          | 50/? [00:41<00:00,  1.22it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444]Epoch 0: |          | 50/? [00:41<00:00,  1.22it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444]Epoch 0: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444]         Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.0
Epoch 1: |          | 1/? [00:02<00:00,  0.38it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444]Epoch 1: |          | 1/? [00:02<00:00,  0.38it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 2/? [00:03<00:00,  0.60it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 2/? [00:03<00:00,  0.60it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 3/? [00:04<00:00,  0.73it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 3/? [00:04<00:00,  0.73it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 4/? [00:04<00:00,  0.83it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 4/? [00:04<00:00,  0.83it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 5/? [00:05<00:00,  0.90it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 5/? [00:05<00:00,  0.90it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 6/? [00:06<00:00,  0.95it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 6/? [00:06<00:00,  0.95it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 7/? [00:07<00:00,  1.00it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 7/? [00:07<00:00,  1.00it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 8/? [00:07<00:00,  1.03it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 8/? [00:07<00:00,  1.03it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 9/? [00:08<00:00,  1.06it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 9/? [00:08<00:00,  1.06it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 10/? [00:09<00:00,  1.09it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 10/? [00:09<00:00,  1.09it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 11/? [00:09<00:00,  1.10it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 11/? [00:09<00:00,  1.10it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 12/? [00:10<00:00,  1.12it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 12/? [00:10<00:00,  1.12it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 13/? [00:11<00:00,  1.14it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 13/? [00:11<00:00,  1.14it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 14/? [00:12<00:00,  1.15it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 14/? [00:12<00:00,  1.15it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 15/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 15/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 16/? [00:13<00:00,  1.18it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 16/? [00:13<00:00,  1.18it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 17/? [00:14<00:00,  1.19it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 17/? [00:14<00:00,  1.19it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 18/? [00:15<00:00,  1.19it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 18/? [00:15<00:00,  1.19it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 19/? [00:15<00:00,  1.20it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 19/? [00:15<00:00,  1.20it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 20/? [00:16<00:00,  1.21it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 20/? [00:16<00:00,  1.21it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 21/? [00:17<00:00,  1.22it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 21/? [00:17<00:00,  1.22it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 22/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 22/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 23/? [00:18<00:00,  1.23it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 23/? [00:18<00:00,  1.23it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 24/? [00:19<00:00,  1.23it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 24/? [00:19<00:00,  1.23it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 25/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 25/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 26/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 26/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 27/? [00:21<00:00,  1.24it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 27/? [00:21<00:00,  1.24it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 28/? [00:22<00:00,  1.25it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 28/? [00:22<00:00,  1.25it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 29/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 29/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 30/? [00:23<00:00,  1.26it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 30/? [00:23<00:00,  1.26it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 31/? [00:24<00:00,  1.26it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 31/? [00:24<00:00,  1.26it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 32/? [00:25<00:00,  1.26it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 32/? [00:25<00:00,  1.26it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 33/? [00:26<00:00,  1.26it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 33/? [00:26<00:00,  1.26it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 35/? [00:27<00:00,  1.27it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 35/? [00:27<00:00,  1.27it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 36/? [00:28<00:00,  1.27it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 36/? [00:28<00:00,  1.27it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 37/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 37/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 38/? [00:29<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 38/? [00:29<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 40/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 40/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 41/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 41/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 42/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 42/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 43/? [00:33<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 43/? [00:33<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 46/? [00:35<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 46/? [00:35<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 47/? [00:36<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 47/? [00:36<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 48/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 48/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 49/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 49/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 50/? [00:38<00:00,  1.31it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 50/? [00:38<00:00,  1.31it/s, v_num=8e77, val_loss=0.281, val_accuracy=0.900, val_auprc=0.741, val_f1_macro=0.695, val_f1_weighted=0.884, val_precision_macro=0.799, val_precision_weighted=0.886, val_recall_macro=0.654, val_recall_weighted=0.900, val_f1_per_label_0=0.945, val_f1_per_label_1=0.444, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.04it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.08it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.30it/s][A
                                                                        [AEpoch 1: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]         Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.0
Epoch 2: |          | 1/? [00:02<00:00,  0.39it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.591, train_accuracy=0.658, train_auprc=0.739, train_f1_macro=0.656, train_f1_weighted=0.656, train_precision_macro=0.662, train_precision_weighted=0.662, train_recall_macro=0.658, train_recall_weighted=0.658, train_f1_per_label_0=0.631, train_f1_per_label_1=0.682]Epoch 2: |          | 1/? [00:02<00:00,  0.39it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 2/? [00:03<00:00,  0.61it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 2/? [00:03<00:00,  0.61it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 3/? [00:04<00:00,  0.74it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 3/? [00:04<00:00,  0.74it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 4/? [00:04<00:00,  0.84it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 4/? [00:04<00:00,  0.84it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 5/? [00:05<00:00,  0.91it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 5/? [00:05<00:00,  0.91it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 6/? [00:06<00:00,  0.96it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 6/? [00:06<00:00,  0.96it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 7/? [00:06<00:00,  1.01it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 7/? [00:06<00:00,  1.01it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 8/? [00:07<00:00,  1.04it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 8/? [00:07<00:00,  1.04it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 9/? [00:08<00:00,  1.07it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 9/? [00:08<00:00,  1.07it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 10/? [00:09<00:00,  1.09it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 10/? [00:09<00:00,  1.09it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 11/? [00:09<00:00,  1.11it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 11/? [00:09<00:00,  1.11it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 12/? [00:10<00:00,  1.12it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 12/? [00:10<00:00,  1.12it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 14/? [00:12<00:00,  1.15it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 14/? [00:12<00:00,  1.15it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 15/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 15/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 16/? [00:13<00:00,  1.17it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 16/? [00:13<00:00,  1.17it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 17/? [00:14<00:00,  1.18it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 17/? [00:14<00:00,  1.18it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 18/? [00:15<00:00,  1.19it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 18/? [00:15<00:00,  1.19it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 19/? [00:15<00:00,  1.20it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 19/? [00:15<00:00,  1.20it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 21/? [00:17<00:00,  1.21it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 21/? [00:17<00:00,  1.21it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 22/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 22/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 24/? [00:19<00:00,  1.23it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 24/? [00:19<00:00,  1.23it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 25/? [00:20<00:00,  1.23it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 25/? [00:20<00:00,  1.23it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 26/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 26/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 27/? [00:21<00:00,  1.24it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 27/? [00:21<00:00,  1.24it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 29/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 29/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 30/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 30/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 32/? [00:25<00:00,  1.26it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 32/? [00:25<00:00,  1.26it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 33/? [00:26<00:00,  1.26it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 33/? [00:26<00:00,  1.26it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 35/? [00:27<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 35/? [00:27<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 36/? [00:28<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 36/? [00:28<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 37/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 37/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 38/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 38/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 40/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 40/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 41/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 41/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 42/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 42/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 43/? [00:33<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 43/? [00:33<00:00,  1.28it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 46/? [00:35<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 46/? [00:35<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 47/? [00:36<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 47/? [00:36<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 48/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 48/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 49/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 49/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 50/? [00:38<00:00,  1.31it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 50/? [00:38<00:00,  1.31it/s, v_num=8e77, val_loss=0.249, val_accuracy=0.920, val_auprc=0.837, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.99it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.03it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.06it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.28it/s][A
                                                                        [AEpoch 2: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]         Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.0
Epoch 3: |          | 1/? [00:02<00:00,  0.38it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.205, train_accuracy=0.922, train_auprc=0.971, train_f1_macro=0.922, train_f1_weighted=0.922, train_precision_macro=0.923, train_precision_weighted=0.923, train_recall_macro=0.922, train_recall_weighted=0.922, train_f1_per_label_0=0.923, train_f1_per_label_1=0.922]Epoch 3: |          | 1/? [00:02<00:00,  0.38it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 2/? [00:03<00:00,  0.59it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 2/? [00:03<00:00,  0.59it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 3/? [00:04<00:00,  0.73it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 3/? [00:04<00:00,  0.73it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 4/? [00:04<00:00,  0.82it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 4/? [00:04<00:00,  0.82it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 5/? [00:05<00:00,  0.90it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 5/? [00:05<00:00,  0.89it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 6/? [00:06<00:00,  0.95it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 6/? [00:06<00:00,  0.95it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 7/? [00:07<00:00,  0.99it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 7/? [00:07<00:00,  0.99it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 8/? [00:07<00:00,  1.03it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 8/? [00:07<00:00,  1.03it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 9/? [00:08<00:00,  1.06it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 9/? [00:08<00:00,  1.06it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 10/? [00:09<00:00,  1.08it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 10/? [00:09<00:00,  1.08it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 11/? [00:09<00:00,  1.10it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 11/? [00:09<00:00,  1.10it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 12/? [00:10<00:00,  1.12it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 12/? [00:10<00:00,  1.12it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 13/? [00:11<00:00,  1.14it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 13/? [00:11<00:00,  1.14it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 14/? [00:12<00:00,  1.15it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 14/? [00:12<00:00,  1.15it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 15/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 15/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 16/? [00:13<00:00,  1.17it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 16/? [00:13<00:00,  1.17it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 17/? [00:14<00:00,  1.18it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 17/? [00:14<00:00,  1.18it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 18/? [00:15<00:00,  1.19it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 18/? [00:15<00:00,  1.19it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 19/? [00:15<00:00,  1.20it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 19/? [00:15<00:00,  1.20it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 20/? [00:16<00:00,  1.21it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 20/? [00:16<00:00,  1.21it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 21/? [00:17<00:00,  1.21it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 21/? [00:17<00:00,  1.21it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 22/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 22/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 23/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 23/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 24/? [00:19<00:00,  1.23it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 24/? [00:19<00:00,  1.23it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 25/? [00:20<00:00,  1.23it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 25/? [00:20<00:00,  1.23it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 26/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 26/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 27/? [00:21<00:00,  1.24it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 27/? [00:21<00:00,  1.24it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 28/? [00:22<00:00,  1.25it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 28/? [00:22<00:00,  1.25it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 29/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 29/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 30/? [00:23<00:00,  1.26it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 30/? [00:23<00:00,  1.26it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 31/? [00:24<00:00,  1.26it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 31/? [00:24<00:00,  1.26it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 32/? [00:25<00:00,  1.26it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 32/? [00:25<00:00,  1.26it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 33/? [00:26<00:00,  1.26it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 33/? [00:26<00:00,  1.26it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 35/? [00:27<00:00,  1.27it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 35/? [00:27<00:00,  1.27it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 36/? [00:28<00:00,  1.27it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 36/? [00:28<00:00,  1.27it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 37/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 37/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 38/? [00:29<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 38/? [00:29<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 40/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 40/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 41/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 41/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 42/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 42/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 43/? [00:33<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 43/? [00:33<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 46/? [00:35<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 46/? [00:35<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 47/? [00:36<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 47/? [00:36<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 48/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 48/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 49/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 49/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 50/? [00:38<00:00,  1.31it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 50/? [00:38<00:00,  1.31it/s, v_num=8e77, val_loss=0.312, val_accuracy=0.922, val_auprc=0.816, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.10it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 3: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]         Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.0
Epoch 4: |          | 1/? [00:02<00:00,  0.36it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0432, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.989, train_f1_weighted=0.989, train_precision_macro=0.989, train_precision_weighted=0.989, train_recall_macro=0.989, train_recall_weighted=0.989, train_f1_per_label_0=0.989, train_f1_per_label_1=0.989]Epoch 4: |          | 1/? [00:02<00:00,  0.36it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 2/? [00:03<00:00,  0.57it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 2/? [00:03<00:00,  0.57it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 3/? [00:04<00:00,  0.70it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 3/? [00:04<00:00,  0.70it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 4/? [00:04<00:00,  0.80it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 4/? [00:04<00:00,  0.80it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 5/? [00:05<00:00,  0.87it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 5/? [00:05<00:00,  0.87it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 6/? [00:06<00:00,  0.93it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 6/? [00:06<00:00,  0.93it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 7/? [00:07<00:00,  0.97it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 7/? [00:07<00:00,  0.97it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 8/? [00:07<00:00,  1.00it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 8/? [00:07<00:00,  1.00it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 9/? [00:08<00:00,  1.03it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 9/? [00:08<00:00,  1.03it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 10/? [00:09<00:00,  1.06it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 10/? [00:09<00:00,  1.06it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 11/? [00:10<00:00,  1.08it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 11/? [00:10<00:00,  1.08it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 12/? [00:10<00:00,  1.10it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 12/? [00:10<00:00,  1.10it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 13/? [00:11<00:00,  1.12it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 13/? [00:11<00:00,  1.12it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 14/? [00:12<00:00,  1.13it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 14/? [00:12<00:00,  1.13it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 15/? [00:13<00:00,  1.14it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 15/? [00:13<00:00,  1.14it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 16/? [00:13<00:00,  1.16it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 16/? [00:13<00:00,  1.16it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 17/? [00:14<00:00,  1.17it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 17/? [00:14<00:00,  1.17it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 18/? [00:15<00:00,  1.18it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 18/? [00:15<00:00,  1.18it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 19/? [00:16<00:00,  1.18it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 19/? [00:16<00:00,  1.18it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 20/? [00:16<00:00,  1.19it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 20/? [00:16<00:00,  1.19it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 21/? [00:17<00:00,  1.20it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 21/? [00:17<00:00,  1.20it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 22/? [00:18<00:00,  1.21it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 22/? [00:18<00:00,  1.21it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 23/? [00:18<00:00,  1.21it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 23/? [00:18<00:00,  1.21it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 24/? [00:19<00:00,  1.22it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 24/? [00:19<00:00,  1.22it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 25/? [00:20<00:00,  1.22it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 25/? [00:20<00:00,  1.22it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 26/? [00:21<00:00,  1.23it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 26/? [00:21<00:00,  1.23it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 27/? [00:21<00:00,  1.23it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 27/? [00:21<00:00,  1.23it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 28/? [00:22<00:00,  1.24it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 28/? [00:22<00:00,  1.24it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 29/? [00:23<00:00,  1.24it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 29/? [00:23<00:00,  1.24it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 30/? [00:24<00:00,  1.24it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 30/? [00:24<00:00,  1.24it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 31/? [00:24<00:00,  1.25it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 31/? [00:24<00:00,  1.25it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 32/? [00:25<00:00,  1.25it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 32/? [00:25<00:00,  1.25it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 33/? [00:26<00:00,  1.25it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 33/? [00:26<00:00,  1.25it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 34/? [00:27<00:00,  1.26it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 34/? [00:27<00:00,  1.26it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 35/? [00:27<00:00,  1.26it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 35/? [00:27<00:00,  1.26it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 36/? [00:28<00:00,  1.26it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 36/? [00:28<00:00,  1.26it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 37/? [00:29<00:00,  1.26it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 37/? [00:29<00:00,  1.26it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 38/? [00:30<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 38/? [00:30<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 39/? [00:30<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 39/? [00:30<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 40/? [00:31<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 40/? [00:31<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 41/? [00:32<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 41/? [00:32<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 42/? [00:32<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 42/? [00:32<00:00,  1.27it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 43/? [00:33<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 43/? [00:33<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 44/? [00:34<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 44/? [00:34<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 45/? [00:35<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 45/? [00:35<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 46/? [00:35<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 46/? [00:35<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 47/? [00:36<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 47/? [00:36<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 48/? [00:37<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 48/? [00:37<00:00,  1.28it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 49/? [00:38<00:00,  1.29it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 49/? [00:38<00:00,  1.29it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 50/? [00:38<00:00,  1.30it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 50/? [00:38<00:00,  1.30it/s, v_num=8e77, val_loss=0.392, val_accuracy=0.923, val_auprc=0.808, val_f1_macro=0.784, val_f1_weighted=0.915, val_precision_macro=0.867, val_precision_weighted=0.917, val_recall_macro=0.738, val_recall_weighted=0.923, val_f1_per_label_0=0.957, val_f1_per_label_1=0.610, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.08it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.08it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.08it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.30it/s][A
                                                                        [AEpoch 4: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]         Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.0
Epoch 5: |          | 1/? [00:02<00:00,  0.40it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0165, train_accuracy=0.997, train_auprc=0.999, train_f1_macro=0.997, train_f1_weighted=0.997, train_precision_macro=0.997, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.997]Epoch 5: |          | 1/? [00:02<00:00,  0.40it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 2/? [00:03<00:00,  0.62it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 2/? [00:03<00:00,  0.62it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 3/? [00:03<00:00,  0.75it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 3/? [00:03<00:00,  0.75it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 4/? [00:04<00:00,  0.85it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 4/? [00:04<00:00,  0.85it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 5/? [00:05<00:00,  0.92it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 5/? [00:05<00:00,  0.91it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 6/? [00:06<00:00,  0.97it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 6/? [00:06<00:00,  0.97it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 7/? [00:06<00:00,  1.01it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 7/? [00:06<00:00,  1.01it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 8/? [00:07<00:00,  1.04it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 8/? [00:07<00:00,  1.04it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 9/? [00:08<00:00,  1.07it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 9/? [00:08<00:00,  1.07it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 10/? [00:09<00:00,  1.10it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 10/? [00:09<00:00,  1.10it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 11/? [00:09<00:00,  1.12it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 11/? [00:09<00:00,  1.12it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 12/? [00:10<00:00,  1.13it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 12/? [00:10<00:00,  1.13it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 13/? [00:11<00:00,  1.15it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 13/? [00:11<00:00,  1.15it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 14/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 14/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 15/? [00:12<00:00,  1.17it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 15/? [00:12<00:00,  1.17it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 16/? [00:13<00:00,  1.18it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 16/? [00:13<00:00,  1.18it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 17/? [00:14<00:00,  1.19it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 17/? [00:14<00:00,  1.19it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 18/? [00:14<00:00,  1.20it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 18/? [00:14<00:00,  1.20it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 19/? [00:15<00:00,  1.21it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 19/? [00:15<00:00,  1.21it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 20/? [00:16<00:00,  1.22it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 20/? [00:16<00:00,  1.22it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 21/? [00:17<00:00,  1.22it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 21/? [00:17<00:00,  1.22it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 22/? [00:17<00:00,  1.23it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 22/? [00:17<00:00,  1.23it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 23/? [00:18<00:00,  1.23it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 23/? [00:18<00:00,  1.23it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 24/? [00:19<00:00,  1.24it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 24/? [00:19<00:00,  1.24it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 25/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 25/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 26/? [00:20<00:00,  1.25it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 26/? [00:20<00:00,  1.25it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 27/? [00:21<00:00,  1.25it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 27/? [00:21<00:00,  1.25it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 28/? [00:22<00:00,  1.26it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 28/? [00:22<00:00,  1.26it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 29/? [00:23<00:00,  1.26it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 29/? [00:23<00:00,  1.26it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 30/? [00:23<00:00,  1.26it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 30/? [00:23<00:00,  1.26it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 31/? [00:24<00:00,  1.27it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 31/? [00:24<00:00,  1.27it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 32/? [00:25<00:00,  1.27it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 32/? [00:25<00:00,  1.27it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 33/? [00:25<00:00,  1.27it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 33/? [00:25<00:00,  1.27it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 35/? [00:27<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 35/? [00:27<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 36/? [00:28<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 36/? [00:28<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 37/? [00:28<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 37/? [00:28<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 38/? [00:29<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 38/? [00:29<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 40/? [00:31<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 40/? [00:31<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 41/? [00:31<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 41/? [00:31<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 42/? [00:32<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 42/? [00:32<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 43/? [00:33<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 43/? [00:33<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 46/? [00:35<00:00,  1.30it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 46/? [00:35<00:00,  1.30it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 47/? [00:36<00:00,  1.30it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 47/? [00:36<00:00,  1.30it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 48/? [00:36<00:00,  1.30it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 48/? [00:36<00:00,  1.30it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 49/? [00:37<00:00,  1.30it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 49/? [00:37<00:00,  1.30it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 50/? [00:38<00:00,  1.32it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 50/? [00:38<00:00,  1.32it/s, v_num=8e77, val_loss=0.325, val_accuracy=0.915, val_auprc=0.811, val_f1_macro=0.755, val_f1_weighted=0.905, val_precision_macro=0.844, val_precision_weighted=0.906, val_recall_macro=0.710, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.557, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s][A
                                                                        [AEpoch 5: |          | 50/? [00:43<00:00,  1.16it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 50/? [00:43<00:00,  1.16it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]         Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.0
Epoch 6: |          | 1/? [00:02<00:00,  0.38it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.0367, train_accuracy=0.992, train_auprc=0.997, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.992, train_f1_per_label_1=0.992]Epoch 6: |          | 1/? [00:02<00:00,  0.38it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 2/? [00:03<00:00,  0.59it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 2/? [00:03<00:00,  0.59it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 3/? [00:04<00:00,  0.73it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 3/? [00:04<00:00,  0.73it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 4/? [00:04<00:00,  0.82it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 4/? [00:04<00:00,  0.82it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 5/? [00:05<00:00,  0.89it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 5/? [00:05<00:00,  0.89it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 6/? [00:06<00:00,  0.95it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 6/? [00:06<00:00,  0.95it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 7/? [00:07<00:00,  0.99it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 7/? [00:07<00:00,  0.99it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 8/? [00:07<00:00,  1.03it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 8/? [00:07<00:00,  1.03it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 9/? [00:08<00:00,  1.06it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 9/? [00:08<00:00,  1.06it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 10/? [00:09<00:00,  1.08it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 10/? [00:09<00:00,  1.08it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 11/? [00:09<00:00,  1.10it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 11/? [00:09<00:00,  1.10it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 12/? [00:10<00:00,  1.12it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 12/? [00:10<00:00,  1.12it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 13/? [00:11<00:00,  1.14it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 13/? [00:11<00:00,  1.14it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 14/? [00:12<00:00,  1.15it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 14/? [00:12<00:00,  1.15it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 15/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 15/? [00:12<00:00,  1.16it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 16/? [00:13<00:00,  1.17it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 16/? [00:13<00:00,  1.17it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 17/? [00:14<00:00,  1.18it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 17/? [00:14<00:00,  1.18it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 18/? [00:15<00:00,  1.19it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 18/? [00:15<00:00,  1.19it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 19/? [00:15<00:00,  1.20it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 19/? [00:15<00:00,  1.20it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 20/? [00:16<00:00,  1.21it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 20/? [00:16<00:00,  1.21it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 21/? [00:17<00:00,  1.21it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 21/? [00:17<00:00,  1.21it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 22/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 22/? [00:18<00:00,  1.22it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 23/? [00:18<00:00,  1.23it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 23/? [00:18<00:00,  1.23it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 24/? [00:19<00:00,  1.23it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 24/? [00:19<00:00,  1.23it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 25/? [00:20<00:00,  1.23it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 25/? [00:20<00:00,  1.23it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 26/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 26/? [00:20<00:00,  1.24it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 27/? [00:21<00:00,  1.24it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 27/? [00:21<00:00,  1.24it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 28/? [00:22<00:00,  1.25it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 28/? [00:22<00:00,  1.25it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 29/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 29/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 30/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 30/? [00:23<00:00,  1.25it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 31/? [00:24<00:00,  1.26it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 31/? [00:24<00:00,  1.26it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 32/? [00:25<00:00,  1.26it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 32/? [00:25<00:00,  1.26it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 33/? [00:26<00:00,  1.26it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 33/? [00:26<00:00,  1.26it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 34/? [00:26<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 35/? [00:27<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 35/? [00:27<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 36/? [00:28<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 36/? [00:28<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 37/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 37/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 38/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 38/? [00:29<00:00,  1.27it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 39/? [00:30<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 40/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 40/? [00:31<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 41/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 41/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 42/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 42/? [00:32<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 43/? [00:33<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 43/? [00:33<00:00,  1.28it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 44/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 45/? [00:34<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 46/? [00:35<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 46/? [00:35<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 47/? [00:36<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 47/? [00:36<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 48/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 48/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 49/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 49/? [00:37<00:00,  1.29it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 50/? [00:38<00:00,  1.31it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 50/? [00:38<00:00,  1.31it/s, v_num=8e77, val_loss=0.465, val_accuracy=0.892, val_auprc=0.838, val_f1_macro=0.769, val_f1_weighted=0.896, val_precision_macro=0.749, val_precision_weighted=0.903, val_recall_macro=0.797, val_recall_weighted=0.892, val_f1_per_label_0=0.937, val_f1_per_label_1=0.601, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 6: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.480, val_accuracy=0.915, val_auprc=0.828, val_f1_macro=0.738, val_f1_weighted=0.901, val_precision_macro=0.872, val_precision_weighted=0.909, val_recall_macro=0.686, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.523, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.480, val_accuracy=0.915, val_auprc=0.828, val_f1_macro=0.738, val_f1_weighted=0.901, val_precision_macro=0.872, val_precision_weighted=0.909, val_recall_macro=0.686, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.523, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Epoch 6: |          | 50/? [00:43<00:00,  1.15it/s, v_num=8e77, val_loss=0.480, val_accuracy=0.915, val_auprc=0.828, val_f1_macro=0.738, val_f1_weighted=0.901, val_precision_macro=0.872, val_precision_weighted=0.909, val_recall_macro=0.686, val_recall_weighted=0.915, val_f1_per_label_0=0.953, val_f1_per_label_1=0.523, train_loss=0.00991, train_accuracy=0.998, train_auprc=1.000, train_f1_macro=0.998, train_f1_weighted=0.998, train_precision_macro=0.998, train_precision_weighted=0.998, train_recall_macro=0.998, train_recall_weighted=0.998, train_f1_per_label_0=0.998, train_f1_per_label_1=0.998]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/a95001dda6e849a58517c85ad6a28e77/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_1-oversampling-sampling_modifiedRS_rho=1.0-seed0-epoch=03-val_f1_macro=0.78.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/a95001dda6e849a58517c85ad6a28e77/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_1-oversampling-sampling_modifiedRS_rho=1.0-seed0-epoch=03-val_f1_macro=0.78.ckpt

Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.88it/s]Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.98it/s]Validation DataLoader 0:  30%|███       | 3/10 [00:01<00:02,  3.00it/s]Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.03it/s]Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.04it/s]Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.05it/s]Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.05it/s]Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.06it/s]Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.07it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.28it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.01it/s]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/a95001dda6e849a58517c85ad6a28e77/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_1-oversampling-sampling_modifiedRS_rho=1.0-seed0-epoch=03-val_f1_macro=0.78.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/a95001dda6e849a58517c85ad6a28e77/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_1-oversampling-sampling_modifiedRS_rho=1.0-seed0-epoch=03-val_f1_macro=0.78.ckpt

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
     Validate metric           DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      val_accuracy          0.9233333468437195
        val_auprc            0.80799400806427
      val_f1_macro          0.7838277816772461
   val_f1_per_label_0       0.9574861526489258
   val_f1_per_label_1       0.6101694703102112
     val_f1_weighted         0.915229320526123
        val_loss            0.3916826844215393
   val_precision_macro      0.8666666746139526
 val_precision_weighted     0.9171110987663269
    val_recall_macro        0.7380364537239075
   val_recall_weighted      0.9233333468437195
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.00it/s]Testing DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.02it/s]Testing DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.03it/s]Testing DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.05it/s]Testing DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.06it/s]Testing DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.07it/s]Testing DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.07it/s]Testing DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.07it/s]Testing DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.07it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.29it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.03it/s]Seed set to 0
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python imbalanced_text_classification/main.py --data_name u ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]

  | Name               | Type                          | Params
---------------------------------------------------------------------
0 | classifier         | BertForSequenceClassification | 109 M 
1 | loss_fn            | CrossEntropyLoss              | 0     
2 | train_metrics      | MetricCollection              | 0     
3 | train_f1_per_label | MulticlassF1Score             | 0     
4 | val_metrics        | MetricCollection              | 0     
5 | val_f1_per_label   | MulticlassF1Score             | 0     
6 | test_metrics       | MetricCollection              | 0     
7 | test_f1_per_label  | MulticlassF1Score             | 0     
---------------------------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.935   Total estimated model params size (MB)

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      test_accuracy         0.9233333468437195
       test_auprc           0.7136064171791077
      test_f1_macro         0.7490909099578857
   test_f1_per_label_0      0.9581817984580994
   test_f1_per_label_1      0.5400000214576721
    test_f1_weighted        0.9170605540275574
  test_precision_macro      0.8006457686424255
 test_precision_weighted    0.9148067831993103
    test_recall_macro       0.7158745527267456
  test_recall_weighted      0.9233333468437195
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
END running Trial 1.



MLFlow run_name = sampling_modifiedRS-us-election-2020-bin-Trial-2
MLflow Saved Child Search Trial 2 Log in './mlruns/5/0ac83a63eb7f47ea9aafca0141d3aa63'
Using Cross Entropy Loss: alpha=None
Runing optuna for hyperparameter search:
Parmas: {'sampling_modifiedRS_mode': 'oversampling', 'sampling_modifiedRS_rho': 1.2, 'sampling_weightedRS_percentage': None, 'wce_alpha': None, 'fl_gamma': None, 'loss': <Loss.CE_Loss: 'CE_Loss'>}
Calling ImbalancedDataModule.setup() for train...
------ Dataset Statistics ------
Raw data reading from CSV files:
| Split	 | Size	 | Label Counts
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
| train_val	 | 2400 |	0: 2107, 87.8%	1: 293, 12.2%
After splitting for training:
| Split	 | Size	 | Label Counts
| train	 | 1800 |	0: 1580, 87.8%	1: 220, 12.2%
| val	 | 600 |	0: 527, 87.8%	1: 73, 12.2%
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
Resampling with sampling_modifiedRS_rho = 1.2 => training set has 2896.666666666667 samples.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2026457) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.2
Label counts after resampling: {0: 1580, 1: 1317}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.2
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] Epoch 0: |          | 1/? [00:00<00:00,  1.35it/s]Epoch 0: |          | 1/? [00:00<00:00,  1.35it/s, v_num=aa63]Epoch 0: |          | 2/? [00:01<00:00,  1.35it/s, v_num=aa63]Epoch 0: |          | 2/? [00:01<00:00,  1.35it/s, v_num=aa63]Epoch 0: |          | 3/? [00:02<00:00,  1.36it/s, v_num=aa63]Epoch 0: |          | 3/? [00:02<00:00,  1.36it/s, v_num=aa63]Epoch 0: |          | 4/? [00:02<00:00,  1.36it/s, v_num=aa63]Epoch 0: |          | 4/? [00:02<00:00,  1.36it/s, v_num=aa63]Epoch 0: |          | 5/? [00:03<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 5/? [00:03<00:00,  1.36it/s, v_num=aa63]Epoch 0: |          | 6/? [00:04<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 6/? [00:04<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 7/? [00:05<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 7/? [00:05<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 8/? [00:05<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 8/? [00:05<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 9/? [00:06<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 9/? [00:06<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 10/? [00:07<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 10/? [00:07<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 11/? [00:08<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 11/? [00:08<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 12/? [00:08<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 12/? [00:08<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 13/? [00:09<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 13/? [00:09<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 14/? [00:10<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 14/? [00:10<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 15/? [00:10<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 15/? [00:10<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 16/? [00:11<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 16/? [00:11<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 17/? [00:12<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 17/? [00:12<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 18/? [00:13<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 18/? [00:13<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 19/? [00:13<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 19/? [00:13<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 20/? [00:14<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 20/? [00:14<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 21/? [00:15<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 21/? [00:15<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 22/? [00:16<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 22/? [00:16<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 23/? [00:16<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 23/? [00:16<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 24/? [00:17<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 24/? [00:17<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 25/? [00:18<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 25/? [00:18<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 26/? [00:18<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 26/? [00:18<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 27/? [00:19<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 27/? [00:19<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 28/? [00:20<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 28/? [00:20<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 29/? [00:21<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 29/? [00:21<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 30/? [00:21<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 30/? [00:21<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 31/? [00:22<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 31/? [00:22<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 32/? [00:23<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 32/? [00:23<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 33/? [00:24<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 33/? [00:24<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 34/? [00:24<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 34/? [00:24<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 35/? [00:25<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 35/? [00:25<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 36/? [00:26<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 36/? [00:26<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 37/? [00:27<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 37/? [00:27<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 38/? [00:27<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 38/? [00:27<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 39/? [00:28<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 39/? [00:28<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 40/? [00:29<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 40/? [00:29<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 41/? [00:29<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 41/? [00:29<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 42/? [00:30<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 42/? [00:30<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 43/? [00:31<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 43/? [00:31<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 44/? [00:32<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 44/? [00:32<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 45/? [00:32<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 45/? [00:32<00:00,  1.37it/s, v_num=aa63]Epoch 0: |          | 46/? [00:33<00:00,  1.39it/s, v_num=aa63]Epoch 0: |          | 46/? [00:33<00:00,  1.39it/s, v_num=aa63]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.00it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.02it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.04it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.05it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.08it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.06it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.28it/s][A/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028

                                                                        [AEpoch 0: |          | 46/? [00:38<00:00,  1.21it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528]Epoch 0: |          | 46/? [00:38<00:00,  1.21it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528]Epoch 0: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528]         Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.2
Epoch 1: |          | 1/? [00:02<00:00,  0.40it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528]Epoch 1: |          | 1/? [00:02<00:00,  0.40it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 2/? [00:03<00:00,  0.62it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 2/? [00:03<00:00,  0.62it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 3/? [00:03<00:00,  0.75it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 3/? [00:03<00:00,  0.75it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 4/? [00:04<00:00,  0.85it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 4/? [00:04<00:00,  0.85it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 5/? [00:05<00:00,  0.92it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 5/? [00:05<00:00,  0.92it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 6/? [00:06<00:00,  0.97it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 6/? [00:06<00:00,  0.97it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 7/? [00:06<00:00,  1.01it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 7/? [00:06<00:00,  1.01it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 8/? [00:07<00:00,  1.05it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 8/? [00:07<00:00,  1.05it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 9/? [00:08<00:00,  1.07it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 9/? [00:08<00:00,  1.07it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 10/? [00:09<00:00,  1.10it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 10/? [00:09<00:00,  1.10it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 11/? [00:09<00:00,  1.12it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 11/? [00:09<00:00,  1.12it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 12/? [00:10<00:00,  1.14it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 12/? [00:10<00:00,  1.14it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 13/? [00:11<00:00,  1.15it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 13/? [00:11<00:00,  1.15it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 14/? [00:12<00:00,  1.16it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 14/? [00:12<00:00,  1.16it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 15/? [00:12<00:00,  1.18it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 15/? [00:12<00:00,  1.18it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 16/? [00:13<00:00,  1.19it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 16/? [00:13<00:00,  1.19it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 17/? [00:14<00:00,  1.20it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 17/? [00:14<00:00,  1.20it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 18/? [00:14<00:00,  1.21it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 18/? [00:14<00:00,  1.21it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 19/? [00:15<00:00,  1.21it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 19/? [00:15<00:00,  1.21it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 20/? [00:16<00:00,  1.22it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 20/? [00:16<00:00,  1.22it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 21/? [00:17<00:00,  1.23it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 21/? [00:17<00:00,  1.23it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 22/? [00:17<00:00,  1.23it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 22/? [00:17<00:00,  1.23it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 23/? [00:18<00:00,  1.24it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 23/? [00:18<00:00,  1.24it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 24/? [00:19<00:00,  1.24it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 24/? [00:19<00:00,  1.24it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 25/? [00:20<00:00,  1.25it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 25/? [00:20<00:00,  1.25it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 26/? [00:20<00:00,  1.25it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 26/? [00:20<00:00,  1.25it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 27/? [00:21<00:00,  1.25it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 27/? [00:21<00:00,  1.25it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 28/? [00:22<00:00,  1.26it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 28/? [00:22<00:00,  1.26it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 29/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 29/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 30/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 30/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 31/? [00:24<00:00,  1.27it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 31/? [00:24<00:00,  1.27it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 32/? [00:25<00:00,  1.27it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 32/? [00:25<00:00,  1.27it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 33/? [00:25<00:00,  1.27it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 33/? [00:25<00:00,  1.27it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 34/? [00:26<00:00,  1.27it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 34/? [00:26<00:00,  1.27it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 35/? [00:27<00:00,  1.28it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 35/? [00:27<00:00,  1.28it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 36/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 36/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 37/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 37/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 38/? [00:29<00:00,  1.28it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 38/? [00:29<00:00,  1.28it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 39/? [00:30<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 39/? [00:30<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 40/? [00:31<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 40/? [00:31<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 41/? [00:31<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 41/? [00:31<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 42/? [00:32<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 42/? [00:32<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 43/? [00:33<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 43/? [00:33<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 44/? [00:33<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 44/? [00:33<00:00,  1.29it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 45/? [00:34<00:00,  1.30it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 45/? [00:34<00:00,  1.30it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 46/? [00:35<00:00,  1.31it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 46/? [00:35<00:00,  1.31it/s, v_num=aa63, val_loss=0.356, val_accuracy=0.860, val_auprc=0.742, val_f1_macro=0.723, val_f1_weighted=0.870, val_precision_macro=0.698, val_precision_weighted=0.887, val_recall_macro=0.767, val_recall_weighted=0.860, val_f1_per_label_0=0.918, val_f1_per_label_1=0.528, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.18it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.10it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 1: |          | 46/? [00:40<00:00,  1.15it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 46/? [00:40<00:00,  1.15it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]         Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.2
Epoch 2: |          | 1/? [00:02<00:00,  0.39it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.548, train_accuracy=0.696, train_auprc=0.778, train_f1_macro=0.689, train_f1_weighted=0.693, train_precision_macro=0.695, train_precision_weighted=0.695, train_recall_macro=0.688, train_recall_weighted=0.696, train_f1_per_label_0=0.736, train_f1_per_label_1=0.641]Epoch 2: |          | 1/? [00:02<00:00,  0.39it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 2/? [00:03<00:00,  0.60it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 2/? [00:03<00:00,  0.60it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 3/? [00:04<00:00,  0.74it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 3/? [00:04<00:00,  0.74it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 4/? [00:04<00:00,  0.83it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 4/? [00:04<00:00,  0.83it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 5/? [00:05<00:00,  0.90it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 5/? [00:05<00:00,  0.90it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 6/? [00:06<00:00,  0.96it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 6/? [00:06<00:00,  0.96it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 7/? [00:07<00:00,  1.00it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 7/? [00:07<00:00,  1.00it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 8/? [00:07<00:00,  1.03it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 8/? [00:07<00:00,  1.03it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 9/? [00:08<00:00,  1.06it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 9/? [00:08<00:00,  1.06it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 10/? [00:09<00:00,  1.09it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 10/? [00:09<00:00,  1.09it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 11/? [00:09<00:00,  1.11it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 11/? [00:09<00:00,  1.11it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 12/? [00:10<00:00,  1.13it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 12/? [00:10<00:00,  1.13it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 14/? [00:12<00:00,  1.15it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 14/? [00:12<00:00,  1.15it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 15/? [00:12<00:00,  1.17it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 15/? [00:12<00:00,  1.17it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 16/? [00:13<00:00,  1.18it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 16/? [00:13<00:00,  1.18it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 17/? [00:14<00:00,  1.19it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 17/? [00:14<00:00,  1.19it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 18/? [00:15<00:00,  1.20it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 18/? [00:15<00:00,  1.20it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 19/? [00:15<00:00,  1.20it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 19/? [00:15<00:00,  1.20it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 21/? [00:17<00:00,  1.22it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 21/? [00:17<00:00,  1.22it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 22/? [00:17<00:00,  1.22it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 22/? [00:17<00:00,  1.22it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 24/? [00:19<00:00,  1.23it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 24/? [00:19<00:00,  1.23it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 25/? [00:20<00:00,  1.24it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 25/? [00:20<00:00,  1.24it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 26/? [00:20<00:00,  1.24it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 26/? [00:20<00:00,  1.24it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 27/? [00:21<00:00,  1.25it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 27/? [00:21<00:00,  1.25it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 29/? [00:23<00:00,  1.25it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 29/? [00:23<00:00,  1.25it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 30/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 30/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 32/? [00:25<00:00,  1.26it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 32/? [00:25<00:00,  1.26it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 33/? [00:26<00:00,  1.27it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 33/? [00:26<00:00,  1.27it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 34/? [00:26<00:00,  1.27it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 34/? [00:26<00:00,  1.27it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 35/? [00:27<00:00,  1.27it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 35/? [00:27<00:00,  1.27it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 36/? [00:28<00:00,  1.27it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 36/? [00:28<00:00,  1.27it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 37/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 37/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 38/? [00:29<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 38/? [00:29<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 39/? [00:30<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 39/? [00:30<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 40/? [00:31<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 40/? [00:31<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 41/? [00:31<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 41/? [00:31<00:00,  1.28it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 42/? [00:32<00:00,  1.29it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 42/? [00:32<00:00,  1.29it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 43/? [00:33<00:00,  1.29it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 43/? [00:33<00:00,  1.29it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 44/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 44/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 45/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 45/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 46/? [00:35<00:00,  1.31it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 46/? [00:35<00:00,  1.31it/s, v_num=aa63, val_loss=0.322, val_accuracy=0.910, val_auprc=0.800, val_f1_macro=0.746, val_f1_weighted=0.900, val_precision_macro=0.819, val_precision_weighted=0.900, val_recall_macro=0.707, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.542, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.01it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.03it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.03it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.03it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.04it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.05it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.05it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.06it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.27it/s][A
                                                                        [AEpoch 2: |          | 46/? [00:40<00:00,  1.14it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 46/? [00:40<00:00,  1.14it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]         Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.2
Epoch 3: |          | 1/? [00:02<00:00,  0.37it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.106, train_accuracy=0.965, train_auprc=0.990, train_f1_macro=0.965, train_f1_weighted=0.965, train_precision_macro=0.965, train_precision_weighted=0.966, train_recall_macro=0.965, train_recall_weighted=0.965, train_f1_per_label_0=0.968, train_f1_per_label_1=0.962]Epoch 3: |          | 1/? [00:02<00:00,  0.37it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 2/? [00:03<00:00,  0.59it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 2/? [00:03<00:00,  0.59it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 3/? [00:04<00:00,  0.72it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 3/? [00:04<00:00,  0.72it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 4/? [00:04<00:00,  0.82it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 4/? [00:04<00:00,  0.82it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 5/? [00:05<00:00,  0.89it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 5/? [00:05<00:00,  0.89it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 6/? [00:06<00:00,  0.95it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 6/? [00:06<00:00,  0.94it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 7/? [00:07<00:00,  0.99it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 7/? [00:07<00:00,  0.99it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 8/? [00:07<00:00,  1.02it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 8/? [00:07<00:00,  1.02it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 9/? [00:08<00:00,  1.05it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 9/? [00:08<00:00,  1.05it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 10/? [00:09<00:00,  1.07it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 10/? [00:09<00:00,  1.07it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 11/? [00:10<00:00,  1.10it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 11/? [00:10<00:00,  1.10it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 12/? [00:10<00:00,  1.11it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 12/? [00:10<00:00,  1.11it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 13/? [00:11<00:00,  1.13it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 13/? [00:11<00:00,  1.13it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 14/? [00:12<00:00,  1.14it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 14/? [00:12<00:00,  1.14it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 15/? [00:12<00:00,  1.16it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 15/? [00:12<00:00,  1.16it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 16/? [00:13<00:00,  1.17it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 16/? [00:13<00:00,  1.17it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 17/? [00:14<00:00,  1.18it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 17/? [00:14<00:00,  1.18it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 18/? [00:15<00:00,  1.19it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 18/? [00:15<00:00,  1.19it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 19/? [00:15<00:00,  1.19it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 19/? [00:15<00:00,  1.19it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 20/? [00:16<00:00,  1.20it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 20/? [00:16<00:00,  1.20it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 21/? [00:17<00:00,  1.21it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 21/? [00:17<00:00,  1.21it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 22/? [00:18<00:00,  1.21it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 22/? [00:18<00:00,  1.21it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 23/? [00:18<00:00,  1.22it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 23/? [00:18<00:00,  1.22it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 24/? [00:19<00:00,  1.22it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 24/? [00:19<00:00,  1.22it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 25/? [00:20<00:00,  1.23it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 25/? [00:20<00:00,  1.23it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 26/? [00:21<00:00,  1.23it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 26/? [00:21<00:00,  1.23it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 27/? [00:21<00:00,  1.24it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 27/? [00:21<00:00,  1.24it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 28/? [00:22<00:00,  1.24it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 28/? [00:22<00:00,  1.24it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 29/? [00:23<00:00,  1.25it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 29/? [00:23<00:00,  1.25it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 30/? [00:24<00:00,  1.25it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 30/? [00:24<00:00,  1.25it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 31/? [00:24<00:00,  1.25it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 31/? [00:24<00:00,  1.25it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 32/? [00:25<00:00,  1.26it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 32/? [00:25<00:00,  1.26it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 33/? [00:26<00:00,  1.26it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 33/? [00:26<00:00,  1.26it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 34/? [00:26<00:00,  1.26it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 34/? [00:26<00:00,  1.26it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 35/? [00:27<00:00,  1.26it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 35/? [00:27<00:00,  1.26it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 36/? [00:28<00:00,  1.27it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 36/? [00:28<00:00,  1.27it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 37/? [00:29<00:00,  1.27it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 37/? [00:29<00:00,  1.27it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 38/? [00:29<00:00,  1.27it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 38/? [00:29<00:00,  1.27it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 39/? [00:30<00:00,  1.27it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 39/? [00:30<00:00,  1.27it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 40/? [00:31<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 40/? [00:31<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 41/? [00:32<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 41/? [00:32<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 42/? [00:32<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 42/? [00:32<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 43/? [00:33<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 43/? [00:33<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 44/? [00:34<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 44/? [00:34<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 45/? [00:35<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 45/? [00:35<00:00,  1.28it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 46/? [00:35<00:00,  1.30it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 46/? [00:35<00:00,  1.30it/s, v_num=aa63, val_loss=0.324, val_accuracy=0.925, val_auprc=0.837, val_f1_macro=0.793, val_f1_weighted=0.918, val_precision_macro=0.864, val_precision_weighted=0.919, val_recall_macro=0.751, val_recall_weighted=0.925, val_f1_per_label_0=0.958, val_f1_per_label_1=0.628, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.03it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.98it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.02it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 3: |          | 46/? [00:40<00:00,  1.14it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 46/? [00:40<00:00,  1.14it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]         Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.2
Epoch 4: |          | 1/? [00:02<00:00,  0.40it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.0223, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.993, train_f1_weighted=0.993, train_precision_macro=0.993, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.993]Epoch 4: |          | 1/? [00:02<00:00,  0.40it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991] Epoch 4: |          | 2/? [00:03<00:00,  0.62it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 2/? [00:03<00:00,  0.62it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 3/? [00:03<00:00,  0.75it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 3/? [00:03<00:00,  0.75it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 4/? [00:04<00:00,  0.85it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 4/? [00:04<00:00,  0.85it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 5/? [00:05<00:00,  0.92it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 5/? [00:05<00:00,  0.92it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 6/? [00:06<00:00,  0.97it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 6/? [00:06<00:00,  0.97it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 7/? [00:06<00:00,  1.01it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 7/? [00:06<00:00,  1.01it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 8/? [00:07<00:00,  1.04it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 8/? [00:07<00:00,  1.04it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 9/? [00:08<00:00,  1.07it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 9/? [00:08<00:00,  1.07it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 10/? [00:09<00:00,  1.09it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 10/? [00:09<00:00,  1.09it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 11/? [00:09<00:00,  1.11it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 11/? [00:09<00:00,  1.11it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 12/? [00:10<00:00,  1.13it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 12/? [00:10<00:00,  1.13it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 13/? [00:11<00:00,  1.15it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 13/? [00:11<00:00,  1.14it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 14/? [00:12<00:00,  1.16it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 14/? [00:12<00:00,  1.16it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 15/? [00:12<00:00,  1.17it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 15/? [00:12<00:00,  1.17it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 16/? [00:13<00:00,  1.18it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 16/? [00:13<00:00,  1.18it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 17/? [00:14<00:00,  1.19it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 17/? [00:14<00:00,  1.19it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 18/? [00:15<00:00,  1.20it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 18/? [00:15<00:00,  1.20it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 19/? [00:15<00:00,  1.21it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 19/? [00:15<00:00,  1.21it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 20/? [00:16<00:00,  1.21it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 20/? [00:16<00:00,  1.21it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 21/? [00:17<00:00,  1.22it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 21/? [00:17<00:00,  1.22it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 22/? [00:17<00:00,  1.23it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 22/? [00:17<00:00,  1.23it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 23/? [00:18<00:00,  1.23it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 23/? [00:18<00:00,  1.23it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 24/? [00:19<00:00,  1.24it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 24/? [00:19<00:00,  1.24it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 25/? [00:20<00:00,  1.24it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 25/? [00:20<00:00,  1.24it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 26/? [00:20<00:00,  1.25it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 26/? [00:20<00:00,  1.25it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 27/? [00:21<00:00,  1.25it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 27/? [00:21<00:00,  1.25it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 28/? [00:22<00:00,  1.25it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 28/? [00:22<00:00,  1.25it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 29/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 29/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 30/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 30/? [00:23<00:00,  1.26it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 31/? [00:24<00:00,  1.26it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 31/? [00:24<00:00,  1.26it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 32/? [00:25<00:00,  1.27it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 32/? [00:25<00:00,  1.27it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 33/? [00:25<00:00,  1.27it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 33/? [00:25<00:00,  1.27it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 34/? [00:26<00:00,  1.27it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 34/? [00:26<00:00,  1.27it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 35/? [00:27<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 35/? [00:27<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 36/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 36/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 37/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 37/? [00:28<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 38/? [00:29<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 38/? [00:29<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 39/? [00:30<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 39/? [00:30<00:00,  1.28it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 40/? [00:31<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 40/? [00:31<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 41/? [00:31<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 41/? [00:31<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 42/? [00:32<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 42/? [00:32<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 43/? [00:33<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 43/? [00:33<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 44/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 44/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 45/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 45/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 46/? [00:35<00:00,  1.31it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 46/? [00:35<00:00,  1.31it/s, v_num=aa63, val_loss=0.443, val_accuracy=0.918, val_auprc=0.830, val_f1_macro=0.753, val_f1_weighted=0.906, val_precision_macro=0.879, val_precision_weighted=0.913, val_recall_macro=0.700, val_recall_weighted=0.918, val_f1_per_label_0=0.955, val_f1_per_label_1=0.550, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.04it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.08it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.30it/s][A
                                                                        [AEpoch 4: |          | 46/? [00:40<00:00,  1.14it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 46/? [00:40<00:00,  1.14it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]         Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.2
Epoch 5: |          | 1/? [00:02<00:00,  0.38it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.026, train_accuracy=0.992, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.992, train_precision_macro=0.992, train_precision_weighted=0.992, train_recall_macro=0.992, train_recall_weighted=0.992, train_f1_per_label_0=0.993, train_f1_per_label_1=0.991]Epoch 5: |          | 1/? [00:02<00:00,  0.38it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 2/? [00:03<00:00,  0.60it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 2/? [00:03<00:00,  0.60it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 3/? [00:04<00:00,  0.74it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 3/? [00:04<00:00,  0.74it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 4/? [00:04<00:00,  0.83it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 4/? [00:04<00:00,  0.83it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 5/? [00:05<00:00,  0.90it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 5/? [00:05<00:00,  0.90it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 6/? [00:06<00:00,  0.95it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 6/? [00:06<00:00,  0.95it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 7/? [00:07<00:00,  1.00it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 7/? [00:07<00:00,  1.00it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 8/? [00:07<00:00,  1.03it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 8/? [00:07<00:00,  1.03it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 9/? [00:08<00:00,  1.06it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 9/? [00:08<00:00,  1.06it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 10/? [00:09<00:00,  1.08it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 10/? [00:09<00:00,  1.08it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 11/? [00:09<00:00,  1.10it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 11/? [00:09<00:00,  1.10it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 12/? [00:10<00:00,  1.12it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 12/? [00:10<00:00,  1.12it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 13/? [00:11<00:00,  1.13it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 13/? [00:11<00:00,  1.13it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 14/? [00:12<00:00,  1.15it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 14/? [00:12<00:00,  1.15it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 15/? [00:12<00:00,  1.16it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 15/? [00:12<00:00,  1.16it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 16/? [00:13<00:00,  1.17it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 16/? [00:13<00:00,  1.17it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 17/? [00:14<00:00,  1.18it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 17/? [00:14<00:00,  1.18it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 18/? [00:15<00:00,  1.19it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 18/? [00:15<00:00,  1.19it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 19/? [00:15<00:00,  1.20it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 19/? [00:15<00:00,  1.20it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 20/? [00:16<00:00,  1.20it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 20/? [00:16<00:00,  1.20it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 21/? [00:17<00:00,  1.21it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 21/? [00:17<00:00,  1.21it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 22/? [00:18<00:00,  1.22it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 22/? [00:18<00:00,  1.22it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 23/? [00:18<00:00,  1.22it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 23/? [00:18<00:00,  1.22it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 24/? [00:19<00:00,  1.23it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 24/? [00:19<00:00,  1.23it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 25/? [00:20<00:00,  1.23it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 25/? [00:20<00:00,  1.23it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 26/? [00:21<00:00,  1.24it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 26/? [00:21<00:00,  1.24it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 27/? [00:21<00:00,  1.24it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 27/? [00:21<00:00,  1.24it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 28/? [00:22<00:00,  1.24it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 28/? [00:22<00:00,  1.24it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 29/? [00:23<00:00,  1.25it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 29/? [00:23<00:00,  1.25it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 30/? [00:23<00:00,  1.25it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 30/? [00:23<00:00,  1.25it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 31/? [00:24<00:00,  1.25it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 31/? [00:24<00:00,  1.25it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 32/? [00:25<00:00,  1.26it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 32/? [00:25<00:00,  1.26it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 33/? [00:26<00:00,  1.26it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 33/? [00:26<00:00,  1.26it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 34/? [00:26<00:00,  1.26it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 34/? [00:26<00:00,  1.26it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 35/? [00:27<00:00,  1.27it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 35/? [00:27<00:00,  1.27it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 36/? [00:28<00:00,  1.27it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 36/? [00:28<00:00,  1.27it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 37/? [00:29<00:00,  1.27it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 37/? [00:29<00:00,  1.27it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 38/? [00:29<00:00,  1.27it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 38/? [00:29<00:00,  1.27it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 39/? [00:30<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 39/? [00:30<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 40/? [00:31<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 40/? [00:31<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 41/? [00:32<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 41/? [00:32<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 42/? [00:32<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 42/? [00:32<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 43/? [00:33<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 43/? [00:33<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 44/? [00:34<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 44/? [00:34<00:00,  1.28it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 45/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 45/? [00:34<00:00,  1.29it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 46/? [00:35<00:00,  1.30it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 46/? [00:35<00:00,  1.30it/s, v_num=aa63, val_loss=0.483, val_accuracy=0.922, val_auprc=0.827, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.14it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.15it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.12it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.33it/s][A
                                                                        [AEpoch 5: |          | 46/? [00:40<00:00,  1.14it/s, v_num=aa63, val_loss=0.510, val_accuracy=0.922, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 46/? [00:40<00:00,  1.14it/s, v_num=aa63, val_loss=0.510, val_accuracy=0.922, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Epoch 5: |          | 46/? [00:40<00:00,  1.13it/s, v_num=aa63, val_loss=0.510, val_accuracy=0.922, val_auprc=0.824, val_f1_macro=0.781, val_f1_weighted=0.914, val_precision_macro=0.858, val_precision_weighted=0.915, val_recall_macro=0.737, val_recall_weighted=0.922, val_f1_per_label_0=0.957, val_f1_per_label_1=0.605, train_loss=0.00115, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=1.000, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=1.000]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/0ac83a63eb7f47ea9aafca0141d3aa63/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_2-oversampling-sampling_modifiedRS_rho=1.2-seed0-epoch=02-val_f1_macro=0.79.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/0ac83a63eb7f47ea9aafca0141d3aa63/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_2-oversampling-sampling_modifiedRS_rho=1.2-seed0-epoch=02-val_f1_macro=0.79.ckpt

Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.81it/s]Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.93it/s]Validation DataLoader 0:  30%|███       | 3/10 [00:01<00:02,  2.96it/s]Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:02,  2.97it/s]Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  2.99it/s]Validation DataLoader 0:  60%|██████    | 6/10 [00:02<00:01,  3.00it/s]Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.00it/s]Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.01it/s]Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.02it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.23it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  2.97it/s]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/0ac83a63eb7f47ea9aafca0141d3aa63/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_2-oversampling-sampling_modifiedRS_rho=1.2-seed0-epoch=02-val_f1_macro=0.79.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/0ac83a63eb7f47ea9aafca0141d3aa63/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_2-oversampling-sampling_modifiedRS_rho=1.2-seed0-epoch=02-val_f1_macro=0.79.ckpt

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
     Validate metric           DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      val_accuracy           0.925000011920929
        val_auprc            0.837302565574646
      val_f1_macro          0.7931969165802002
   val_f1_per_label_0        0.958294689655304
   val_f1_per_label_1       0.6280992031097412
     val_f1_weighted        0.9181209206581116
        val_loss            0.32371124625205994
   val_precision_macro       0.864130437374115
 val_precision_weighted     0.9189613461494446
    val_recall_macro         0.750786304473877
   val_recall_weighted       0.925000011920929
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.93it/s]Testing DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.96it/s]Testing DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.00it/s]Testing DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.03it/s]Testing DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.03it/s]Testing DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.04it/s]Testing DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.03it/s]Testing DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.03it/s]Testing DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.03it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.25it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  2.99it/s]Seed set to 0
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python imbalanced_text_classification/main.py --data_name u ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]

  | Name               | Type                          | Params
---------------------------------------------------------------------
0 | classifier         | BertForSequenceClassification | 109 M 
1 | loss_fn            | CrossEntropyLoss              | 0     
2 | train_metrics      | MetricCollection              | 0     
3 | train_f1_per_label | MulticlassF1Score             | 0     
4 | val_metrics        | MetricCollection              | 0     
5 | val_f1_per_label   | MulticlassF1Score             | 0     
6 | test_metrics       | MetricCollection              | 0     
7 | test_f1_per_label  | MulticlassF1Score             | 0     
---------------------------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.935   Total estimated model params size (MB)

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      test_accuracy         0.9116666913032532
       test_auprc           0.7618160247802734
      test_f1_macro          0.723418116569519
   test_f1_per_label_0      0.9515981674194336
   test_f1_per_label_1      0.4952380955219269
    test_f1_weighted        0.9067228436470032
  test_precision_macro      0.7528252601623535
 test_precision_weighted    0.9035369753837585
    test_recall_macro       0.7018547058105469
  test_recall_weighted      0.9116666913032532
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
END running Trial 2.



MLFlow run_name = sampling_modifiedRS-us-election-2020-bin-Trial-3
MLflow Saved Child Search Trial 3 Log in './mlruns/5/58d6c380a1e341b1bd71c4204f65f980'
Using Cross Entropy Loss: alpha=None
Runing optuna for hyperparameter search:
Parmas: {'sampling_modifiedRS_mode': 'oversampling', 'sampling_modifiedRS_rho': 2.0, 'sampling_weightedRS_percentage': None, 'wce_alpha': None, 'fl_gamma': None, 'loss': <Loss.CE_Loss: 'CE_Loss'>}
Calling ImbalancedDataModule.setup() for train...
------ Dataset Statistics ------
Raw data reading from CSV files:
| Split	 | Size	 | Label Counts
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
| train_val	 | 2400 |	0: 2107, 87.8%	1: 293, 12.2%
After splitting for training:
| Split	 | Size	 | Label Counts
| train	 | 1800 |	0: 1580, 87.8%	1: 220, 12.2%
| val	 | 600 |	0: 527, 87.8%	1: 73, 12.2%
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
Resampling with sampling_modifiedRS_rho = 2.0 => training set has 2370.0 samples.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2026457) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Label counts after resampling: {1: 790, 0: 1580}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] Epoch 0: |          | 1/? [00:00<00:00,  1.33it/s]Epoch 0: |          | 1/? [00:00<00:00,  1.33it/s, v_num=f980]Epoch 0: |          | 2/? [00:01<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 2/? [00:01<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 3/? [00:02<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 3/? [00:02<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 4/? [00:02<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 4/? [00:02<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 5/? [00:03<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 5/? [00:03<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 6/? [00:04<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 6/? [00:04<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 7/? [00:05<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 7/? [00:05<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 8/? [00:05<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 8/? [00:05<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 9/? [00:06<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 9/? [00:06<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 10/? [00:07<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 10/? [00:07<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 11/? [00:08<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 11/? [00:08<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 12/? [00:08<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 12/? [00:08<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 13/? [00:09<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 13/? [00:09<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 14/? [00:10<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 14/? [00:10<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 15/? [00:11<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 15/? [00:11<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 16/? [00:11<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 16/? [00:11<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 17/? [00:12<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 17/? [00:12<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 18/? [00:13<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 18/? [00:13<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 19/? [00:13<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 19/? [00:13<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 20/? [00:14<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 20/? [00:14<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 21/? [00:15<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 21/? [00:15<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 22/? [00:16<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 22/? [00:16<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 23/? [00:16<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 23/? [00:16<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 24/? [00:17<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 24/? [00:17<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 25/? [00:18<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 25/? [00:18<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 26/? [00:19<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 26/? [00:19<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 27/? [00:19<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 27/? [00:19<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 28/? [00:20<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 28/? [00:20<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 29/? [00:21<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 29/? [00:21<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 30/? [00:22<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 30/? [00:22<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 31/? [00:22<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 31/? [00:22<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 32/? [00:23<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 32/? [00:23<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 33/? [00:24<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 33/? [00:24<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 34/? [00:24<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 34/? [00:24<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 35/? [00:25<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 35/? [00:25<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 36/? [00:26<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 36/? [00:26<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 37/? [00:27<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 37/? [00:27<00:00,  1.36it/s, v_num=f980]Epoch 0: |          | 38/? [00:27<00:00,  1.38it/s, v_num=f980]Epoch 0: |          | 38/? [00:27<00:00,  1.38it/s, v_num=f980]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.94it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.00it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:01<00:02,  2.99it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.01it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.04it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.04it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.04it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.03it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.04it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.26it/s][A/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028

                                                                        [AEpoch 0: |          | 38/? [00:32<00:00,  1.16it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455]Epoch 0: |          | 38/? [00:32<00:00,  1.16it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455]Epoch 0: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455]         Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 1: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455]Epoch 1: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 2/? [00:03<00:00,  0.59it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 3/? [00:04<00:00,  0.73it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 3/? [00:04<00:00,  0.73it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 4/? [00:04<00:00,  0.83it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 4/? [00:04<00:00,  0.83it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 23/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 23/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 27/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 27/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 37/? [00:29<00:00,  1.28it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 37/? [00:29<00:00,  1.28it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.354, val_accuracy=0.840, val_auprc=0.696, val_f1_macro=0.680, val_f1_weighted=0.851, val_precision_macro=0.661, val_precision_weighted=0.867, val_recall_macro=0.714, val_recall_weighted=0.840, val_f1_per_label_0=0.906, val_f1_per_label_1=0.455, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.13it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.03it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s][A
                                                                        [AEpoch 1: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]         Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 2: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.617, train_accuracy=0.659, train_auprc=0.612, train_f1_macro=0.440, train_f1_weighted=0.557, train_precision_macro=0.539, train_precision_weighted=0.583, train_recall_macro=0.507, train_recall_weighted=0.659, train_f1_per_label_0=0.790, train_f1_per_label_1=0.0901]Epoch 2: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827] Epoch 2: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 3/? [00:04<00:00,  0.73it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 3/? [00:04<00:00,  0.73it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 4/? [00:04<00:00,  0.82it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 4/? [00:04<00:00,  0.82it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 5/? [00:05<00:00,  0.89it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 29/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 29/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 32/? [00:25<00:00,  1.27it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 32/? [00:25<00:00,  1.27it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 36/? [00:28<00:00,  1.28it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 36/? [00:28<00:00,  1.28it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 37/? [00:28<00:00,  1.28it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 37/? [00:28<00:00,  1.28it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.267, val_accuracy=0.893, val_auprc=0.814, val_f1_macro=0.762, val_f1_weighted=0.896, val_precision_macro=0.751, val_precision_weighted=0.899, val_recall_macro=0.774, val_recall_weighted=0.893, val_f1_per_label_0=0.939, val_f1_per_label_1=0.584, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.10it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.33it/s][A
                                                                        [AEpoch 2: |          | 38/? [00:34<00:00,  1.11it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 38/? [00:34<00:00,  1.11it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]         Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 3: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.290, train_accuracy=0.880, train_auprc=0.927, train_f1_macro=0.868, train_f1_weighted=0.881, train_precision_macro=0.862, train_precision_weighted=0.883, train_recall_macro=0.875, train_recall_weighted=0.880, train_f1_per_label_0=0.908, train_f1_per_label_1=0.827]Epoch 3: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 3/? [00:04<00:00,  0.74it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 3/? [00:04<00:00,  0.74it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 4/? [00:04<00:00,  0.83it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 4/? [00:04<00:00,  0.83it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 6/? [00:06<00:00,  0.96it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 6/? [00:06<00:00,  0.96it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 7/? [00:06<00:00,  1.00it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 7/? [00:06<00:00,  1.00it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 8/? [00:07<00:00,  1.04it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 8/? [00:07<00:00,  1.04it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 10/? [00:09<00:00,  1.09it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 10/? [00:09<00:00,  1.09it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 11/? [00:09<00:00,  1.11it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 11/? [00:09<00:00,  1.11it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 12/? [00:10<00:00,  1.13it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 12/? [00:10<00:00,  1.13it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 15/? [00:12<00:00,  1.17it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 15/? [00:12<00:00,  1.17it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 16/? [00:13<00:00,  1.18it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 16/? [00:13<00:00,  1.18it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 17/? [00:14<00:00,  1.19it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 17/? [00:14<00:00,  1.19it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 18/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 18/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 22/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 22/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 29/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 29/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 37/? [00:28<00:00,  1.28it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 37/? [00:28<00:00,  1.28it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.397, val_accuracy=0.910, val_auprc=0.815, val_f1_macro=0.688, val_f1_weighted=0.887, val_precision_macro=0.930, val_precision_weighted=0.914, val_recall_macro=0.636, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.426, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.10it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.10it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 3: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]         Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 4: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0714, train_accuracy=0.979, train_auprc=0.992, train_f1_macro=0.977, train_f1_weighted=0.979, train_precision_macro=0.977, train_precision_weighted=0.979, train_recall_macro=0.977, train_recall_weighted=0.979, train_f1_per_label_0=0.984, train_f1_per_label_1=0.969]Epoch 4: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 2/? [00:03<00:00,  0.59it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 2/? [00:03<00:00,  0.59it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 3/? [00:04<00:00,  0.73it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 3/? [00:04<00:00,  0.73it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 4/? [00:04<00:00,  0.82it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 4/? [00:04<00:00,  0.82it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 5/? [00:05<00:00,  0.89it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 5/? [00:05<00:00,  0.89it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 8/? [00:07<00:00,  1.02it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 8/? [00:07<00:00,  1.02it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 9/? [00:08<00:00,  1.05it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 9/? [00:08<00:00,  1.05it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 10/? [00:09<00:00,  1.07it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 10/? [00:09<00:00,  1.07it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 11/? [00:10<00:00,  1.09it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 11/? [00:10<00:00,  1.09it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 12/? [00:10<00:00,  1.11it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 12/? [00:10<00:00,  1.11it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 13/? [00:11<00:00,  1.13it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 13/? [00:11<00:00,  1.13it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 14/? [00:12<00:00,  1.14it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 14/? [00:12<00:00,  1.14it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 20/? [00:16<00:00,  1.20it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 20/? [00:16<00:00,  1.20it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 23/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 23/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 25/? [00:20<00:00,  1.23it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 25/? [00:20<00:00,  1.23it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 26/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 26/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 27/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 27/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 30/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 30/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 33/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 33/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 34/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 34/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 37/? [00:29<00:00,  1.27it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 37/? [00:29<00:00,  1.27it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.366, val_accuracy=0.908, val_auprc=0.815, val_f1_macro=0.792, val_f1_weighted=0.910, val_precision_macro=0.784, val_precision_weighted=0.911, val_recall_macro=0.800, val_recall_weighted=0.908, val_f1_per_label_0=0.948, val_f1_per_label_1=0.636, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.00it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.03it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.05it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.05it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.29it/s][A
                                                                        [AEpoch 4: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]         Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 5: |          | 1/? [00:02<00:00,  0.39it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0394, train_accuracy=0.985, train_auprc=0.999, train_f1_macro=0.983, train_f1_weighted=0.985, train_precision_macro=0.984, train_precision_weighted=0.985, train_recall_macro=0.982, train_recall_weighted=0.985, train_f1_per_label_0=0.989, train_f1_per_label_1=0.977]Epoch 5: |          | 1/? [00:02<00:00,  0.39it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 3/? [00:04<00:00,  0.74it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 3/? [00:04<00:00,  0.74it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 4/? [00:04<00:00,  0.83it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 4/? [00:04<00:00,  0.83it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 13/? [00:11<00:00,  1.13it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 13/? [00:11<00:00,  1.13it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 20/? [00:16<00:00,  1.20it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 20/? [00:16<00:00,  1.20it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 23/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 23/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 25/? [00:20<00:00,  1.23it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 25/? [00:20<00:00,  1.23it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 26/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 26/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 27/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 27/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 30/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 30/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 33/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 33/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 34/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 34/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 37/? [00:29<00:00,  1.27it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 37/? [00:29<00:00,  1.27it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 38/? [00:29<00:00,  1.29it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 38/? [00:29<00:00,  1.29it/s, v_num=f980, val_loss=0.483, val_accuracy=0.910, val_auprc=0.793, val_f1_macro=0.705, val_f1_weighted=0.891, val_precision_macro=0.882, val_precision_weighted=0.905, val_recall_macro=0.654, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.460, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 5: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 38/? [00:34<00:00,  1.09it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]         Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 6: |          | 1/? [00:02<00:00,  0.37it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.0134, train_accuracy=0.996, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.996, train_precision_macro=0.996, train_precision_weighted=0.996, train_recall_macro=0.996, train_recall_weighted=0.996, train_f1_per_label_0=0.997, train_f1_per_label_1=0.994]Epoch 6: |          | 1/? [00:02<00:00,  0.37it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 2/? [00:03<00:00,  0.59it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 2/? [00:03<00:00,  0.59it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 3/? [00:04<00:00,  0.72it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 3/? [00:04<00:00,  0.72it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 4/? [00:04<00:00,  0.82it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 4/? [00:04<00:00,  0.82it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 5/? [00:05<00:00,  0.89it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 5/? [00:05<00:00,  0.89it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 33/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 33/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 37/? [00:29<00:00,  1.27it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 37/? [00:29<00:00,  1.27it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.422, val_accuracy=0.920, val_auprc=0.814, val_f1_macro=0.806, val_f1_weighted=0.919, val_precision_macro=0.818, val_precision_weighted=0.917, val_recall_macro=0.795, val_recall_weighted=0.920, val_f1_per_label_0=0.955, val_f1_per_label_1=0.657, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.02it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.29it/s][A
                                                                        [AEpoch 6: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]         Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 7: |          | 1/? [00:02<00:00,  0.39it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00345, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.999, train_precision_weighted=0.999, train_recall_macro=0.998, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 1/? [00:02<00:00,  0.39it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 2/? [00:03<00:00,  0.60it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 3/? [00:04<00:00,  0.74it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 3/? [00:04<00:00,  0.74it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 4/? [00:04<00:00,  0.84it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 4/? [00:04<00:00,  0.84it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 5/? [00:05<00:00,  0.91it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 5/? [00:05<00:00,  0.91it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 6/? [00:06<00:00,  0.96it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 6/? [00:06<00:00,  0.96it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 7/? [00:06<00:00,  1.00it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 7/? [00:06<00:00,  1.00it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 8/? [00:07<00:00,  1.04it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 8/? [00:07<00:00,  1.04it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 9/? [00:08<00:00,  1.07it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 9/? [00:08<00:00,  1.07it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 10/? [00:09<00:00,  1.09it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 10/? [00:09<00:00,  1.09it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 11/? [00:09<00:00,  1.11it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 11/? [00:09<00:00,  1.11it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 12/? [00:10<00:00,  1.13it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 12/? [00:10<00:00,  1.13it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 14/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 14/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 15/? [00:12<00:00,  1.17it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 15/? [00:12<00:00,  1.17it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 16/? [00:13<00:00,  1.18it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 16/? [00:13<00:00,  1.18it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 17/? [00:14<00:00,  1.19it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 17/? [00:14<00:00,  1.19it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 18/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 18/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 22/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 22/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 37/? [00:28<00:00,  1.28it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 37/? [00:28<00:00,  1.28it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.536, val_accuracy=0.900, val_auprc=0.788, val_f1_macro=0.794, val_f1_weighted=0.906, val_precision_macro=0.766, val_precision_weighted=0.916, val_recall_macro=0.837, val_recall_weighted=0.900, val_f1_per_label_0=0.942, val_f1_per_label_1=0.647, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.04it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.04it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.04it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.26it/s][A
                                                                        [AEpoch 7: |          | 38/? [00:34<00:00,  1.09it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 38/? [00:34<00:00,  1.09it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]         Epoch 8: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 8: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.00561, train_accuracy=0.999, train_auprc=1.000, train_f1_macro=0.999, train_f1_weighted=0.999, train_precision_macro=0.998, train_precision_weighted=0.999, train_recall_macro=0.999, train_recall_weighted=0.999, train_f1_per_label_0=0.999, train_f1_per_label_1=0.998]Epoch 8: |          | 1/? [00:02<00:00,  0.38it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995] Epoch 8: |          | 2/? [00:03<00:00,  0.59it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 2/? [00:03<00:00,  0.59it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 3/? [00:04<00:00,  0.73it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 3/? [00:04<00:00,  0.73it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 4/? [00:04<00:00,  0.82it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 4/? [00:04<00:00,  0.82it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 6/? [00:06<00:00,  0.95it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 7/? [00:07<00:00,  0.99it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 10/? [00:09<00:00,  1.08it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 11/? [00:09<00:00,  1.10it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 16/? [00:13<00:00,  1.17it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 20/? [00:16<00:00,  1.20it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 20/? [00:16<00:00,  1.20it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 21/? [00:17<00:00,  1.21it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 23/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 23/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 25/? [00:20<00:00,  1.23it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 25/? [00:20<00:00,  1.23it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 26/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 26/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 27/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 27/? [00:21<00:00,  1.24it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 30/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 30/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 33/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 33/? [00:26<00:00,  1.26it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 37/? [00:29<00:00,  1.27it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 37/? [00:29<00:00,  1.27it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.410, val_accuracy=0.927, val_auprc=0.826, val_f1_macro=0.810, val_f1_weighted=0.923, val_precision_macro=0.850, val_precision_weighted=0.922, val_recall_macro=0.781, val_recall_weighted=0.927, val_f1_per_label_0=0.959, val_f1_per_label_1=0.662, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 8: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 8: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]         Epoch 9: |          | 0/? [00:00<?, ?it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 2.0
Epoch 9: |          | 1/? [00:02<00:00,  0.40it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.0122, train_accuracy=0.997, train_auprc=1.000, train_f1_macro=0.996, train_f1_weighted=0.997, train_precision_macro=0.996, train_precision_weighted=0.997, train_recall_macro=0.997, train_recall_weighted=0.997, train_f1_per_label_0=0.997, train_f1_per_label_1=0.995]Epoch 9: |          | 1/? [00:02<00:00,  0.40it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 2/? [00:03<00:00,  0.61it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 2/? [00:03<00:00,  0.61it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 3/? [00:04<00:00,  0.75it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 3/? [00:04<00:00,  0.75it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 4/? [00:04<00:00,  0.83it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 4/? [00:04<00:00,  0.83it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 5/? [00:05<00:00,  0.90it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 6/? [00:06<00:00,  0.96it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 6/? [00:06<00:00,  0.96it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 7/? [00:07<00:00,  1.00it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 7/? [00:07<00:00,  1.00it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 8/? [00:07<00:00,  1.03it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 9/? [00:08<00:00,  1.06it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 10/? [00:09<00:00,  1.09it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 10/? [00:09<00:00,  1.09it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 11/? [00:09<00:00,  1.11it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 11/? [00:09<00:00,  1.11it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 12/? [00:10<00:00,  1.12it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 13/? [00:11<00:00,  1.14it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 14/? [00:12<00:00,  1.15it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 15/? [00:12<00:00,  1.16it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 16/? [00:13<00:00,  1.18it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 16/? [00:13<00:00,  1.18it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 17/? [00:14<00:00,  1.18it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 18/? [00:15<00:00,  1.19it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 19/? [00:15<00:00,  1.20it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 20/? [00:16<00:00,  1.21it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 21/? [00:17<00:00,  1.22it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 22/? [00:18<00:00,  1.22it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 23/? [00:18<00:00,  1.23it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 24/? [00:19<00:00,  1.23it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 25/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 26/? [00:20<00:00,  1.24it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 27/? [00:21<00:00,  1.25it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 28/? [00:22<00:00,  1.25it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 29/? [00:23<00:00,  1.25it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 30/? [00:23<00:00,  1.26it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 31/? [00:24<00:00,  1.26it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 32/? [00:25<00:00,  1.26it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 33/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 34/? [00:26<00:00,  1.27it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 35/? [00:27<00:00,  1.27it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 36/? [00:28<00:00,  1.27it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 37/? [00:29<00:00,  1.28it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 37/? [00:29<00:00,  1.28it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 38/? [00:29<00:00,  1.30it/s, v_num=f980, val_loss=0.461, val_accuracy=0.915, val_auprc=0.816, val_f1_macro=0.775, val_f1_weighted=0.909, val_precision_macro=0.820, val_precision_weighted=0.908, val_recall_macro=0.745, val_recall_weighted=0.915, val_f1_per_label_0=0.952, val_f1_per_label_1=0.598, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.12it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.33it/s][A
                                                                        [AEpoch 9: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.498, val_accuracy=0.917, val_auprc=0.819, val_f1_macro=0.793, val_f1_weighted=0.914, val_precision_macro=0.813, val_precision_weighted=0.912, val_recall_macro=0.776, val_recall_weighted=0.917, val_f1_per_label_0=0.953, val_f1_per_label_1=0.632, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Epoch 9: |          | 38/? [00:34<00:00,  1.10it/s, v_num=f980, val_loss=0.498, val_accuracy=0.917, val_auprc=0.819, val_f1_macro=0.793, val_f1_weighted=0.914, val_precision_macro=0.813, val_precision_weighted=0.912, val_recall_macro=0.776, val_recall_weighted=0.917, val_f1_per_label_0=0.953, val_f1_per_label_1=0.632, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]`Trainer.fit` stopped: `max_epochs=10` reached.
Epoch 9: |          | 38/? [00:34<00:00,  1.09it/s, v_num=f980, val_loss=0.498, val_accuracy=0.917, val_auprc=0.819, val_f1_macro=0.793, val_f1_weighted=0.914, val_precision_macro=0.813, val_precision_weighted=0.912, val_recall_macro=0.776, val_recall_weighted=0.917, val_f1_per_label_0=0.953, val_f1_per_label_1=0.632, train_loss=0.00393, train_accuracy=1.000, train_auprc=1.000, train_f1_macro=1.000, train_f1_weighted=1.000, train_precision_macro=0.999, train_precision_weighted=1.000, train_recall_macro=1.000, train_recall_weighted=1.000, train_f1_per_label_0=1.000, train_f1_per_label_1=0.999]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/58d6c380a1e341b1bd71c4204f65f980/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_3-oversampling-sampling_modifiedRS_rho=2.0-seed0-epoch=07-val_f1_macro=0.81.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/58d6c380a1e341b1bd71c4204f65f980/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_3-oversampling-sampling_modifiedRS_rho=2.0-seed0-epoch=07-val_f1_macro=0.81.ckpt

Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.84it/s]Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.94it/s]Validation DataLoader 0:  30%|███       | 3/10 [00:01<00:02,  2.96it/s]Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:02,  2.98it/s]Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.00it/s]Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.03it/s]Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.03it/s]Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.04it/s]Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.04it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.25it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  2.99it/s]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/58d6c380a1e341b1bd71c4204f65f980/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_3-oversampling-sampling_modifiedRS_rho=2.0-seed0-epoch=07-val_f1_macro=0.81.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/58d6c380a1e341b1bd71c4204f65f980/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_3-oversampling-sampling_modifiedRS_rho=2.0-seed0-epoch=07-val_f1_macro=0.81.ckpt

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
     Validate metric           DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      val_accuracy          0.9266666769981384
        val_auprc           0.8261783719062805
      val_f1_macro          0.8102084994316101
   val_f1_per_label_0       0.9588785171508789
   val_f1_per_label_1       0.6615384817123413
     val_f1_weighted        0.9227021336555481
        val_loss            0.4102017879486084
   val_precision_macro      0.8495686650276184
 val_precision_weighted      0.921590268611908
    val_recall_macro        0.7812378406524658
   val_recall_weighted      0.9266666769981384
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.02it/s]Testing DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.09it/s]Testing DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.10it/s]Testing DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s]Testing DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.10it/s]Testing DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.11it/s]Testing DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.11it/s]Testing DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s]Testing DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.12it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:02<00:00,  3.34it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.07it/s]Seed set to 0
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python imbalanced_text_classification/main.py --data_name u ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]

  | Name               | Type                          | Params
---------------------------------------------------------------------
0 | classifier         | BertForSequenceClassification | 109 M 
1 | loss_fn            | CrossEntropyLoss              | 0     
2 | train_metrics      | MetricCollection              | 0     
3 | train_f1_per_label | MulticlassF1Score             | 0     
4 | val_metrics        | MetricCollection              | 0     
5 | val_f1_per_label   | MulticlassF1Score             | 0     
6 | test_metrics       | MetricCollection              | 0     
7 | test_f1_per_label  | MulticlassF1Score             | 0     
---------------------------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.935   Total estimated model params size (MB)

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      test_accuracy         0.9133333563804626
       test_auprc           0.7653558254241943
      test_f1_macro         0.7439600825309753
   test_f1_per_label_0      0.9522058963775635
   test_f1_per_label_1      0.5357142686843872
    test_f1_weighted          0.9112508893013
  test_precision_macro      0.7565106153488159
 test_precision_weighted    0.9095239043235779
    test_recall_macro       0.7329803705215454
  test_recall_weighted      0.9133333563804626
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
END running Trial 3.


Trial 3 achieved value: 0.8102084994316101 with  1.7676% improvement

MLFlow run_name = sampling_modifiedRS-us-election-2020-bin-Trial-4
MLflow Saved Child Search Trial 4 Log in './mlruns/5/88ede3c8160e45d2b21bd78dabe6c05d'
Using Cross Entropy Loss: alpha=None
Runing optuna for hyperparameter search:
Parmas: {'sampling_modifiedRS_mode': 'oversampling', 'sampling_modifiedRS_rho': 1.5, 'sampling_weightedRS_percentage': None, 'wce_alpha': None, 'fl_gamma': None, 'loss': <Loss.CE_Loss: 'CE_Loss'>}
Calling ImbalancedDataModule.setup() for train...
------ Dataset Statistics ------
Raw data reading from CSV files:
| Split	 | Size	 | Label Counts
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
| train_val	 | 2400 |	0: 2107, 87.8%	1: 293, 12.2%
After splitting for training:
| Split	 | Size	 | Label Counts
| train	 | 1800 |	0: 1580, 87.8%	1: 220, 12.2%
| val	 | 600 |	0: 527, 87.8%	1: 73, 12.2%
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
Resampling with sampling_modifiedRS_rho = 1.5 => training set has 2633.333333333333 samples.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2026457) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.5
Label counts after resampling: {0: 1580, 1: 1054}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.5
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] Epoch 0: |          | 1/? [00:00<00:00,  1.34it/s]Epoch 0: |          | 1/? [00:00<00:00,  1.34it/s, v_num=c05d]Epoch 0: |          | 2/? [00:01<00:00,  1.35it/s, v_num=c05d]Epoch 0: |          | 2/? [00:01<00:00,  1.35it/s, v_num=c05d]Epoch 0: |          | 3/? [00:02<00:00,  1.36it/s, v_num=c05d]Epoch 0: |          | 3/? [00:02<00:00,  1.36it/s, v_num=c05d]Epoch 0: |          | 4/? [00:02<00:00,  1.36it/s, v_num=c05d]Epoch 0: |          | 4/? [00:02<00:00,  1.36it/s, v_num=c05d]Epoch 0: |          | 5/? [00:03<00:00,  1.36it/s, v_num=c05d]Epoch 0: |          | 5/? [00:03<00:00,  1.36it/s, v_num=c05d]Epoch 0: |          | 6/? [00:04<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 6/? [00:04<00:00,  1.36it/s, v_num=c05d]Epoch 0: |          | 7/? [00:05<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 7/? [00:05<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 8/? [00:05<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 8/? [00:05<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 9/? [00:06<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 9/? [00:06<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 10/? [00:07<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 10/? [00:07<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 11/? [00:08<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 11/? [00:08<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 12/? [00:08<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 12/? [00:08<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 13/? [00:09<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 13/? [00:09<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 14/? [00:10<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 14/? [00:10<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 15/? [00:10<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 15/? [00:10<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 16/? [00:11<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 16/? [00:11<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 17/? [00:12<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 17/? [00:12<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 18/? [00:13<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 18/? [00:13<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 19/? [00:13<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 19/? [00:13<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 20/? [00:14<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 20/? [00:14<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 21/? [00:15<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 21/? [00:15<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 22/? [00:16<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 22/? [00:16<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 23/? [00:16<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 23/? [00:16<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 24/? [00:17<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 24/? [00:17<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 25/? [00:18<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 25/? [00:18<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 26/? [00:18<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 26/? [00:18<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 27/? [00:19<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 27/? [00:19<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 28/? [00:20<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 28/? [00:20<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 29/? [00:21<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 29/? [00:21<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 30/? [00:21<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 30/? [00:21<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 31/? [00:22<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 31/? [00:22<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 32/? [00:23<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 32/? [00:23<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 33/? [00:24<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 33/? [00:24<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 34/? [00:24<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 34/? [00:24<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 35/? [00:25<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 35/? [00:25<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 36/? [00:26<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 36/? [00:26<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 37/? [00:26<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 37/? [00:26<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 38/? [00:27<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 38/? [00:27<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 39/? [00:28<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 39/? [00:28<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 40/? [00:29<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 40/? [00:29<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 41/? [00:29<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 41/? [00:29<00:00,  1.37it/s, v_num=c05d]Epoch 0: |          | 42/? [00:30<00:00,  1.39it/s, v_num=c05d]Epoch 0: |          | 42/? [00:30<00:00,  1.39it/s, v_num=c05d]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s][A/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028

                                                                        [AEpoch 0: |          | 42/? [00:35<00:00,  1.20it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534]Epoch 0: |          | 42/? [00:35<00:00,  1.20it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534]Epoch 0: |          | 0/? [00:00<?, ?it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534]         Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.5
Epoch 1: |          | 1/? [00:02<00:00,  0.39it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534]Epoch 1: |          | 1/? [00:02<00:00,  0.39it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 2/? [00:03<00:00,  0.60it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 2/? [00:03<00:00,  0.60it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 3/? [00:04<00:00,  0.74it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 3/? [00:04<00:00,  0.74it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 4/? [00:04<00:00,  0.84it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 4/? [00:04<00:00,  0.84it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 5/? [00:05<00:00,  0.91it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 5/? [00:05<00:00,  0.91it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 6/? [00:06<00:00,  0.96it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 6/? [00:06<00:00,  0.96it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 7/? [00:06<00:00,  1.00it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 7/? [00:06<00:00,  1.00it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 8/? [00:07<00:00,  1.04it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 8/? [00:07<00:00,  1.04it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 9/? [00:08<00:00,  1.07it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 9/? [00:08<00:00,  1.07it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 10/? [00:09<00:00,  1.09it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 10/? [00:09<00:00,  1.09it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 11/? [00:09<00:00,  1.11it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 11/? [00:09<00:00,  1.11it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 12/? [00:10<00:00,  1.13it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 12/? [00:10<00:00,  1.13it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 13/? [00:11<00:00,  1.15it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 13/? [00:11<00:00,  1.15it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 14/? [00:12<00:00,  1.16it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 14/? [00:12<00:00,  1.16it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 15/? [00:12<00:00,  1.17it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 15/? [00:12<00:00,  1.17it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 16/? [00:13<00:00,  1.18it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 16/? [00:13<00:00,  1.18it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 17/? [00:14<00:00,  1.19it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 17/? [00:14<00:00,  1.19it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 18/? [00:14<00:00,  1.20it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 18/? [00:14<00:00,  1.20it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 19/? [00:15<00:00,  1.21it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 19/? [00:15<00:00,  1.21it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 20/? [00:16<00:00,  1.22it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 20/? [00:16<00:00,  1.22it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 21/? [00:17<00:00,  1.22it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 21/? [00:17<00:00,  1.22it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 22/? [00:17<00:00,  1.23it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 22/? [00:17<00:00,  1.23it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 23/? [00:18<00:00,  1.24it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 23/? [00:18<00:00,  1.24it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 24/? [00:19<00:00,  1.24it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 24/? [00:19<00:00,  1.24it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 25/? [00:20<00:00,  1.24it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 25/? [00:20<00:00,  1.24it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 26/? [00:20<00:00,  1.25it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 26/? [00:20<00:00,  1.25it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 27/? [00:21<00:00,  1.25it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 27/? [00:21<00:00,  1.25it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 28/? [00:22<00:00,  1.26it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 28/? [00:22<00:00,  1.26it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 29/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 29/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 30/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 30/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 31/? [00:24<00:00,  1.27it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 31/? [00:24<00:00,  1.27it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 32/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 32/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 33/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 33/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 34/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 34/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 35/? [00:27<00:00,  1.28it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 35/? [00:27<00:00,  1.28it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 36/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 36/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 37/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 37/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 38/? [00:29<00:00,  1.28it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 38/? [00:29<00:00,  1.28it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 39/? [00:30<00:00,  1.29it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 39/? [00:30<00:00,  1.29it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 40/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 40/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 41/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 41/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 42/? [00:32<00:00,  1.31it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 42/? [00:32<00:00,  1.31it/s, v_num=c05d, val_loss=0.333, val_accuracy=0.863, val_auprc=0.779, val_f1_macro=0.727, val_f1_weighted=0.873, val_precision_macro=0.702, val_precision_weighted=0.888, val_recall_macro=0.769, val_recall_weighted=0.863, val_f1_per_label_0=0.920, val_f1_per_label_1=0.534, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 1: |          | 42/? [00:37<00:00,  1.13it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 42/? [00:37<00:00,  1.13it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]         Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.5
Epoch 2: |          | 1/? [00:02<00:00,  0.39it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.520, train_accuracy=0.712, train_auprc=0.788, train_f1_macro=0.673, train_f1_weighted=0.696, train_precision_macro=0.715, train_precision_weighted=0.714, train_recall_macro=0.670, train_recall_weighted=0.712, train_f1_per_label_0=0.786, train_f1_per_label_1=0.560]Epoch 2: |          | 1/? [00:02<00:00,  0.39it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 2/? [00:03<00:00,  0.61it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 2/? [00:03<00:00,  0.61it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 3/? [00:04<00:00,  0.75it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 3/? [00:04<00:00,  0.75it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 4/? [00:04<00:00,  0.84it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 4/? [00:04<00:00,  0.84it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 5/? [00:05<00:00,  0.91it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 5/? [00:05<00:00,  0.91it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 6/? [00:06<00:00,  0.96it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 6/? [00:06<00:00,  0.96it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 7/? [00:06<00:00,  1.01it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 7/? [00:06<00:00,  1.01it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 8/? [00:07<00:00,  1.04it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 8/? [00:07<00:00,  1.04it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 9/? [00:08<00:00,  1.07it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 9/? [00:08<00:00,  1.07it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 10/? [00:09<00:00,  1.09it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 10/? [00:09<00:00,  1.09it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 11/? [00:09<00:00,  1.11it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 11/? [00:09<00:00,  1.11it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 12/? [00:10<00:00,  1.13it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 12/? [00:10<00:00,  1.13it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 14/? [00:12<00:00,  1.16it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 14/? [00:12<00:00,  1.16it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 15/? [00:12<00:00,  1.17it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 15/? [00:12<00:00,  1.17it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 16/? [00:13<00:00,  1.18it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 16/? [00:13<00:00,  1.18it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 17/? [00:14<00:00,  1.19it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 17/? [00:14<00:00,  1.19it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 18/? [00:15<00:00,  1.20it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 18/? [00:15<00:00,  1.20it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 19/? [00:15<00:00,  1.21it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 19/? [00:15<00:00,  1.21it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 21/? [00:17<00:00,  1.22it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 21/? [00:17<00:00,  1.22it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 22/? [00:17<00:00,  1.23it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 22/? [00:17<00:00,  1.23it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 24/? [00:19<00:00,  1.24it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 24/? [00:19<00:00,  1.24it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 25/? [00:20<00:00,  1.24it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 25/? [00:20<00:00,  1.24it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 26/? [00:20<00:00,  1.25it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 26/? [00:20<00:00,  1.25it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 27/? [00:21<00:00,  1.25it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 27/? [00:21<00:00,  1.25it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 29/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 29/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 30/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 30/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 32/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 32/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 33/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 33/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 34/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 34/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 35/? [00:27<00:00,  1.27it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 35/? [00:27<00:00,  1.27it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 36/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 36/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 37/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 37/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 38/? [00:29<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 38/? [00:29<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 39/? [00:30<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 39/? [00:30<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 40/? [00:31<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 40/? [00:31<00:00,  1.28it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 41/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 41/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 42/? [00:32<00:00,  1.31it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 42/? [00:32<00:00,  1.31it/s, v_num=c05d, val_loss=0.341, val_accuracy=0.913, val_auprc=0.800, val_f1_macro=0.769, val_f1_weighted=0.907, val_precision_macro=0.816, val_precision_weighted=0.905, val_recall_macro=0.738, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.587, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.03it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.05it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.30it/s][A
                                                                        [AEpoch 2: |          | 42/? [00:37<00:00,  1.13it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 42/? [00:37<00:00,  1.13it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]         Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.5
Epoch 3: |          | 1/? [00:02<00:00,  0.40it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.127, train_accuracy=0.954, train_auprc=0.987, train_f1_macro=0.952, train_f1_weighted=0.954, train_precision_macro=0.952, train_precision_weighted=0.954, train_recall_macro=0.952, train_recall_weighted=0.954, train_f1_per_label_0=0.962, train_f1_per_label_1=0.943]Epoch 3: |          | 1/? [00:02<00:00,  0.40it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 2/? [00:03<00:00,  0.62it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 2/? [00:03<00:00,  0.62it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 3/? [00:03<00:00,  0.76it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 3/? [00:03<00:00,  0.76it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 4/? [00:04<00:00,  0.86it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 4/? [00:04<00:00,  0.85it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 5/? [00:05<00:00,  0.92it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 5/? [00:05<00:00,  0.92it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 6/? [00:06<00:00,  0.97it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 6/? [00:06<00:00,  0.97it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 7/? [00:06<00:00,  1.01it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 7/? [00:06<00:00,  1.01it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 8/? [00:07<00:00,  1.05it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 8/? [00:07<00:00,  1.05it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 9/? [00:08<00:00,  1.07it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 9/? [00:08<00:00,  1.07it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 10/? [00:09<00:00,  1.10it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 10/? [00:09<00:00,  1.10it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 11/? [00:09<00:00,  1.12it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 11/? [00:09<00:00,  1.12it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 12/? [00:10<00:00,  1.14it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 12/? [00:10<00:00,  1.14it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 13/? [00:11<00:00,  1.15it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 13/? [00:11<00:00,  1.15it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 14/? [00:12<00:00,  1.16it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 14/? [00:12<00:00,  1.16it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 15/? [00:12<00:00,  1.17it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 15/? [00:12<00:00,  1.17it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 16/? [00:13<00:00,  1.18it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 16/? [00:13<00:00,  1.18it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 17/? [00:14<00:00,  1.19it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 17/? [00:14<00:00,  1.19it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 18/? [00:14<00:00,  1.20it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 18/? [00:14<00:00,  1.20it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 19/? [00:15<00:00,  1.21it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 19/? [00:15<00:00,  1.21it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 20/? [00:16<00:00,  1.22it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 20/? [00:16<00:00,  1.22it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 21/? [00:17<00:00,  1.22it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 21/? [00:17<00:00,  1.22it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 22/? [00:17<00:00,  1.23it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 22/? [00:17<00:00,  1.23it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 23/? [00:18<00:00,  1.23it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 23/? [00:18<00:00,  1.23it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 24/? [00:19<00:00,  1.24it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 24/? [00:19<00:00,  1.24it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 25/? [00:20<00:00,  1.24it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 25/? [00:20<00:00,  1.24it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 26/? [00:20<00:00,  1.25it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 26/? [00:20<00:00,  1.25it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 27/? [00:21<00:00,  1.25it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 27/? [00:21<00:00,  1.25it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 28/? [00:22<00:00,  1.25it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 28/? [00:22<00:00,  1.25it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 29/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 29/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 30/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 30/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 31/? [00:24<00:00,  1.26it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 31/? [00:24<00:00,  1.26it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 32/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 32/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 33/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 33/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 34/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 34/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 35/? [00:27<00:00,  1.27it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 35/? [00:27<00:00,  1.27it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 36/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 36/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 37/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 37/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 38/? [00:29<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 38/? [00:29<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 39/? [00:30<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 39/? [00:30<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 40/? [00:31<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 40/? [00:31<00:00,  1.28it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 41/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 41/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 42/? [00:32<00:00,  1.31it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 42/? [00:32<00:00,  1.31it/s, v_num=c05d, val_loss=0.415, val_accuracy=0.907, val_auprc=0.787, val_f1_macro=0.720, val_f1_weighted=0.893, val_precision_macro=0.824, val_precision_weighted=0.895, val_recall_macro=0.675, val_recall_weighted=0.907, val_f1_per_label_0=0.949, val_f1_per_label_1=0.491, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.13it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.13it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.12it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 3: |          | 42/? [00:37<00:00,  1.13it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 42/? [00:37<00:00,  1.13it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]         Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 1.5
Epoch 4: |          | 1/? [00:02<00:00,  0.40it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0286, train_accuracy=0.991, train_auprc=0.999, train_f1_macro=0.991, train_f1_weighted=0.991, train_precision_macro=0.991, train_precision_weighted=0.991, train_recall_macro=0.991, train_recall_weighted=0.991, train_f1_per_label_0=0.993, train_f1_per_label_1=0.989]Epoch 4: |          | 1/? [00:02<00:00,  0.40it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 2/? [00:03<00:00,  0.61it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 2/? [00:03<00:00,  0.61it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 3/? [00:04<00:00,  0.75it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 3/? [00:04<00:00,  0.75it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 4/? [00:04<00:00,  0.85it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 4/? [00:04<00:00,  0.85it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 5/? [00:05<00:00,  0.92it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 5/? [00:05<00:00,  0.92it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 6/? [00:06<00:00,  0.97it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 6/? [00:06<00:00,  0.97it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 7/? [00:06<00:00,  1.01it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 7/? [00:06<00:00,  1.01it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 8/? [00:07<00:00,  1.05it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 8/? [00:07<00:00,  1.05it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 9/? [00:08<00:00,  1.07it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 9/? [00:08<00:00,  1.07it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 10/? [00:09<00:00,  1.10it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 10/? [00:09<00:00,  1.10it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 11/? [00:09<00:00,  1.12it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 11/? [00:09<00:00,  1.12it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 12/? [00:10<00:00,  1.14it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 12/? [00:10<00:00,  1.14it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 13/? [00:11<00:00,  1.15it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 13/? [00:11<00:00,  1.15it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 14/? [00:12<00:00,  1.16it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 14/? [00:12<00:00,  1.16it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 15/? [00:12<00:00,  1.18it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 15/? [00:12<00:00,  1.18it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 16/? [00:13<00:00,  1.19it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 16/? [00:13<00:00,  1.19it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 17/? [00:14<00:00,  1.20it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 17/? [00:14<00:00,  1.20it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 18/? [00:14<00:00,  1.20it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 18/? [00:14<00:00,  1.20it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 19/? [00:15<00:00,  1.21it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 19/? [00:15<00:00,  1.21it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 20/? [00:16<00:00,  1.22it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 20/? [00:16<00:00,  1.22it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 21/? [00:17<00:00,  1.22it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 21/? [00:17<00:00,  1.22it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 22/? [00:17<00:00,  1.23it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 22/? [00:17<00:00,  1.23it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 23/? [00:18<00:00,  1.23it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 23/? [00:18<00:00,  1.23it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 24/? [00:19<00:00,  1.24it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 24/? [00:19<00:00,  1.24it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 25/? [00:20<00:00,  1.24it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 25/? [00:20<00:00,  1.24it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 26/? [00:20<00:00,  1.25it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 26/? [00:20<00:00,  1.25it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 27/? [00:21<00:00,  1.25it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 27/? [00:21<00:00,  1.25it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 28/? [00:22<00:00,  1.25it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 28/? [00:22<00:00,  1.25it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 29/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 29/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 30/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 30/? [00:23<00:00,  1.26it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 31/? [00:24<00:00,  1.26it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 31/? [00:24<00:00,  1.26it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 32/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 32/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 33/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 33/? [00:25<00:00,  1.27it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 34/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 34/? [00:26<00:00,  1.27it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 35/? [00:27<00:00,  1.27it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 35/? [00:27<00:00,  1.27it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 36/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 36/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 37/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 37/? [00:28<00:00,  1.28it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 38/? [00:29<00:00,  1.28it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 38/? [00:29<00:00,  1.28it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 39/? [00:30<00:00,  1.28it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 39/? [00:30<00:00,  1.28it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 40/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 40/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 41/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 41/? [00:31<00:00,  1.29it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 42/? [00:32<00:00,  1.31it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 42/? [00:32<00:00,  1.31it/s, v_num=c05d, val_loss=0.400, val_accuracy=0.910, val_auprc=0.819, val_f1_macro=0.716, val_f1_weighted=0.894, val_precision_macro=0.861, val_precision_weighted=0.902, val_recall_macro=0.666, val_recall_weighted=0.910, val_f1_per_label_0=0.951, val_f1_per_label_1=0.481, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.04it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.06it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.29it/s][A
                                                                        [AEpoch 4: |          | 42/? [00:37<00:00,  1.13it/s, v_num=c05d, val_loss=0.452, val_accuracy=0.910, val_auprc=0.830, val_f1_macro=0.757, val_f1_weighted=0.903, val_precision_macro=0.809, val_precision_weighted=0.901, val_recall_macro=0.725, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.565, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 42/? [00:37<00:00,  1.13it/s, v_num=c05d, val_loss=0.452, val_accuracy=0.910, val_auprc=0.830, val_f1_macro=0.757, val_f1_weighted=0.903, val_precision_macro=0.809, val_precision_weighted=0.901, val_recall_macro=0.725, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.565, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Epoch 4: |          | 42/? [00:37<00:00,  1.12it/s, v_num=c05d, val_loss=0.452, val_accuracy=0.910, val_auprc=0.830, val_f1_macro=0.757, val_f1_weighted=0.903, val_precision_macro=0.809, val_precision_weighted=0.901, val_recall_macro=0.725, val_recall_weighted=0.910, val_f1_per_label_0=0.950, val_f1_per_label_1=0.565, train_loss=0.0216, train_accuracy=0.993, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.993, train_precision_macro=0.992, train_precision_weighted=0.993, train_recall_macro=0.993, train_recall_weighted=0.993, train_f1_per_label_0=0.994, train_f1_per_label_1=0.991]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/88ede3c8160e45d2b21bd78dabe6c05d/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_4-oversampling-sampling_modifiedRS_rho=1.5-seed0-epoch=01-val_f1_macro=0.77.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/88ede3c8160e45d2b21bd78dabe6c05d/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_4-oversampling-sampling_modifiedRS_rho=1.5-seed0-epoch=01-val_f1_macro=0.77.ckpt

Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.80it/s]Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.91it/s]Validation DataLoader 0:  30%|███       | 3/10 [00:01<00:02,  2.90it/s]Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:02,  2.95it/s]Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  2.98it/s]Validation DataLoader 0:  60%|██████    | 6/10 [00:02<00:01,  3.00it/s]Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.02it/s]Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.04it/s]Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.04it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.26it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.00it/s]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/88ede3c8160e45d2b21bd78dabe6c05d/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_4-oversampling-sampling_modifiedRS_rho=1.5-seed0-epoch=01-val_f1_macro=0.77.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/88ede3c8160e45d2b21bd78dabe6c05d/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_4-oversampling-sampling_modifiedRS_rho=1.5-seed0-epoch=01-val_f1_macro=0.77.ckpt

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
     Validate metric           DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      val_accuracy          0.9133333563804626
        val_auprc           0.8001786470413208
      val_f1_macro          0.7694422006607056
   val_f1_per_label_0       0.9515828490257263
   val_f1_per_label_1       0.5873016119003296
     val_f1_weighted        0.9072619080543518
        val_loss            0.34101060032844543
   val_precision_macro      0.8161498308181763
 val_precision_weighted     0.9054641723632812
    val_recall_macro        0.7382444143295288
   val_recall_weighted      0.9133333563804626
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.06it/s]Testing DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.07it/s]Testing DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.09it/s]Testing DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s]Testing DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.10it/s]Testing DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.11it/s]Testing DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s]Testing DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s]Testing DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.05it/s]Seed set to 0
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/lightning_fabric/plugins/environments/slurm.py:191: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python imbalanced_text_classification/main.py --data_name u ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]

  | Name               | Type                          | Params
---------------------------------------------------------------------
0 | classifier         | BertForSequenceClassification | 109 M 
1 | loss_fn            | CrossEntropyLoss              | 0     
2 | train_metrics      | MetricCollection              | 0     
3 | train_f1_per_label | MulticlassF1Score             | 0     
4 | val_metrics        | MetricCollection              | 0     
5 | val_f1_per_label   | MulticlassF1Score             | 0     
6 | test_metrics       | MetricCollection              | 0     
7 | test_f1_per_label  | MulticlassF1Score             | 0     
---------------------------------------------------------------------
109 M     Trainable params
0         Non-trainable params
109 M     Total params
437.935   Total estimated model params size (MB)

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      test_accuracy         0.9150000214576721
       test_auprc           0.7506489753723145
      test_f1_macro         0.7383519411087036
   test_f1_per_label_0      0.9533394575119019
   test_f1_per_label_1      0.5233644843101501
    test_f1_weighted        0.9110586047172546
  test_precision_macro      0.7635869383811951
 test_precision_weighted    0.9083907008171082
    test_recall_macro       0.7188038229942322
  test_recall_weighted      0.9150000214576721
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
END running Trial 4.



MLFlow run_name = sampling_modifiedRS-us-election-2020-bin-Trial-5
MLflow Saved Child Search Trial 5 Log in './mlruns/5/38b0390c58e842caafc3da32cdb34b5a'
Using Cross Entropy Loss: alpha=None
Runing optuna for hyperparameter search:
Parmas: {'sampling_modifiedRS_mode': 'oversampling', 'sampling_modifiedRS_rho': 3, 'sampling_weightedRS_percentage': None, 'wce_alpha': None, 'fl_gamma': None, 'loss': <Loss.CE_Loss: 'CE_Loss'>}
Calling ImbalancedDataModule.setup() for train...
------ Dataset Statistics ------
Raw data reading from CSV files:
| Split	 | Size	 | Label Counts
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
| train_val	 | 2400 |	0: 2107, 87.8%	1: 293, 12.2%
After splitting for training:
| Split	 | Size	 | Label Counts
| train	 | 1800 |	0: 1580, 87.8%	1: 220, 12.2%
| val	 | 600 |	0: 527, 87.8%	1: 73, 12.2%
| test	 | 600 |	0: 541, 90.2%	1: 59, 9.8%
Resampling with sampling_modifiedRS_rho = 3 => training set has 2106.6666666666665 samples.
/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/multiprocessing/popen_fork.py:66: DeprecationWarning: This process (pid=2026457) is multi-threaded, use of fork() may lead to deadlocks in the child.
  self.pid = os.fork()
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Label counts after resampling: {0: 1580, 1: 527}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Training: |          | 0/? [00:00<?, ?it/s]Training: |          | 0/? [00:00<?, ?it/s]Epoch 0: |          | 0/? [00:00<?, ?it/s] Epoch 0: |          | 1/? [00:00<00:00,  1.34it/s]Epoch 0: |          | 1/? [00:00<00:00,  1.34it/s, v_num=4b5a]Epoch 0: |          | 2/? [00:01<00:00,  1.36it/s, v_num=4b5a]Epoch 0: |          | 2/? [00:01<00:00,  1.36it/s, v_num=4b5a]Epoch 0: |          | 3/? [00:02<00:00,  1.36it/s, v_num=4b5a]Epoch 0: |          | 3/? [00:02<00:00,  1.36it/s, v_num=4b5a]Epoch 0: |          | 4/? [00:02<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 4/? [00:02<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 5/? [00:03<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 5/? [00:03<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 6/? [00:04<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 6/? [00:04<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 7/? [00:05<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 7/? [00:05<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 8/? [00:05<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 8/? [00:05<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 9/? [00:06<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 9/? [00:06<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 10/? [00:07<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 10/? [00:07<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 11/? [00:08<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 11/? [00:08<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 12/? [00:08<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 12/? [00:08<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 13/? [00:09<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 13/? [00:09<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 14/? [00:10<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 14/? [00:10<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 15/? [00:10<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 15/? [00:10<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 16/? [00:11<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 16/? [00:11<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 17/? [00:12<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 17/? [00:12<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 18/? [00:13<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 18/? [00:13<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 19/? [00:13<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 19/? [00:13<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 20/? [00:14<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 20/? [00:14<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 21/? [00:15<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 21/? [00:15<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 22/? [00:16<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 22/? [00:16<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 23/? [00:16<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 23/? [00:16<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 24/? [00:17<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 24/? [00:17<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 25/? [00:18<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 25/? [00:18<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 26/? [00:18<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 26/? [00:18<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 27/? [00:19<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 27/? [00:19<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 28/? [00:20<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 28/? [00:20<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 29/? [00:21<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 29/? [00:21<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 30/? [00:21<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 30/? [00:21<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 31/? [00:22<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 31/? [00:22<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 32/? [00:23<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 32/? [00:23<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 33/? [00:24<00:00,  1.37it/s, v_num=4b5a]Epoch 0: |          | 33/? [00:24<00:00,  1.37it/s, v_num=4b5a]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A/mounts/Users/cisintern/zhangyaq/anaconda3/envs/pl_env/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: TorchMetricsUserWarning: You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum`, which is currently not supported. The tensor will be copied to the CPU memory to compute it and then copied back to GPU. Expect some slowdowns.
  warnings.warn(*args, **kwargs)  # noqa: B028

                                                                        [AEpoch 0: |          | 33/? [00:29<00:00,  1.13it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304]Epoch 0: |          | 33/? [00:29<00:00,  1.13it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304]Epoch 0: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304]         Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Epoch 1: |          | 1/? [00:02<00:00,  0.38it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304]Epoch 1: |          | 1/? [00:02<00:00,  0.38it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 2/? [00:03<00:00,  0.60it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 2/? [00:03<00:00,  0.60it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 3/? [00:04<00:00,  0.73it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 3/? [00:04<00:00,  0.73it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 4/? [00:04<00:00,  0.82it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 4/? [00:04<00:00,  0.82it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 5/? [00:05<00:00,  0.89it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 5/? [00:05<00:00,  0.89it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 6/? [00:06<00:00,  0.95it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 6/? [00:06<00:00,  0.95it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 7/? [00:07<00:00,  0.99it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 7/? [00:07<00:00,  0.99it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 8/? [00:07<00:00,  1.03it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 8/? [00:07<00:00,  1.02it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 9/? [00:08<00:00,  1.05it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 9/? [00:08<00:00,  1.05it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 10/? [00:09<00:00,  1.08it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 10/? [00:09<00:00,  1.08it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 11/? [00:10<00:00,  1.10it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 11/? [00:10<00:00,  1.10it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 12/? [00:10<00:00,  1.12it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 12/? [00:10<00:00,  1.12it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 13/? [00:11<00:00,  1.13it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 13/? [00:11<00:00,  1.13it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 14/? [00:12<00:00,  1.15it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 14/? [00:12<00:00,  1.15it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 15/? [00:12<00:00,  1.16it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 15/? [00:12<00:00,  1.16it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 16/? [00:13<00:00,  1.17it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 16/? [00:13<00:00,  1.17it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 17/? [00:14<00:00,  1.18it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 17/? [00:14<00:00,  1.18it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 18/? [00:15<00:00,  1.19it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 18/? [00:15<00:00,  1.19it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 19/? [00:15<00:00,  1.20it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 19/? [00:15<00:00,  1.20it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 20/? [00:16<00:00,  1.21it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 20/? [00:16<00:00,  1.21it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 21/? [00:17<00:00,  1.21it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 21/? [00:17<00:00,  1.21it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 22/? [00:18<00:00,  1.22it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 22/? [00:18<00:00,  1.22it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 23/? [00:18<00:00,  1.22it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 23/? [00:18<00:00,  1.22it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 24/? [00:19<00:00,  1.23it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 24/? [00:19<00:00,  1.23it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 25/? [00:20<00:00,  1.23it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 25/? [00:20<00:00,  1.23it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 26/? [00:21<00:00,  1.24it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 26/? [00:21<00:00,  1.24it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 27/? [00:21<00:00,  1.24it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 27/? [00:21<00:00,  1.24it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 28/? [00:22<00:00,  1.25it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 28/? [00:22<00:00,  1.25it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 29/? [00:23<00:00,  1.25it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 29/? [00:23<00:00,  1.25it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 30/? [00:23<00:00,  1.25it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 30/? [00:23<00:00,  1.25it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 31/? [00:24<00:00,  1.26it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 31/? [00:24<00:00,  1.26it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 32/? [00:25<00:00,  1.26it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 32/? [00:25<00:00,  1.26it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 33/? [00:26<00:00,  1.26it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 33/? [00:26<00:00,  1.26it/s, v_num=4b5a, val_loss=0.327, val_accuracy=0.870, val_auprc=0.680, val_f1_macro=0.616, val_f1_weighted=0.852, val_precision_macro=0.668, val_precision_weighted=0.844, val_recall_macro=0.596, val_recall_weighted=0.870, val_f1_per_label_0=0.928, val_f1_per_label_1=0.304, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.11it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.06it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.07it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 1: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 1: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]         Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Epoch 2: |          | 1/? [00:02<00:00,  0.39it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.544, train_accuracy=0.738, train_auprc=0.588, train_f1_macro=0.467, train_f1_weighted=0.657, train_precision_macro=0.545, train_precision_weighted=0.649, train_recall_macro=0.509, train_recall_weighted=0.738, train_f1_per_label_0=0.847, train_f1_per_label_1=0.0861]Epoch 2: |          | 1/? [00:02<00:00,  0.39it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732] Epoch 2: |          | 2/? [00:03<00:00,  0.61it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 2/? [00:03<00:00,  0.61it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 3/? [00:04<00:00,  0.75it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 3/? [00:04<00:00,  0.75it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 4/? [00:04<00:00,  0.84it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 4/? [00:04<00:00,  0.84it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 5/? [00:05<00:00,  0.91it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 5/? [00:05<00:00,  0.91it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 6/? [00:06<00:00,  0.96it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 6/? [00:06<00:00,  0.96it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 7/? [00:06<00:00,  1.01it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 7/? [00:06<00:00,  1.01it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 8/? [00:07<00:00,  1.04it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 8/? [00:07<00:00,  1.04it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 9/? [00:08<00:00,  1.06it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 9/? [00:08<00:00,  1.06it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 10/? [00:09<00:00,  1.09it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 10/? [00:09<00:00,  1.09it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 11/? [00:09<00:00,  1.11it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 11/? [00:09<00:00,  1.11it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 12/? [00:10<00:00,  1.13it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 12/? [00:10<00:00,  1.13it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 13/? [00:11<00:00,  1.14it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 14/? [00:12<00:00,  1.16it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 14/? [00:12<00:00,  1.16it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 15/? [00:12<00:00,  1.17it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 15/? [00:12<00:00,  1.17it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 16/? [00:13<00:00,  1.18it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 16/? [00:13<00:00,  1.18it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 17/? [00:14<00:00,  1.19it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 17/? [00:14<00:00,  1.19it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 18/? [00:15<00:00,  1.20it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 18/? [00:15<00:00,  1.20it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 19/? [00:15<00:00,  1.20it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 19/? [00:15<00:00,  1.20it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 20/? [00:16<00:00,  1.21it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 21/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 21/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 22/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 22/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 23/? [00:18<00:00,  1.23it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 24/? [00:19<00:00,  1.23it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 24/? [00:19<00:00,  1.23it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 25/? [00:20<00:00,  1.24it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 25/? [00:20<00:00,  1.24it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 28/? [00:22<00:00,  1.25it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 29/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 29/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 31/? [00:24<00:00,  1.26it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 33/? [00:26<00:00,  1.26it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 33/? [00:26<00:00,  1.26it/s, v_num=4b5a, val_loss=0.259, val_accuracy=0.905, val_auprc=0.810, val_f1_macro=0.745, val_f1_weighted=0.898, val_precision_macro=0.791, val_precision_weighted=0.895, val_recall_macro=0.716, val_recall_weighted=0.905, val_f1_per_label_0=0.947, val_f1_per_label_1=0.544, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.14it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.08it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.05it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.05it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.06it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.06it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.06it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.07it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.29it/s][A
                                                                        [AEpoch 2: |          | 33/? [00:31<00:00,  1.05it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 33/? [00:31<00:00,  1.05it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 2: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]         Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Epoch 3: |          | 1/? [00:02<00:00,  0.41it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.313, train_accuracy=0.868, train_auprc=0.881, train_f1_macro=0.822, train_f1_weighted=0.867, train_precision_macro=0.826, train_precision_weighted=0.867, train_recall_macro=0.819, train_recall_weighted=0.868, train_f1_per_label_0=0.912, train_f1_per_label_1=0.732]Epoch 3: |          | 1/? [00:02<00:00,  0.41it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 2/? [00:03<00:00,  0.63it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 2/? [00:03<00:00,  0.63it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 3/? [00:03<00:00,  0.77it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 3/? [00:03<00:00,  0.77it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 4/? [00:04<00:00,  0.86it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 4/? [00:04<00:00,  0.86it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 5/? [00:05<00:00,  0.93it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 5/? [00:05<00:00,  0.93it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 6/? [00:06<00:00,  0.98it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 6/? [00:06<00:00,  0.98it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 7/? [00:06<00:00,  1.02it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 7/? [00:06<00:00,  1.02it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 8/? [00:07<00:00,  1.05it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 8/? [00:07<00:00,  1.05it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 9/? [00:08<00:00,  1.08it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 9/? [00:08<00:00,  1.08it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 10/? [00:09<00:00,  1.10it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 10/? [00:09<00:00,  1.10it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 11/? [00:09<00:00,  1.12it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 11/? [00:09<00:00,  1.12it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 12/? [00:10<00:00,  1.14it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 12/? [00:10<00:00,  1.14it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 13/? [00:11<00:00,  1.15it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 13/? [00:11<00:00,  1.15it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 14/? [00:12<00:00,  1.16it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 14/? [00:12<00:00,  1.16it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 15/? [00:12<00:00,  1.17it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 15/? [00:12<00:00,  1.17it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 16/? [00:13<00:00,  1.18it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 16/? [00:13<00:00,  1.18it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 17/? [00:14<00:00,  1.19it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 17/? [00:14<00:00,  1.19it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 18/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 18/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 19/? [00:15<00:00,  1.21it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 19/? [00:15<00:00,  1.21it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 21/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 21/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 23/? [00:18<00:00,  1.23it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 23/? [00:18<00:00,  1.23it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 25/? [00:20<00:00,  1.24it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 25/? [00:20<00:00,  1.24it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 29/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 29/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 31/? [00:24<00:00,  1.26it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 33/? [00:26<00:00,  1.27it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 33/? [00:26<00:00,  1.27it/s, v_num=4b5a, val_loss=0.413, val_accuracy=0.898, val_auprc=0.801, val_f1_macro=0.630, val_f1_weighted=0.868, val_precision_macro=0.887, val_precision_weighted=0.896, val_recall_macro=0.594, val_recall_weighted=0.898, val_f1_per_label_0=0.945, val_f1_per_label_1=0.315, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.13it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.13it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.14it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.13it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.13it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.12it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 3: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 3: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]         Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Epoch 4: |          | 1/? [00:02<00:00,  0.42it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.107, train_accuracy=0.965, train_auprc=0.980, train_f1_macro=0.953, train_f1_weighted=0.965, train_precision_macro=0.954, train_precision_weighted=0.965, train_recall_macro=0.953, train_recall_weighted=0.965, train_f1_per_label_0=0.977, train_f1_per_label_1=0.930]Epoch 4: |          | 1/? [00:02<00:00,  0.42it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 2/? [00:03<00:00,  0.64it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 2/? [00:03<00:00,  0.64it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 3/? [00:03<00:00,  0.77it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 3/? [00:03<00:00,  0.77it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 4/? [00:04<00:00,  0.87it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 4/? [00:04<00:00,  0.87it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 5/? [00:05<00:00,  0.93it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 5/? [00:05<00:00,  0.93it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 6/? [00:06<00:00,  0.98it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 6/? [00:06<00:00,  0.98it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 7/? [00:06<00:00,  1.02it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 7/? [00:06<00:00,  1.02it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 8/? [00:07<00:00,  1.06it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 8/? [00:07<00:00,  1.06it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 9/? [00:08<00:00,  1.08it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 9/? [00:08<00:00,  1.08it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 10/? [00:09<00:00,  1.11it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 10/? [00:09<00:00,  1.11it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 11/? [00:09<00:00,  1.13it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 11/? [00:09<00:00,  1.13it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 12/? [00:10<00:00,  1.14it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 12/? [00:10<00:00,  1.14it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 13/? [00:11<00:00,  1.15it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 13/? [00:11<00:00,  1.15it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 14/? [00:11<00:00,  1.17it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 14/? [00:11<00:00,  1.17it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 15/? [00:12<00:00,  1.18it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 15/? [00:12<00:00,  1.18it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 16/? [00:13<00:00,  1.19it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 16/? [00:13<00:00,  1.19it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 17/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 17/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 18/? [00:14<00:00,  1.21it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 18/? [00:14<00:00,  1.21it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 19/? [00:15<00:00,  1.21it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 19/? [00:15<00:00,  1.21it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 21/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 21/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 23/? [00:18<00:00,  1.24it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 23/? [00:18<00:00,  1.24it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 25/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 25/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 29/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 29/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 33/? [00:26<00:00,  1.27it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 33/? [00:26<00:00,  1.27it/s, v_num=4b5a, val_loss=0.421, val_accuracy=0.913, val_auprc=0.788, val_f1_macro=0.740, val_f1_weighted=0.901, val_precision_macro=0.853, val_precision_weighted=0.905, val_recall_macro=0.691, val_recall_weighted=0.913, val_f1_per_label_0=0.952, val_f1_per_label_1=0.527, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.13it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.17it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.14it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.11it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.12it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.11it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 4: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 4: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]         Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Epoch 5: |          | 1/? [00:02<00:00,  0.40it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.038, train_accuracy=0.989, train_auprc=0.997, train_f1_macro=0.985, train_f1_weighted=0.989, train_precision_macro=0.986, train_precision_weighted=0.989, train_recall_macro=0.985, train_recall_weighted=0.989, train_f1_per_label_0=0.993, train_f1_per_label_1=0.978]Epoch 5: |          | 1/? [00:02<00:00,  0.40it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 2/? [00:03<00:00,  0.62it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 2/? [00:03<00:00,  0.62it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 3/? [00:03<00:00,  0.76it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 3/? [00:03<00:00,  0.76it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 4/? [00:04<00:00,  0.85it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 4/? [00:04<00:00,  0.85it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 5/? [00:05<00:00,  0.92it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 5/? [00:05<00:00,  0.92it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 6/? [00:06<00:00,  0.97it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 6/? [00:06<00:00,  0.97it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 7/? [00:06<00:00,  1.02it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 7/? [00:06<00:00,  1.02it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 8/? [00:07<00:00,  1.05it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 8/? [00:07<00:00,  1.05it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 9/? [00:08<00:00,  1.08it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 9/? [00:08<00:00,  1.08it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 10/? [00:09<00:00,  1.10it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 10/? [00:09<00:00,  1.10it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 11/? [00:09<00:00,  1.12it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 11/? [00:09<00:00,  1.12it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 12/? [00:10<00:00,  1.14it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 12/? [00:10<00:00,  1.14it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 13/? [00:11<00:00,  1.15it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 13/? [00:11<00:00,  1.15it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 14/? [00:12<00:00,  1.17it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 14/? [00:12<00:00,  1.17it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 15/? [00:12<00:00,  1.18it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 15/? [00:12<00:00,  1.18it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 16/? [00:13<00:00,  1.19it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 16/? [00:13<00:00,  1.19it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 17/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 17/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 18/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 18/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 19/? [00:15<00:00,  1.21it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 19/? [00:15<00:00,  1.21it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 21/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 21/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 23/? [00:18<00:00,  1.24it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 23/? [00:18<00:00,  1.24it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 25/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 25/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 29/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 29/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 33/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 33/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.392, val_accuracy=0.902, val_auprc=0.805, val_f1_macro=0.774, val_f1_weighted=0.903, val_precision_macro=0.769, val_precision_weighted=0.903, val_recall_macro=0.779, val_recall_weighted=0.902, val_f1_per_label_0=0.944, val_f1_per_label_1=0.604, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.04it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.01it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.05it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.32it/s][A
                                                                        [AEpoch 5: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 5: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]         Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Epoch 6: |          | 1/? [00:02<00:00,  0.40it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0218, train_accuracy=0.994, train_auprc=0.999, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.993, train_precision_weighted=0.994, train_recall_macro=0.992, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.989]Epoch 6: |          | 1/? [00:02<00:00,  0.40it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 2/? [00:03<00:00,  0.62it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 2/? [00:03<00:00,  0.62it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 3/? [00:03<00:00,  0.76it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 3/? [00:03<00:00,  0.76it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 4/? [00:04<00:00,  0.85it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 4/? [00:04<00:00,  0.85it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 5/? [00:05<00:00,  0.92it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 5/? [00:05<00:00,  0.92it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 6/? [00:06<00:00,  0.97it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 6/? [00:06<00:00,  0.97it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 7/? [00:06<00:00,  1.02it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 7/? [00:06<00:00,  1.02it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 8/? [00:07<00:00,  1.05it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 8/? [00:07<00:00,  1.05it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 9/? [00:08<00:00,  1.08it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 9/? [00:08<00:00,  1.08it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 10/? [00:09<00:00,  1.10it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 10/? [00:09<00:00,  1.10it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 11/? [00:09<00:00,  1.12it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 11/? [00:09<00:00,  1.12it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 12/? [00:10<00:00,  1.14it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 12/? [00:10<00:00,  1.14it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 13/? [00:11<00:00,  1.15it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 13/? [00:11<00:00,  1.15it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 14/? [00:12<00:00,  1.16it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 14/? [00:12<00:00,  1.16it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 15/? [00:12<00:00,  1.18it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 15/? [00:12<00:00,  1.18it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 16/? [00:13<00:00,  1.19it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 16/? [00:13<00:00,  1.19it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 17/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 17/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 18/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 18/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 19/? [00:15<00:00,  1.21it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 19/? [00:15<00:00,  1.21it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 21/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 21/? [00:17<00:00,  1.22it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 23/? [00:18<00:00,  1.24it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 23/? [00:18<00:00,  1.24it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 25/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 25/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 27/? [00:21<00:00,  1.25it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 29/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 29/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 30/? [00:23<00:00,  1.26it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 33/? [00:26<00:00,  1.27it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 33/? [00:26<00:00,  1.27it/s, v_num=4b5a, val_loss=0.442, val_accuracy=0.915, val_auprc=0.831, val_f1_macro=0.714, val_f1_weighted=0.895, val_precision_macro=0.936, val_precision_weighted=0.919, val_recall_macro=0.657, val_recall_weighted=0.915, val_f1_per_label_0=0.954, val_f1_per_label_1=0.474, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.10it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.10it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s][A
                                                                        [AEpoch 6: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 33/? [00:31<00:00,  1.06it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 6: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]         Epoch 7: |          | 0/? [00:00<?, ?it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Original label_counts: {1: 220, 0: 1580}
target rho = 3
Epoch 7: |          | 1/? [00:02<00:00,  0.41it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0325, train_accuracy=0.992, train_auprc=0.998, train_f1_macro=0.989, train_f1_weighted=0.992, train_precision_macro=0.987, train_precision_weighted=0.992, train_recall_macro=0.991, train_recall_weighted=0.992, train_f1_per_label_0=0.995, train_f1_per_label_1=0.984]Epoch 7: |          | 1/? [00:02<00:00,  0.41it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 2/? [00:03<00:00,  0.64it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 2/? [00:03<00:00,  0.64it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 3/? [00:03<00:00,  0.77it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 3/? [00:03<00:00,  0.77it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 4/? [00:04<00:00,  0.87it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 4/? [00:04<00:00,  0.87it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 5/? [00:05<00:00,  0.93it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 5/? [00:05<00:00,  0.93it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 6/? [00:06<00:00,  0.98it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 6/? [00:06<00:00,  0.98it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 7/? [00:06<00:00,  1.03it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 7/? [00:06<00:00,  1.03it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 8/? [00:07<00:00,  1.06it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 8/? [00:07<00:00,  1.06it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 9/? [00:08<00:00,  1.09it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 9/? [00:08<00:00,  1.09it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 10/? [00:09<00:00,  1.11it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 10/? [00:09<00:00,  1.11it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 11/? [00:09<00:00,  1.13it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 11/? [00:09<00:00,  1.13it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 12/? [00:10<00:00,  1.15it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 12/? [00:10<00:00,  1.15it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 13/? [00:11<00:00,  1.16it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 13/? [00:11<00:00,  1.16it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 14/? [00:11<00:00,  1.17it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 14/? [00:11<00:00,  1.17it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 15/? [00:12<00:00,  1.18it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 15/? [00:12<00:00,  1.18it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 16/? [00:13<00:00,  1.19it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 16/? [00:13<00:00,  1.19it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 17/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 17/? [00:14<00:00,  1.20it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 18/? [00:14<00:00,  1.21it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 18/? [00:14<00:00,  1.21it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 19/? [00:15<00:00,  1.22it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 19/? [00:15<00:00,  1.22it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 20/? [00:16<00:00,  1.22it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 21/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 21/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 22/? [00:17<00:00,  1.23it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 23/? [00:18<00:00,  1.24it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 23/? [00:18<00:00,  1.24it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 24/? [00:19<00:00,  1.24it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 25/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 25/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 26/? [00:20<00:00,  1.25it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 27/? [00:21<00:00,  1.26it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 27/? [00:21<00:00,  1.26it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 28/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 29/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 29/? [00:22<00:00,  1.26it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 30/? [00:23<00:00,  1.27it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 30/? [00:23<00:00,  1.27it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 31/? [00:24<00:00,  1.27it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 32/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 33/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 33/? [00:25<00:00,  1.27it/s, v_num=4b5a, val_loss=0.410, val_accuracy=0.920, val_auprc=0.818, val_f1_macro=0.771, val_f1_weighted=0.911, val_precision_macro=0.860, val_precision_weighted=0.913, val_recall_macro=0.724, val_recall_weighted=0.920, val_f1_per_label_0=0.956, val_f1_per_label_1=0.586, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Validation: |          | 0/? [00:00<?, ?it/s][A
Validation:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s][A
Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:02,  3.09it/s][A
Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.07it/s][A
Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.08it/s][A
Validation DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.09it/s][A
Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.10it/s][A
Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.09it/s][A
Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.31it/s][A
                                                                        [AEpoch 7: |          | 33/? [00:30<00:00,  1.07it/s, v_num=4b5a, val_loss=0.402, val_accuracy=0.897, val_auprc=0.822, val_f1_macro=0.772, val_f1_weighted=0.899, val_precision_macro=0.758, val_precision_weighted=0.903, val_recall_macro=0.788, val_recall_weighted=0.897, val_f1_per_label_0=0.941, val_f1_per_label_1=0.603, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 33/? [00:30<00:00,  1.07it/s, v_num=4b5a, val_loss=0.402, val_accuracy=0.897, val_auprc=0.822, val_f1_macro=0.772, val_f1_weighted=0.899, val_precision_macro=0.758, val_precision_weighted=0.903, val_recall_macro=0.788, val_recall_weighted=0.897, val_f1_per_label_0=0.941, val_f1_per_label_1=0.603, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Epoch 7: |          | 33/? [00:31<00:00,  1.05it/s, v_num=4b5a, val_loss=0.402, val_accuracy=0.897, val_auprc=0.822, val_f1_macro=0.772, val_f1_weighted=0.899, val_precision_macro=0.758, val_precision_weighted=0.903, val_recall_macro=0.788, val_recall_weighted=0.897, val_f1_per_label_0=0.941, val_f1_per_label_1=0.603, train_loss=0.0192, train_accuracy=0.994, train_auprc=1.000, train_f1_macro=0.992, train_f1_weighted=0.994, train_precision_macro=0.992, train_precision_weighted=0.994, train_recall_macro=0.991, train_recall_weighted=0.994, train_f1_per_label_0=0.996, train_f1_per_label_1=0.988]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/38b0390c58e842caafc3da32cdb34b5a/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_5-oversampling-sampling_modifiedRS_rho=3-seed0-epoch=04-val_f1_macro=0.77.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/38b0390c58e842caafc3da32cdb34b5a/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_5-oversampling-sampling_modifiedRS_rho=3-seed0-epoch=04-val_f1_macro=0.77.ckpt

Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Validation: |          | 0/? [00:00<?, ?it/s]Validation:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Validation DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.71it/s]Validation DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  2.87it/s]Validation DataLoader 0:  30%|███       | 3/10 [00:01<00:02,  2.91it/s]Validation DataLoader 0:  40%|████      | 4/10 [00:01<00:02,  2.95it/s]Validation DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  2.96it/s]Validation DataLoader 0:  60%|██████    | 6/10 [00:02<00:01,  2.98it/s]Validation DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.00it/s]Validation DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.01it/s]Validation DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.02it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.23it/s]Validation DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  2.92it/s]Restoring states from the checkpoint path at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/38b0390c58e842caafc3da32cdb34b5a/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_5-oversampling-sampling_modifiedRS_rho=3-seed0-epoch=04-val_f1_macro=0.77.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6]
Loaded model weights from the checkpoint at /mounts/Users/cisintern/zhangyaq/imbalanced_text_classification/mlruns/5/38b0390c58e842caafc3da32cdb34b5a/artifacts/model_checkpoints/us-election-2020-sampling_modifiedRS-Trial_5-oversampling-sampling_modifiedRS_rho=3-seed0-epoch=04-val_f1_macro=0.77.ckpt

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
     Validate metric           DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      val_accuracy          0.9016666412353516
        val_auprc           0.8045237064361572
      val_f1_macro          0.7739449143409729
   val_f1_per_label_0       0.9438629746437073
   val_f1_per_label_1       0.6040268540382385
     val_f1_weighted         0.90251624584198
        val_loss            0.3916124105453491
   val_precision_macro      0.7693350911140442
 val_precision_weighted     0.9034390449523926
    val_recall_macro        0.7788074016571045
   val_recall_weighted      0.9016666412353516
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Calling ImbalancedDataModule.setup() for validation/test...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Testing: |          | 0/? [00:00<?, ?it/s]Testing:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:   0%|          | 0/10 [00:00<?, ?it/s]Testing DataLoader 0:  10%|█         | 1/10 [00:00<00:03,  2.99it/s]Testing DataLoader 0:  20%|██        | 2/10 [00:00<00:02,  3.01it/s]Testing DataLoader 0:  30%|███       | 3/10 [00:00<00:02,  3.03it/s]Testing DataLoader 0:  40%|████      | 4/10 [00:01<00:01,  3.05it/s]Testing DataLoader 0:  50%|█████     | 5/10 [00:01<00:01,  3.04it/s]Testing DataLoader 0:  60%|██████    | 6/10 [00:01<00:01,  3.04it/s]Testing DataLoader 0:  70%|███████   | 7/10 [00:02<00:00,  3.05it/s]Testing DataLoader 0:  80%|████████  | 8/10 [00:02<00:00,  3.05it/s]Testing DataLoader 0:  90%|█████████ | 9/10 [00:02<00:00,  3.06it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.27it/s]Testing DataLoader 0: 100%|██████████| 10/10 [00:03<00:00,  3.01it/s]
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
      test_accuracy         0.9049999713897705
       test_auprc           0.7801848649978638
      test_f1_macro         0.7490295171737671
   test_f1_per_label_0      0.9468778967857361
   test_f1_per_label_1      0.5511810779571533
    test_f1_weighted        0.9079676866531372
  test_precision_macro      0.7347965240478516
 test_precision_weighted    0.9116027355194092
    test_recall_macro       0.7661111354827881
  test_recall_weighted      0.9050000309944153
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
END running Trial 5.


OPTUNA - Number of finished trials: 6
Best trial:
  val_f1_macro: 0.8102084994316101
  Params:
    sampling_modifiedRS_rho: 2.0
